[
  [
    "def main(hf_ckpt_path, save_path, n_experts, mp):\n    \"\"\"\n    Converts and saves model checkpoint files into a specified format.\n\n    Args:\n        hf_ckpt_path (str): Path to the directory containing the input checkpoint files.\n        save_path (str): Path to the directory where the converted checkpoint files will be saved.\n        n_experts (int): Total number of experts in the model.\n        mp (int): Model parallelism factor.\n        \n    Returns:\n        None\n    \"\"\"\n    torch.set_num_threads(8)\n    n_local_experts = n_experts // mp\n    state_dicts = [{} for _ in range(mp)]\n\n    for file_path in tqdm(glob(os.path.join(hf_ckpt_path, \"*.safetensors\"))):\n        with safe_open(file_path, framework=\"pt\", device=\"cpu\") as f:\n            for name in f.keys():\n                if \"model.layers.61\" in name:\n                    continue\n                param: torch.Tensor = f.get_tensor(name)\n                if name.startswith(\"model.\"):\n                    name = name[len(\"model.\"):]\n                name = name.replace(\"self_attn\", \"attn\")\n                name = name.replace(\"mlp\", \"ffn\")\n                name = name.replace(\"weight_scale_inv\", \"scale\")\n                name = name.replace(\"e_score_correction_bias\", \"bias\")\n                key = name.split(\".\")[-2]\n                assert key in mapping, f\"Key {key} not found in mapping\"\n                new_key, dim = mapping[key]\n                name = name.replace(key, new_key)\n                for i in range(mp):\n                    new_param = param\n                    if \"experts\" in name and \"shared_experts\" not in name:\n                        idx = int(name.split(\".\")[-3])\n                        if idx < i * n_local_experts or idx >= (i + 1) * n_local_experts:\n                            continue\n                    elif dim is not None:\n                        assert param.size(dim) % mp == 0, f\"Dimension {dim} must be divisible by {mp}\"\n                        shard_size = param.size(dim) // mp\n                        new_param = param.narrow(dim, i * shard_size, shard_size).contiguous()\n                    state_dicts[i][name] = new_param\n\n    os.makedirs(save_path, exist_ok=True)\n\n    for i in trange(mp):\n        save_file(state_dicts[i], os.path.join(save_path, f\"model{i}-mp{mp}.safetensors\"))\n\n    for file_path in glob(os.path.join(hf_ckpt_path, \"*token*\")):\n        new_file_path = os.path.join(save_path, os.path.basename(file_path))\n        shutil.copyfile(file_path, new_file_path)",
    "Converts and saves model checkpoint files into a specified format.\n\nArgs:\n    hf_ckpt_path (str): Path to the directory containing the input checkpoint files.\n    save_path (str): Path to the directory where the converted checkpoint files will be saved.\n    n_experts (int): Total number of experts in the model.\n    mp (int): Model parallelism factor.\n    \nReturns:\n    None"
  ],
  [
    "def main(fp8_path, bf16_path):\n    \"\"\"\n    Converts FP8 weights to BF16 and saves the converted weights.\n\n    This function reads FP8 weights from the specified directory, converts them to BF16,\n    and saves the converted weights to another specified directory. It also updates the\n    model index file to reflect the changes.\n\n    Args:\n    fp8_path (str): The path to the directory containing the FP8 weights and model index file.\n    bf16_path (str): The path to the directory where the converted BF16 weights will be saved.\n\n    Raises:\n    KeyError: If a required scale_inv tensor is missing for a weight.\n\n    Notes:\n    - The function assumes that the FP8 weights are stored in safetensor files.\n    - The function caches loaded safetensor files to optimize memory usage.\n    - The function updates the model index file to remove references to scale_inv tensors.\n    \"\"\"\n    torch.set_default_dtype(torch.bfloat16)\n    os.makedirs(bf16_path, exist_ok=True)\n    model_index_file = os.path.join(fp8_path, \"model.safetensors.index.json\")\n    with open(model_index_file, \"r\") as f:\n        model_index = json.load(f)\n    weight_map = model_index[\"weight_map\"]\n    \n    # Cache for loaded safetensor files\n    loaded_files = {}\n    fp8_weight_names = []\n\n    # Helper function to get tensor from the correct file\n    def get_tensor(tensor_name):\n        \"\"\"\n        Retrieves a tensor from the cached safetensor files or loads it from disk if not cached.\n\n        Args:\n            tensor_name (str): The name of the tensor to retrieve.\n\n        Returns:\n            torch.Tensor: The retrieved tensor.\n\n        Raises:\n            KeyError: If the tensor does not exist in the safetensor file.\n        \"\"\"\n        file_name = weight_map[tensor_name]\n        if file_name not in loaded_files:\n            file_path = os.path.join(fp8_path, file_name)\n            loaded_files[file_name] = load_file(file_path, device=\"cuda\")\n        return loaded_files[file_name][tensor_name]\n\n    safetensor_files = list(glob(os.path.join(fp8_path, \"*.safetensors\")))\n    safetensor_files.sort()\n    for safetensor_file in tqdm(safetensor_files):\n        file_name = os.path.basename(safetensor_file)\n        current_state_dict = load_file(safetensor_file, device=\"cuda\")\n        loaded_files[file_name] = current_state_dict\n        \n        new_state_dict = {}\n        for weight_name, weight in current_state_dict.items():\n            if weight_name.endswith(\"_scale_inv\"):\n                continue\n            elif weight.element_size() == 1:  # FP8 weight\n                scale_inv_name = f\"{weight_name}_scale_inv\"\n                try:\n                    # Get scale_inv from the correct file\n                    scale_inv = get_tensor(scale_inv_name)\n                    fp8_weight_names.append(weight_name)\n                    new_state_dict[weight_name] = weight_dequant(weight, scale_inv)\n                except KeyError:\n                    print(f\"Warning: Missing scale_inv tensor for {weight_name}, skipping conversion\")\n                    new_state_dict[weight_name] = weight\n            else:\n                new_state_dict[weight_name] = weight\n                \n        new_safetensor_file = os.path.join(bf16_path, file_name)\n        save_file(new_state_dict, new_safetensor_file)\n        \n        # Memory management: keep only the 2 most recently used files\n        if len(loaded_files) > 2:\n            oldest_file = next(iter(loaded_files))\n            del loaded_files[oldest_file]\n            torch.cuda.empty_cache()\n    \n    # Update model index\n    new_model_index_file = os.path.join(bf16_path, \"model.safetensors.index.json\")\n    for weight_name in fp8_weight_names:\n        scale_inv_name = f\"{weight_name}_scale_inv\"\n        if scale_inv_name in weight_map:\n            weight_map.pop(scale_inv_name)\n    with open(new_model_index_file, \"w\") as f:\n        json.dump({\"metadata\": {}, \"weight_map\": weight_map}, f, indent=2)",
    "Converts FP8 weights to BF16 and saves the converted weights.\n\nThis function reads FP8 weights from the specified directory, converts them to BF16,\nand saves the converted weights to another specified directory. It also updates the\nmodel index file to reflect the changes.\n\nArgs:\nfp8_path (str): The path to the directory containing the FP8 weights and model index file.\nbf16_path (str): The path to the directory where the converted BF16 weights will be saved.\n\nRaises:\nKeyError: If a required scale_inv tensor is missing for a weight.\n\nNotes:\n- The function assumes that the FP8 weights are stored in safetensor files.\n- The function caches loaded safetensor files to optimize memory usage.\n- The function updates the model index file to remove references to scale_inv tensors."
  ],
  [
    "def get_tensor(tensor_name):\n        \"\"\"\n        Retrieves a tensor from the cached safetensor files or loads it from disk if not cached.\n\n        Args:\n            tensor_name (str): The name of the tensor to retrieve.\n\n        Returns:\n            torch.Tensor: The retrieved tensor.\n\n        Raises:\n            KeyError: If the tensor does not exist in the safetensor file.\n        \"\"\"\n        file_name = weight_map[tensor_name]\n        if file_name not in loaded_files:\n            file_path = os.path.join(fp8_path, file_name)\n            loaded_files[file_name] = load_file(file_path, device=\"cuda\")\n        return loaded_files[file_name][tensor_name]",
    "Retrieves a tensor from the cached safetensor files or loads it from disk if not cached.\n\nArgs:\n    tensor_name (str): The name of the tensor to retrieve.\n\nReturns:\n    torch.Tensor: The retrieved tensor.\n\nRaises:\n    KeyError: If the tensor does not exist in the safetensor file."
  ],
  [
    "def sample(logits, temperature: float = 1.0):\n    \"\"\"\n    Samples a token from the logits using temperature scaling.\n\n    Args:\n        logits (torch.Tensor): The logits tensor for token predictions.\n        temperature (float, optional): Temperature for scaling logits. Defaults to 1.0.\n\n    Returns:\n        torch.Tensor: The sampled token.\n    \"\"\"\n    logits = logits / max(temperature, 1e-5)\n    probs = torch.softmax(logits, dim=-1)\n    return probs.div_(torch.empty_like(probs).exponential_(1)).argmax(dim=-1)",
    "Samples a token from the logits using temperature scaling.\n\nArgs:\n    logits (torch.Tensor): The logits tensor for token predictions.\n    temperature (float, optional): Temperature for scaling logits. Defaults to 1.0.\n\nReturns:\n    torch.Tensor: The sampled token."
  ],
  [
    "def generate(\n    model: Transformer,\n    prompt_tokens: List[List[int]],\n    max_new_tokens: int,\n    eos_id: int,\n    temperature: float = 1.0\n) -> List[List[int]]:\n    \"\"\"\n    Generates new tokens based on the given prompt tokens using the specified model.\n\n    Args:\n        model (Transformer): The transformer model used for token generation.\n        prompt_tokens (List[List[int]]): A list of lists containing the prompt tokens for each sequence.\n        max_new_tokens (int): The maximum number of new tokens to generate.\n        eos_id (int): The end-of-sequence token ID.\n        temperature (float, optional): The temperature value for sampling. Defaults to 1.0.\n\n    Returns:\n        List[List[int]]: A list of lists containing the generated tokens for each sequence.\n    \"\"\"\n    prompt_lens = [len(t) for t in prompt_tokens]\n    assert max(prompt_lens) <= model.max_seq_len, f\"Prompt length exceeds model maximum sequence length (max_seq_len={model.max_seq_len})\"\n    total_len = min(model.max_seq_len, max_new_tokens + max(prompt_lens))\n    tokens = torch.full((len(prompt_tokens), total_len), -1, dtype=torch.long, device=\"cuda\")\n    for i, t in enumerate(prompt_tokens):\n        tokens[i, :len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n    prev_pos = 0\n    finished = torch.tensor([False] * len(prompt_tokens), device=\"cuda\")\n    prompt_mask = tokens != -1\n    for cur_pos in range(min(prompt_lens), total_len):\n        logits = model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n        if temperature > 0:\n            next_token = sample(logits, temperature)\n        else:\n            next_token = logits.argmax(dim=-1)\n        next_token = torch.where(prompt_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n        tokens[:, cur_pos] = next_token\n        finished |= torch.logical_and(~prompt_mask[:, cur_pos], next_token == eos_id)\n        prev_pos = cur_pos\n        if finished.all():\n            break\n    completion_tokens = []\n    for i, toks in enumerate(tokens.tolist()):\n        toks = toks[prompt_lens[i]:prompt_lens[i]+max_new_tokens]\n        if eos_id in toks:\n            toks = toks[:toks.index(eos_id)]\n        completion_tokens.append(toks)\n    return completion_tokens",
    "Generates new tokens based on the given prompt tokens using the specified model.\n\nArgs:\n    model (Transformer): The transformer model used for token generation.\n    prompt_tokens (List[List[int]]): A list of lists containing the prompt tokens for each sequence.\n    max_new_tokens (int): The maximum number of new tokens to generate.\n    eos_id (int): The end-of-sequence token ID.\n    temperature (float, optional): The temperature value for sampling. Defaults to 1.0.\n\nReturns:\n    List[List[int]]: A list of lists containing the generated tokens for each sequence."
  ],
  [
    "def main(\n    ckpt_path: str,\n    config: str,\n    input_file: str = \"\",\n    interactive: bool = True,\n    max_new_tokens: int = 100,\n    temperature: float = 1.0,\n) -> None:\n    \"\"\"\n    Main function to load the model and perform interactive or batch text generation.\n\n    Args:\n        ckpt_path (str): Path to the model checkpoint directory.\n        config (str): Path to the model configuration file.\n        input_file (str, optional): Path to a file containing input prompts. Defaults to \"\".\n        interactive (bool, optional): Whether to run in interactive mode. Defaults to True.\n        max_new_tokens (int, optional): Maximum number of new tokens to generate. Defaults to 100.\n        temperature (float, optional): Temperature for sampling. Defaults to 1.0.\n    \"\"\"\n    world_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n    rank = int(os.getenv(\"RANK\", \"0\"))\n    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n    if world_size > 1:\n        dist.init_process_group(\"nccl\")\n    global print\n    if rank != 0:\n        print = lambda *_, **__: None\n    torch.cuda.set_device(local_rank)\n    torch.set_default_dtype(torch.bfloat16)\n    torch.set_num_threads(8)\n    torch.manual_seed(965)\n    with open(config) as f:\n        args = ModelArgs(**json.load(f))\n    print(args)\n    with torch.device(\"cuda\"):\n        model = Transformer(args)\n    tokenizer = AutoTokenizer.from_pretrained(ckpt_path)\n    tokenizer.decode(generate(model, [tokenizer.encode(\"DeepSeek\")], 2, -1, 1.)[0])\n    load_model(model, os.path.join(ckpt_path, f\"model{rank}-mp{world_size}.safetensors\"))\n\n    if interactive:\n        messages = []\n        while True:\n            if world_size == 1:\n                prompt = input(\">>> \")\n            elif rank == 0:\n                prompt = input(\">>> \")\n                objects = [prompt]\n                dist.broadcast_object_list(objects, 0)\n            else:\n                objects = [None]\n                dist.broadcast_object_list(objects, 0)\n                prompt = objects[0]\n            if prompt == \"/exit\":\n                break\n            elif prompt == \"/clear\":\n                messages.clear()\n                continue\n            messages.append({\"role\": \"user\", \"content\": prompt})\n            prompt_tokens = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n            completion_tokens = generate(model, [prompt_tokens], max_new_tokens, tokenizer.eos_token_id, temperature)\n            completion = tokenizer.decode(completion_tokens[0], skip_special_tokens=True)\n            print(completion)\n            messages.append({\"role\": \"assistant\", \"content\": completion})\n    else:\n        with open(input_file) as f:\n            prompts = [line.strip() for line in f.readlines()]\n        assert len(prompts) <= args.max_batch_size, f\"Number of prompts exceeds maximum batch size ({args.max_batch_size})\"\n        prompt_tokens = [tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], add_generation_prompt=True) for prompt in prompts]\n        completion_tokens = generate(model, prompt_tokens, max_new_tokens, tokenizer.eos_token_id, temperature)\n        completions = tokenizer.batch_decode(completion_tokens, skip_special_tokens=True)\n        for prompt, completion in zip(prompts, completions):\n            print(\"Prompt:\", prompt)\n            print(\"Completion:\", completion)\n            print()\n\n    if world_size > 1:\n        dist.destroy_process_group()",
    "Main function to load the model and perform interactive or batch text generation.\n\nArgs:\n    ckpt_path (str): Path to the model checkpoint directory.\n    config (str): Path to the model configuration file.\n    input_file (str, optional): Path to a file containing input prompts. Defaults to \"\".\n    interactive (bool, optional): Whether to run in interactive mode. Defaults to True.\n    max_new_tokens (int, optional): Maximum number of new tokens to generate. Defaults to 100.\n    temperature (float, optional): Temperature for sampling. Defaults to 1.0."
  ],
  [
    "def act_quant_kernel(x_ptr, y_ptr, s_ptr, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    Quantizes the input tensor `x_ptr` and stores the result in `y_ptr` and the scaling factor in `s_ptr`.\n\n    Args:\n        x_ptr (triton.Pointer): Pointer to the input tensor.\n        y_ptr (triton.Pointer): Pointer to the output tensor where quantized values will be stored.\n        s_ptr (triton.Pointer): Pointer to the output tensor where scaling factors will be stored.\n        BLOCK_SIZE (tl.constexpr): The size of the block to be processed by each program instance.\n\n    Returns:\n        None\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    x = tl.load(x_ptr + offs).to(tl.float32)\n    s = tl.max(tl.abs(x)) / 448.\n    y = x / s\n    y = y.to(y_ptr.dtype.element_ty)\n    tl.store(y_ptr + offs, y)\n    tl.store(s_ptr + pid, s)",
    "Quantizes the input tensor `x_ptr` and stores the result in `y_ptr` and the scaling factor in `s_ptr`.\n\nArgs:\n    x_ptr (triton.Pointer): Pointer to the input tensor.\n    y_ptr (triton.Pointer): Pointer to the output tensor where quantized values will be stored.\n    s_ptr (triton.Pointer): Pointer to the output tensor where scaling factors will be stored.\n    BLOCK_SIZE (tl.constexpr): The size of the block to be processed by each program instance.\n\nReturns:\n    None"
  ],
  [
    "def act_quant(x: torch.Tensor, block_size: int = 128) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Quantizes the input tensor `x` using block-wise quantization.\n\n    Args:\n        x (torch.Tensor): The input tensor to be quantized. Must be contiguous and its last dimension size must be divisible by `block_size`.\n        block_size (int, optional): The size of the blocks to be used for quantization. Default is 128.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n            - The quantized tensor with dtype `torch.float8_e4m3fn`.\n            - A tensor of scaling factors with dtype `torch.float32`.\n    \"\"\"\n    assert x.is_contiguous(), 'Input tensor must be contiguous'\n    assert x.size(-1) % block_size == 0, f'Last dimension size must be divisible by block_size (block_size={block_size})'\n    y = torch.empty_like(x, dtype=torch.float8_e4m3fn)\n    s = x.new_empty(*x.size()[:-1], x.size(-1) // block_size, dtype=torch.float32)\n    grid = lambda meta: (triton.cdiv(x.numel(), meta['BLOCK_SIZE']), )\n    act_quant_kernel[grid](x, y, s, BLOCK_SIZE=block_size)\n    return y, s",
    "Quantizes the input tensor `x` using block-wise quantization.\n\nArgs:\n    x (torch.Tensor): The input tensor to be quantized. Must be contiguous and its last dimension size must be divisible by `block_size`.\n    block_size (int, optional): The size of the blocks to be used for quantization. Default is 128.\n\nReturns:\n    Tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n        - The quantized tensor with dtype `torch.float8_e4m3fn`.\n        - A tensor of scaling factors with dtype `torch.float32`."
  ],
  [
    "def weight_dequant_kernel(x_ptr, s_ptr, y_ptr, M, N, BLOCK_SIZE: tl.constexpr):\n    \"\"\"\n    Dequantizes weights using the provided scaling factors and stores the result.\n\n    Args:\n        x_ptr (tl.pointer): Pointer to the quantized weights.\n        s_ptr (tl.pointer): Pointer to the scaling factors.\n        y_ptr (tl.pointer): Pointer to the output buffer for dequantized weights.\n        M (int): Number of rows in the weight matrix.\n        N (int): Number of columns in the weight matrix.\n        BLOCK_SIZE (tl.constexpr): Size of the block for tiling.\n\n    Returns:\n        None\n    \"\"\"\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n    n = tl.cdiv(N, BLOCK_SIZE)\n    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_n = pid_n * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs = offs_m[:, None] * N + offs_n[None, :]\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    x = tl.load(x_ptr + offs, mask=mask).to(tl.float32)\n    s = tl.load(s_ptr + pid_m * n + pid_n)\n    y = x * s\n    tl.store(y_ptr + offs, y, mask=mask)",
    "Dequantizes weights using the provided scaling factors and stores the result.\n\nArgs:\n    x_ptr (tl.pointer): Pointer to the quantized weights.\n    s_ptr (tl.pointer): Pointer to the scaling factors.\n    y_ptr (tl.pointer): Pointer to the output buffer for dequantized weights.\n    M (int): Number of rows in the weight matrix.\n    N (int): Number of columns in the weight matrix.\n    BLOCK_SIZE (tl.constexpr): Size of the block for tiling.\n\nReturns:\n    None"
  ],
  [
    "def weight_dequant(x: torch.Tensor, s: torch.Tensor, block_size: int = 128) -> torch.Tensor:\n    \"\"\"\n    Dequantizes the given weight tensor using the provided scale tensor.\n\n    Args:\n        x (torch.Tensor): The quantized weight tensor of shape (M, N).\n        s (torch.Tensor): The scale tensor of shape (M//block_size, N//block_size).\n        block_size (int, optional): The block size to use for dequantization. Defaults to 128.\n\n    Returns:\n        torch.Tensor: The dequantized weight tensor of the same shape as `x`.\n\n    Raises:\n        AssertionError: If `x` or `s` are not contiguous or if their dimensions are not 2.\n    \"\"\"\n    assert x.is_contiguous() and s.is_contiguous(), 'Input tensors must be contiguous'\n    assert x.dim() == 2 and s.dim() == 2, 'Input tensors must have 2 dimensions'\n    M, N = x.size()\n    y = torch.empty_like(x, dtype=torch.get_default_dtype())\n    grid = lambda meta: (triton.cdiv(M, meta['BLOCK_SIZE']), triton.cdiv(N, meta['BLOCK_SIZE']))\n    weight_dequant_kernel[grid](x, s, y, M, N, BLOCK_SIZE=block_size)\n    return y",
    "Dequantizes the given weight tensor using the provided scale tensor.\n\nArgs:\n    x (torch.Tensor): The quantized weight tensor of shape (M, N).\n    s (torch.Tensor): The scale tensor of shape (M//block_size, N//block_size).\n    block_size (int, optional): The block size to use for dequantization. Defaults to 128.\n\nReturns:\n    torch.Tensor: The dequantized weight tensor of the same shape as `x`.\n\nRaises:\n    AssertionError: If `x` or `s` are not contiguous or if their dimensions are not 2."
  ],
  [
    "def fp8_gemm_kernel(a_ptr, b_ptr, c_ptr,\n                    a_s_ptr, b_s_ptr,\n                    M, N: tl.constexpr, K: tl.constexpr,\n                    BLOCK_SIZE_M: tl.constexpr,\n                    BLOCK_SIZE_N: tl.constexpr,\n                    BLOCK_SIZE_K: tl.constexpr):\n    \"\"\"\n    Performs a matrix multiplication operation on FP8 matrices with scaling factors.\n\n    Args:\n        a_ptr (tl.tensor): Pointer to the first input matrix A.\n        b_ptr (tl.tensor): Pointer to the second input matrix B.\n        c_ptr (tl.tensor): Pointer to the output matrix C.\n        a_s_ptr (tl.tensor): Pointer to the scaling factors for matrix A.\n        b_s_ptr (tl.tensor): Pointer to the scaling factors for matrix B.\n        M (int): Number of rows in matrix A and C.\n        N (tl.constexpr): Number of columns in matrix B and C.\n        K (tl.constexpr): Number of columns in matrix A and rows in matrix B.\n        BLOCK_SIZE_M (tl.constexpr): Block size for the M dimension.\n        BLOCK_SIZE_N (tl.constexpr): Block size for the N dimension.\n        BLOCK_SIZE_K (tl.constexpr): Block size for the K dimension.\n\n    Returns:\n        None\n    \"\"\"\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n    k = tl.cdiv(K, BLOCK_SIZE_K)\n    offs_m = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_n = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + offs_m[:, None] * K + offs_k[None, :]\n    b_ptrs = b_ptr + offs_n[None, :] * K + offs_k[:, None]\n    a_s_ptrs = a_s_ptr + offs_m * k\n    b_s_ptrs = b_s_ptr + (offs_n // BLOCK_SIZE_K) * k\n\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(k):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - i * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - i * BLOCK_SIZE_K, other=0.0)\n        a_s = tl.load(a_s_ptrs)\n        b_s = tl.load(b_s_ptrs)\n        accumulator += tl.dot(a, b) * a_s[:, None] * b_s[None, :]\n        a_ptrs += BLOCK_SIZE_K\n        b_ptrs += BLOCK_SIZE_K\n        a_s_ptrs += 1\n        b_s_ptrs += 1\n    c = accumulator.to(c_ptr.dtype.element_ty)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + offs_m[:, None] * N + offs_n[None, :]\n    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(c_ptrs, c, mask=mask)",
    "Performs a matrix multiplication operation on FP8 matrices with scaling factors.\n\nArgs:\n    a_ptr (tl.tensor): Pointer to the first input matrix A.\n    b_ptr (tl.tensor): Pointer to the second input matrix B.\n    c_ptr (tl.tensor): Pointer to the output matrix C.\n    a_s_ptr (tl.tensor): Pointer to the scaling factors for matrix A.\n    b_s_ptr (tl.tensor): Pointer to the scaling factors for matrix B.\n    M (int): Number of rows in matrix A and C.\n    N (tl.constexpr): Number of columns in matrix B and C.\n    K (tl.constexpr): Number of columns in matrix A and rows in matrix B.\n    BLOCK_SIZE_M (tl.constexpr): Block size for the M dimension.\n    BLOCK_SIZE_N (tl.constexpr): Block size for the N dimension.\n    BLOCK_SIZE_K (tl.constexpr): Block size for the K dimension.\n\nReturns:\n    None"
  ],
  [
    "def fp8_gemm(a: torch.Tensor, a_s: torch.Tensor, b: torch.Tensor, b_s: torch.Tensor):\n    \"\"\"\n    Perform a matrix multiplication using FP8 precision.\n\n    Args:\n        a (torch.Tensor): The first input matrix, must be contiguous.\n        a_s (torch.Tensor): The scaling factor for the first input matrix, must be contiguous.\n        b (torch.Tensor): The second input matrix, must be contiguous.\n        b_s (torch.Tensor): The scaling factor for the second input matrix, must be contiguous.\n\n    Returns:\n        torch.Tensor: The result of the matrix multiplication.\n    \"\"\"\n    assert a.is_contiguous() and b.is_contiguous(), 'Input tensors must be contiguous'\n    assert a_s.is_contiguous() and b_s.is_contiguous(), 'Scaling factor tensors must be contiguous'\n    K = a.size(-1)\n    M = a.numel() // K\n    N = b.size(0)\n    c = a.new_empty(*a.size()[:-1], N, dtype=torch.get_default_dtype())\n    grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']), triton.cdiv(N, META['BLOCK_SIZE_N']))\n    fp8_gemm_kernel[grid](a, b, c, a_s, b_s, M, N, K)\n    return c",
    "Perform a matrix multiplication using FP8 precision.\n\nArgs:\n    a (torch.Tensor): The first input matrix, must be contiguous.\n    a_s (torch.Tensor): The scaling factor for the first input matrix, must be contiguous.\n    b (torch.Tensor): The second input matrix, must be contiguous.\n    b_s (torch.Tensor): The scaling factor for the second input matrix, must be contiguous.\n\nReturns:\n    torch.Tensor: The result of the matrix multiplication."
  ],
  [
    "class ModelArgs:\n    \"\"\"\n    Data class for defining model arguments and hyperparameters.\n\n    Attributes:\n        max_batch_size (int): Maximum batch size.\n        max_seq_len (int): Maximum sequence length.\n        dtype (Literal[\"bf16\", \"fp8\"]): Data type for computations.\n        vocab_size (int): Vocabulary size.\n        dim (int): Model dimension.\n        inter_dim (int): Intermediate dimension for MLP layers.\n        moe_inter_dim (int): Intermediate dimension for MoE layers.\n        n_layers (int): Number of transformer layers.\n        n_dense_layers (int): Number of dense layers in the model.\n        n_heads (int): Number of attention heads.\n        n_routed_experts (int): Number of routed experts for MoE layers.\n        n_shared_experts (int): Number of shared experts for MoE layers.\n        n_activated_experts (int): Number of activated experts in MoE layers.\n        n_expert_groups (int): Number of expert groups.\n        n_limited_groups (int): Number of limited groups for MoE routing.\n        score_func (Literal[\"softmax\", \"sigmoid\"]): Scoring function for MoE routing.\n        route_scale (float): Scaling factor for routing scores.\n        q_lora_rank (int): LoRA rank for query projections.\n        kv_lora_rank (int): LoRA rank for key-value projections.\n        qk_nope_head_dim (int): Dimension for query-key projections without positional embeddings.\n        qk_rope_head_dim (int): Dimension for query-key projections with rotary embeddings.\n        v_head_dim (int): Dimension for value projections.\n        original_seq_len (int): Original sequence length.\n        rope_theta (float): Base for rotary positional encoding.\n        rope_factor (float): Scaling factor for extended sequence lengths.\n        beta_fast (int): Fast beta correction factor.\n        beta_slow (int): Slow beta correction factor.\n        mscale (float): Scaling factor for extended attention.\n    \"\"\"\n    max_batch_size: int = 8\n    max_seq_len: int = 4096 * 4\n    dtype: Literal[\"bf16\", \"fp8\"] = \"bf16\"\n    vocab_size: int = 102400\n    dim: int = 2048\n    inter_dim: int = 10944\n    moe_inter_dim: int = 1408\n    n_layers: int = 27\n    n_dense_layers: int = 1\n    n_heads: int = 16\n    # moe\n    n_routed_experts: int = 64\n    n_shared_experts: int = 2\n    n_activated_experts: int = 6\n    n_expert_groups: int = 1\n    n_limited_groups: int = 1\n    score_func: Literal[\"softmax\", \"sigmoid\"] = \"softmax\"\n    route_scale: float = 1.\n    # mla\n    q_lora_rank: int = 0\n    kv_lora_rank: int = 512\n    qk_nope_head_dim: int = 128\n    qk_rope_head_dim: int = 64\n    v_head_dim: int = 128\n    # yarn\n    original_seq_len: int = 4096\n    rope_theta: float = 10000.0\n    rope_factor: float = 40\n    beta_fast: int = 32\n    beta_slow: int = 1\n    mscale: float = 1.",
    "Data class for defining model arguments and hyperparameters.\n\nAttributes:\n    max_batch_size (int): Maximum batch size.\n    max_seq_len (int): Maximum sequence length.\n    dtype (Literal[\"bf16\", \"fp8\"]): Data type for computations.\n    vocab_size (int): Vocabulary size.\n    dim (int): Model dimension.\n    inter_dim (int): Intermediate dimension for MLP layers.\n    moe_inter_dim (int): Intermediate dimension for MoE layers.\n    n_layers (int): Number of transformer layers.\n    n_dense_layers (int): Number of dense layers in the model.\n    n_heads (int): Number of attention heads.\n    n_routed_experts (int): Number of routed experts for MoE layers.\n    n_shared_experts (int): Number of shared experts for MoE layers.\n    n_activated_experts (int): Number of activated experts in MoE layers.\n    n_expert_groups (int): Number of expert groups.\n    n_limited_groups (int): Number of limited groups for MoE routing.\n    score_func (Literal[\"softmax\", \"sigmoid\"]): Scoring function for MoE routing.\n    route_scale (float): Scaling factor for routing scores.\n    q_lora_rank (int): LoRA rank for query projections.\n    kv_lora_rank (int): LoRA rank for key-value projections.\n    qk_nope_head_dim (int): Dimension for query-key projections without positional embeddings.\n    qk_rope_head_dim (int): Dimension for query-key projections with rotary embeddings.\n    v_head_dim (int): Dimension for value projections.\n    original_seq_len (int): Original sequence length.\n    rope_theta (float): Base for rotary positional encoding.\n    rope_factor (float): Scaling factor for extended sequence lengths.\n    beta_fast (int): Fast beta correction factor.\n    beta_slow (int): Slow beta correction factor.\n    mscale (float): Scaling factor for extended attention."
  ],
  [
    "class ParallelEmbedding(nn.Module):\n    \"\"\"\n    Embedding layer with parallelism support across distributed processes.\n\n    Args:\n        vocab_size (int): Vocabulary size.\n        dim (int): Embedding dimension.\n    \"\"\"\n    def __init__(self, vocab_size: int, dim: int):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.dim = dim\n        assert vocab_size % world_size == 0, f\"Vocabulary size must be divisible by world size (world_size={world_size})\"\n        self.part_vocab_size = (vocab_size // world_size)\n        self.vocab_start_idx = rank * self.part_vocab_size\n        self.vocab_end_idx = self.vocab_start_idx + self.part_vocab_size\n        self.weight = nn.Parameter(torch.empty(self.part_vocab_size, self.dim))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for parallel embedding layer.\n\n        Args:\n            x (torch.Tensor): Input tensor containing token indices.\n\n        Returns:\n            torch.Tensor: Embedded representations.\n\n        Raises:\n            ValueError: If `world_size` is not defined.\n        \"\"\"\n        if world_size > 1:\n            mask = (x < self.vocab_start_idx) | (x >= self.vocab_end_idx)\n            x = x - self.vocab_start_idx\n            x[mask] = 0\n        y = F.embedding(x, self.weight)\n        if world_size > 1:\n            y[mask] = 0\n            dist.all_reduce(y)\n        return y",
    "Embedding layer with parallelism support across distributed processes.\n\nArgs:\n    vocab_size (int): Vocabulary size.\n    dim (int): Embedding dimension."
  ],
  [
    "def linear(x: torch.Tensor, weight: torch.Tensor, bias: Optional[torch.Tensor] = None) -> torch.Tensor:\n    \"\"\"\n    Applies a linear transformation to the incoming data: y = xA^T + b.\n    This function supports specialized implementations based on quantization\n    and tensor formats.\n\n    Args:\n        x (torch.Tensor): The input tensor.\n        weight (torch.Tensor): The weight tensor. It may be quantized and \n            requires dequantization for certain cases.\n        bias (Optional[torch.Tensor]): The bias tensor to be added. Default is None.\n\n    Returns:\n        torch.Tensor: The result of the linear transformation, which may involve \n        quantization-aware computations depending on the input parameters.\n\n    Notes:\n        - If `weight` is quantized (e.g., `element_size() == 1`), a dequantized version \n          is used for computation.\n        - If `gemm_impl == \"bf16\"`, dequantization and a `bf16` GEMM operation are applied.\n        - For other cases, the function applies quantization to `x` and uses `fp8_gemm` for computation.\n    \"\"\"\n    if weight.element_size() > 1:\n        return F.linear(x, weight, bias)\n    elif gemm_impl == \"bf16\":\n        weight = weight_dequant(weight, weight.scale)\n        return F.linear(x, weight, bias)\n    else:\n        x, scale = act_quant(x, block_size)\n        y = fp8_gemm(x, scale, weight, weight.scale)\n        if bias is not None:\n            y += bias\n        return y",
    "Applies a linear transformation to the incoming data: y = xA^T + b.\nThis function supports specialized implementations based on quantization\nand tensor formats.\n\nArgs:\n    x (torch.Tensor): The input tensor.\n    weight (torch.Tensor): The weight tensor. It may be quantized and \n        requires dequantization for certain cases.\n    bias (Optional[torch.Tensor]): The bias tensor to be added. Default is None.\n\nReturns:\n    torch.Tensor: The result of the linear transformation, which may involve \n    quantization-aware computations depending on the input parameters.\n\nNotes:\n    - If `weight` is quantized (e.g., `element_size() == 1`), a dequantized version \n      is used for computation.\n    - If `gemm_impl == \"bf16\"`, dequantization and a `bf16` GEMM operation are applied.\n    - For other cases, the function applies quantization to `x` and uses `fp8_gemm` for computation."
  ],
  [
    "class Linear(nn.Module):\n    \"\"\"\n    Custom linear layer with support for quantized weights and optional bias.\n\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Number of output features.\n        bias (bool): Whether to include a bias term. Defaults to False.\n        dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`.\n    \"\"\"\n    dtype = torch.bfloat16\n\n    def __init__(self, in_features: int, out_features: int, bias: bool = False, dtype = None):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.empty(out_features, in_features, dtype=dtype or Linear.dtype))\n        if self.weight.element_size() == 1:\n            scale_out_features = (out_features + block_size - 1) // block_size\n            scale_in_features = (in_features + block_size - 1) // block_size\n            self.weight.scale = self.scale = nn.Parameter(torch.empty(scale_out_features, scale_in_features, dtype=torch.float32))\n        else:\n            self.register_parameter(\"scale\", None)\n        if bias:\n            self.bias = nn.Parameter(torch.empty(out_features))\n        else:\n            self.register_parameter(\"bias\", None)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the custom linear layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Transformed tensor after linear computation.\n        \"\"\"\n        return linear(x, self.weight, self.bias)",
    "Custom linear layer with support for quantized weights and optional bias.\n\nArgs:\n    in_features (int): Number of input features.\n    out_features (int): Number of output features.\n    bias (bool): Whether to include a bias term. Defaults to False.\n    dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`."
  ],
  [
    "class ColumnParallelLinear(Linear):\n    \"\"\"\n    Linear layer with column parallelism, splitting output features across distributed processes.\n\n    Args:\n        in_features (int): Number of input features.\n        out_features (int): Total number of output features.\n        bias (bool): Whether to include a bias term. Defaults to False.\n        dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`.\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int, bias: bool = False, dtype = None):\n        assert out_features % world_size == 0, f\"Output features must be divisible by world size (world_size={world_size})\"\n        self.part_out_features = out_features // world_size\n        super().__init__(in_features, self.part_out_features, bias, dtype)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for column parallel linear layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Transformed tensor with column-parallel computation.\n        \"\"\"\n        y = linear(x, self.weight, self.bias)\n        return y",
    "Linear layer with column parallelism, splitting output features across distributed processes.\n\nArgs:\n    in_features (int): Number of input features.\n    out_features (int): Total number of output features.\n    bias (bool): Whether to include a bias term. Defaults to False.\n    dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`."
  ],
  [
    "class RowParallelLinear(Linear):\n    \"\"\"\n    Linear layer with row parallelism, splitting input features across distributed processes.\n\n    Args:\n        in_features (int): Total number of input features.\n        out_features (int): Number of output features.\n        bias (bool): Whether to include a bias term. Defaults to False.\n        dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`.\n    \"\"\"\n    def __init__(self, in_features: int, out_features: int, bias: bool = False, dtype = None):\n        assert in_features % world_size == 0, f\"Input features must be divisible by world size (world_size={world_size})\"\n        self.part_in_features = in_features // world_size\n        super().__init__(self.part_in_features, out_features, bias, dtype)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for row parallel linear layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Transformed tensor with row-parallel computation.\n        \"\"\"\n        y = linear(x, self.weight)\n        if world_size > 1:\n            dist.all_reduce(y)\n        if self.bias is not None:\n            y += self.bias\n        return y",
    "Linear layer with row parallelism, splitting input features across distributed processes.\n\nArgs:\n    in_features (int): Total number of input features.\n    out_features (int): Number of output features.\n    bias (bool): Whether to include a bias term. Defaults to False.\n    dtype (optional): Data type for the layer. Defaults to `torch.bfloat16`."
  ],
  [
    "class RMSNorm(nn.Module):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    Args:\n        dim (int): Dimension of the input tensor.\n        eps (float): Epsilon value for numerical stability. Defaults to 1e-6.\n    \"\"\"\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.dim = dim\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        Forward pass for RMSNorm.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Normalized tensor with the same shape as input.\n        \"\"\"\n        return F.rms_norm(x, (self.dim,), self.weight, self.eps)",
    "Root Mean Square Layer Normalization (RMSNorm).\n\nArgs:\n    dim (int): Dimension of the input tensor.\n    eps (float): Epsilon value for numerical stability. Defaults to 1e-6."
  ],
  [
    "def precompute_freqs_cis(args: ModelArgs) -> torch.Tensor:\n    \"\"\"\n    Precomputes frequency-based complex exponential values for rotary positional embeddings.\n\n    Args:\n        args (ModelArgs): Model arguments containing positional embedding parameters.\n\n    Returns:\n        torch.Tensor: Precomputed complex exponential values for positional embeddings.\n    \"\"\"\n    dim = args.qk_rope_head_dim\n    seqlen = args.max_seq_len\n    beta_fast = args.beta_fast\n    beta_slow = args.beta_slow\n    base = args.rope_theta\n    factor = args.rope_factor\n\n    def find_correction_dim(num_rotations, dim, base, max_seq_len):\n        \"\"\"\n        Computes the correction dimension for a given number of rotations in the rotary positional embedding.\n\n        Args:\n            num_rotations (float): Number of rotations to compute the correction for.\n            dim (int): Dimensionality of the embedding space.\n            base (float): Base value for the exponential computation.\n            max_seq_len (int): Maximum sequence length.\n\n        Returns:\n            float: The correction dimension based on the input parameters.\n        \"\"\"\n        return dim * math.log(max_seq_len / (num_rotations * 2 * math.pi)) / (2 * math.log(base))\n\n    def find_correction_range(low_rot, high_rot, dim, base, max_seq_len):\n        \"\"\"\n        Computes the range of correction dimensions for rotary positional embeddings.\n\n        Args:\n            low_rot (float): Lower bound for the number of rotations.\n            high_rot (float): Upper bound for the number of rotations.\n            dim (int): Dimensionality of the embedding space.\n            base (float): Base value for the exponential computation.\n            max_seq_len (int): Maximum sequence length.\n\n        Returns:\n            Tuple[int, int]: The range of correction dimensions (low, high), clamped to valid indices.\n        \"\"\"\n        low = math.floor(find_correction_dim(low_rot, dim, base, max_seq_len))\n        high = math.ceil(find_correction_dim(high_rot, dim, base, max_seq_len))\n        return max(low, 0), min(high, dim-1)\n\n    def linear_ramp_factor(min, max, dim):\n        \"\"\"\n        Computes a linear ramp function used to smooth values between a minimum and maximum range.\n\n        Args:\n            min (float): Minimum value for the ramp function.\n            max (float): Maximum value for the ramp function.\n            dim (int): Dimensionality of the ramp tensor.\n\n        Returns:\n            torch.Tensor: A tensor of shape (dim,) with values linearly interpolated between 0 and 1,\n                clamped to the range [0, 1].\n        \"\"\"\n        if min == max:\n            max += 0.001\n        linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n        ramp_func = torch.clamp(linear_func, 0, 1)\n        return ramp_func\n\n    freqs = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n    if seqlen > args.original_seq_len:\n        low, high = find_correction_range(beta_fast, beta_slow, dim, base, args.original_seq_len)\n        smooth = 1 - linear_ramp_factor(low, high, dim // 2)\n        freqs = freqs / factor * (1 - smooth) + freqs * smooth\n\n    t = torch.arange(seqlen)\n    freqs = torch.outer(t, freqs)\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n    return freqs_cis",
    "Precomputes frequency-based complex exponential values for rotary positional embeddings.\n\nArgs:\n    args (ModelArgs): Model arguments containing positional embedding parameters.\n\nReturns:\n    torch.Tensor: Precomputed complex exponential values for positional embeddings."
  ],
  [
    "def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Applies rotary positional embeddings to the input tensor.\n\n    Args:\n        x (torch.Tensor): Input tensor with positional embeddings to be applied.\n        freqs_cis (torch.Tensor): Precomputed complex exponential values for positional embeddings.\n\n    Returns:\n        torch.Tensor: Tensor with rotary embeddings applied.\n    \"\"\"\n    dtype = x.dtype\n    x = torch.view_as_complex(x.float().view(*x.shape[:-1], -1, 2))\n    freqs_cis = freqs_cis.view(1, x.size(1), 1, x.size(-1))\n    y = torch.view_as_real(x * freqs_cis).flatten(3)\n    return y.to(dtype)",
    "Applies rotary positional embeddings to the input tensor.\n\nArgs:\n    x (torch.Tensor): Input tensor with positional embeddings to be applied.\n    freqs_cis (torch.Tensor): Precomputed complex exponential values for positional embeddings.\n\nReturns:\n    torch.Tensor: Tensor with rotary embeddings applied."
  ],
  [
    "class MLA(nn.Module):\n    \"\"\"\n    Multi-Head Latent Attention (MLA) Layer.\n\n    Attributes:\n        dim (int): Dimensionality of the input features.\n        n_heads (int): Number of attention heads.\n        n_local_heads (int): Number of local attention heads for distributed systems.\n        q_lora_rank (int): Rank for low-rank query projection.\n        kv_lora_rank (int): Rank for low-rank key/value projection.\n        qk_nope_head_dim (int): Dimensionality of non-positional query/key projections.\n        qk_rope_head_dim (int): Dimensionality of rotary-positional query/key projections.\n        qk_head_dim (int): Total dimensionality of query/key projections.\n        v_head_dim (int): Dimensionality of value projections.\n        softmax_scale (float): Scaling factor for softmax in attention computation.\n    \"\"\"\n    def __init__(self, args: ModelArgs):\n        super().__init__()\n        self.dim = args.dim\n        self.n_heads = args.n_heads\n        self.n_local_heads = args.n_heads // world_size\n        self.q_lora_rank = args.q_lora_rank\n        self.kv_lora_rank = args.kv_lora_rank\n        self.qk_nope_head_dim = args.qk_nope_head_dim\n        self.qk_rope_head_dim = args.qk_rope_head_dim\n        self.qk_head_dim = args.qk_nope_head_dim + args.qk_rope_head_dim\n        self.v_head_dim = args.v_head_dim\n\n        if self.q_lora_rank == 0:\n            self.wq = ColumnParallelLinear(self.dim, self.n_heads * self.qk_head_dim)\n        else:\n            self.wq_a = Linear(self.dim, self.q_lora_rank)\n            self.q_norm = RMSNorm(self.q_lora_rank)\n            self.wq_b = ColumnParallelLinear(self.q_lora_rank, self.n_heads * self.qk_head_dim)\n        self.wkv_a = Linear(self.dim, self.kv_lora_rank + self.qk_rope_head_dim)\n        self.kv_norm = RMSNorm(self.kv_lora_rank)\n        self.wkv_b = ColumnParallelLinear(self.kv_lora_rank, self.n_heads * (self.qk_nope_head_dim + self.v_head_dim))\n        self.wo = RowParallelLinear(self.n_heads * self.v_head_dim, self.dim)\n        self.softmax_scale = self.qk_head_dim ** -0.5\n        if args.max_seq_len > args.original_seq_len:\n            mscale = 0.1 * args.mscale * math.log(args.rope_factor) + 1.0\n            self.softmax_scale = self.softmax_scale * mscale * mscale\n\n        if attn_impl == \"naive\":\n            self.register_buffer(\"k_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.qk_head_dim), persistent=False)\n            self.register_buffer(\"v_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.n_local_heads, self.v_head_dim), persistent=False)\n        else:\n            self.register_buffer(\"kv_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.kv_lora_rank), persistent=False)\n            self.register_buffer(\"pe_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.qk_rope_head_dim), persistent=False)\n\n    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n        \"\"\"\n        Forward pass for the Multi-Head Latent Attention (MLA) Layer.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim).\n            start_pos (int): Starting position in the sequence for caching.\n            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.\n\n        Returns:\n            torch.Tensor: Output tensor with the same shape as the input.\n        \"\"\"\n        bsz, seqlen, _ = x.size()\n        end_pos = start_pos + seqlen\n        if self.q_lora_rank == 0:\n            q = self.wq(x)\n        else:\n            q = self.wq_b(self.q_norm(self.wq_a(x)))\n        q = q.view(bsz, seqlen, self.n_local_heads, self.qk_head_dim)\n        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n        q_pe = apply_rotary_emb(q_pe, freqs_cis)\n        kv = self.wkv_a(x)\n        kv, k_pe = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis)\n        if attn_impl == \"naive\":\n            q = torch.cat([q_nope, q_pe], dim=-1)\n            kv = self.wkv_b(self.kv_norm(kv))\n            kv = kv.view(bsz, seqlen, self.n_local_heads, self.qk_nope_head_dim + self.v_head_dim)\n            k_nope, v = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n            k = torch.cat([k_nope, k_pe.expand(-1, -1, self.n_local_heads, -1)], dim=-1)\n            self.k_cache[:bsz, start_pos:end_pos] = k\n            self.v_cache[:bsz, start_pos:end_pos] = v\n            scores = torch.einsum(\"bshd,bthd->bsht\", q, self.k_cache[:bsz, :end_pos]) * self.softmax_scale\n        else:\n            wkv_b = self.wkv_b.weight if self.wkv_b.scale is None else weight_dequant(self.wkv_b.weight, self.wkv_b.scale, block_size) \n            wkv_b = wkv_b.view(self.n_local_heads, -1, self.kv_lora_rank)\n            q_nope = torch.einsum(\"bshd,hdc->bshc\", q_nope, wkv_b[:, :self.qk_nope_head_dim])\n            self.kv_cache[:bsz, start_pos:end_pos] = self.kv_norm(kv)\n            self.pe_cache[:bsz, start_pos:end_pos] = k_pe.squeeze(2)\n            scores = (torch.einsum(\"bshc,btc->bsht\", q_nope, self.kv_cache[:bsz, :end_pos]) +\n                      torch.einsum(\"bshr,btr->bsht\", q_pe, self.pe_cache[:bsz, :end_pos])) * self.softmax_scale\n        if mask is not None:\n            scores += mask.unsqueeze(1)\n        scores = scores.softmax(dim=-1, dtype=torch.float32).type_as(x)\n        if attn_impl == \"naive\":\n            x = torch.einsum(\"bsht,bthd->bshd\", scores, self.v_cache[:bsz, :end_pos])\n        else:\n            x = torch.einsum(\"bsht,btc->bshc\", scores, self.kv_cache[:bsz, :end_pos])\n            x = torch.einsum(\"bshc,hdc->bshd\", x, wkv_b[:, -self.v_head_dim:])\n        x = self.wo(x.flatten(2))\n        return x",
    "Multi-Head Latent Attention (MLA) Layer.\n\nAttributes:\n    dim (int): Dimensionality of the input features.\n    n_heads (int): Number of attention heads.\n    n_local_heads (int): Number of local attention heads for distributed systems.\n    q_lora_rank (int): Rank for low-rank query projection.\n    kv_lora_rank (int): Rank for low-rank key/value projection.\n    qk_nope_head_dim (int): Dimensionality of non-positional query/key projections.\n    qk_rope_head_dim (int): Dimensionality of rotary-positional query/key projections.\n    qk_head_dim (int): Total dimensionality of query/key projections.\n    v_head_dim (int): Dimensionality of value projections.\n    softmax_scale (float): Scaling factor for softmax in attention computation."
  ],
  [
    "class MLP(nn.Module):\n    \"\"\"\n    Multi-Layer Perceptron (MLP) used as a feed-forward layer.\n\n    Attributes:\n        w1 (nn.Module): Linear layer for input-to-hidden transformation.\n        w2 (nn.Module): Linear layer for hidden-to-output transformation.\n        w3 (nn.Module): Additional linear layer for feature transformation.\n    \"\"\"\n    def __init__(self, dim: int, inter_dim: int):\n        \"\"\"\n        Initializes the MLP layer.\n\n        Args:\n            dim (int): Input and output dimensionality.\n            inter_dim (int): Hidden layer dimensionality.\n        \"\"\"\n        super().__init__()\n        self.w1 = ColumnParallelLinear(dim, inter_dim)\n        self.w2 = RowParallelLinear(inter_dim, dim)\n        self.w3 = ColumnParallelLinear(dim, inter_dim)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the MLP layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after MLP computation.\n        \"\"\"\n        return self.w2(F.silu(self.w1(x)) * self.w3(x))",
    "Multi-Layer Perceptron (MLP) used as a feed-forward layer.\n\nAttributes:\n    w1 (nn.Module): Linear layer for input-to-hidden transformation.\n    w2 (nn.Module): Linear layer for hidden-to-output transformation.\n    w3 (nn.Module): Additional linear layer for feature transformation."
  ],
  [
    "class Gate(nn.Module):\n    \"\"\"\n    Gating mechanism for routing inputs in a mixture-of-experts (MoE) model.\n\n    Attributes:\n        dim (int): Dimensionality of input features.\n        topk (int): Number of top experts activated for each input.\n        n_groups (int): Number of groups for routing.\n        topk_groups (int): Number of groups to route inputs to.\n        score_func (str): Scoring function ('softmax' or 'sigmoid').\n        route_scale (float): Scaling factor for routing weights.\n        weight (torch.nn.Parameter): Learnable weights for the gate.\n        bias (Optional[torch.nn.Parameter]): Optional bias term for the gate.\n    \"\"\"\n    def __init__(self, args: ModelArgs):\n        \"\"\"\n        Initializes the Gate module.\n\n        Args:\n            args (ModelArgs): Model arguments containing gating parameters.\n        \"\"\"\n        super().__init__()\n        self.dim = args.dim\n        self.topk = args.n_activated_experts\n        self.n_groups = args.n_expert_groups\n        self.topk_groups = args.n_limited_groups\n        self.score_func = args.score_func\n        self.route_scale = args.route_scale\n        self.weight = nn.Parameter(torch.empty(args.n_routed_experts, args.dim))\n        self.bias = nn.Parameter(torch.empty(args.n_routed_experts)) if self.dim == 7168 else None\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass for the gating mechanism.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]: Routing weights and selected expert indices.\n        \"\"\"\n        scores = linear(x, self.weight)\n        if self.score_func == \"softmax\":\n            scores = scores.softmax(dim=-1, dtype=torch.float32)\n        else:\n            scores = scores.sigmoid()\n        original_scores = scores\n        if self.bias is not None:\n            scores = scores + self.bias\n        if self.n_groups > 1:\n            scores = scores.view(x.size(0), self.n_groups, -1)\n            if self.bias is None:\n                group_scores = scores.amax(dim=-1)\n            else:\n                group_scores = scores.topk(2, dim=-1)[0].sum(dim=-1)\n            indices = group_scores.topk(self.topk_groups, dim=-1)[1]\n            mask = scores.new_ones(x.size(0), self.n_groups, dtype=bool).scatter_(1, indices, False)\n            scores = scores.masked_fill_(mask.unsqueeze(-1), float(\"-inf\")).flatten(1)\n        indices = torch.topk(scores, self.topk, dim=-1)[1]\n        weights = original_scores.gather(1, indices)\n        if self.score_func == \"sigmoid\":\n            weights /= weights.sum(dim=-1, keepdim=True)\n        weights *= self.route_scale\n        return weights.type_as(x), indices",
    "Gating mechanism for routing inputs in a mixture-of-experts (MoE) model.\n\nAttributes:\n    dim (int): Dimensionality of input features.\n    topk (int): Number of top experts activated for each input.\n    n_groups (int): Number of groups for routing.\n    topk_groups (int): Number of groups to route inputs to.\n    score_func (str): Scoring function ('softmax' or 'sigmoid').\n    route_scale (float): Scaling factor for routing weights.\n    weight (torch.nn.Parameter): Learnable weights for the gate.\n    bias (Optional[torch.nn.Parameter]): Optional bias term for the gate."
  ],
  [
    "class Expert(nn.Module):\n    \"\"\"\n    Expert layer for Mixture-of-Experts (MoE) models.\n\n    Attributes:\n        w1 (nn.Module): Linear layer for input-to-hidden transformation.\n        w2 (nn.Module): Linear layer for hidden-to-output transformation.\n        w3 (nn.Module): Additional linear layer for feature transformation.\n    \"\"\"\n    def __init__(self, dim: int, inter_dim: int):\n        \"\"\"\n        Initializes the Expert layer.\n\n        Args:\n            dim (int): Input and output dimensionality.\n            inter_dim (int): Hidden layer dimensionality.\n        \"\"\"\n        super().__init__()\n        self.w1 = Linear(dim, inter_dim)\n        self.w2 = Linear(inter_dim, dim)\n        self.w3 = Linear(dim, inter_dim)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the Expert layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after expert computation.\n        \"\"\"\n        return self.w2(F.silu(self.w1(x)) * self.w3(x))",
    "Expert layer for Mixture-of-Experts (MoE) models.\n\nAttributes:\n    w1 (nn.Module): Linear layer for input-to-hidden transformation.\n    w2 (nn.Module): Linear layer for hidden-to-output transformation.\n    w3 (nn.Module): Additional linear layer for feature transformation."
  ],
  [
    "class MoE(nn.Module):\n    \"\"\"\n    Mixture-of-Experts (MoE) module.\n\n    Attributes:\n        dim (int): Dimensionality of input features.\n        n_routed_experts (int): Total number of experts in the model.\n        n_local_experts (int): Number of experts handled locally in distributed systems.\n        n_activated_experts (int): Number of experts activated for each input.\n        gate (nn.Module): Gating mechanism to route inputs to experts.\n        experts (nn.ModuleList): List of expert modules.\n        shared_experts (nn.Module): Shared experts applied to all inputs.\n    \"\"\"\n    def __init__(self, args: ModelArgs):\n        \"\"\"\n        Initializes the MoE module.\n\n        Args:\n            args (ModelArgs): Model arguments containing MoE parameters.\n        \"\"\"\n        super().__init__()\n        self.dim = args.dim\n        assert args.n_routed_experts % world_size == 0, f\"Number of experts must be divisible by world size (world_size={world_size})\"\n        self.n_routed_experts = args.n_routed_experts\n        self.n_local_experts = args.n_routed_experts // world_size\n        self.n_activated_experts = args.n_activated_experts\n        self.experts_start_idx = rank * self.n_local_experts\n        self.experts_end_idx = self.experts_start_idx + self.n_local_experts\n        self.gate = Gate(args)\n        self.experts = nn.ModuleList([Expert(args.dim, args.moe_inter_dim) if self.experts_start_idx <= i < self.experts_end_idx else None\n                                      for i in range(self.n_routed_experts)])\n        self.shared_experts = MLP(args.dim, args.n_shared_experts * args.moe_inter_dim)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the MoE module.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after expert routing and computation.\n        \"\"\"\n        shape = x.size()\n        x = x.view(-1, self.dim)\n        weights, indices = self.gate(x)\n        y = torch.zeros_like(x)\n        counts = torch.bincount(indices.flatten(), minlength=self.n_routed_experts).tolist()\n        for i in range(self.experts_start_idx, self.experts_end_idx):\n            if counts[i] == 0:\n                continue\n            expert = self.experts[i]\n            idx, top = torch.where(indices == i)\n            y[idx] += expert(x[idx]) * weights[idx, top, None]\n        z = self.shared_experts(x)\n        if world_size > 1:\n            dist.all_reduce(y)\n        return (y + z).view(shape)",
    "Mixture-of-Experts (MoE) module.\n\nAttributes:\n    dim (int): Dimensionality of input features.\n    n_routed_experts (int): Total number of experts in the model.\n    n_local_experts (int): Number of experts handled locally in distributed systems.\n    n_activated_experts (int): Number of experts activated for each input.\n    gate (nn.Module): Gating mechanism to route inputs to experts.\n    experts (nn.ModuleList): List of expert modules.\n    shared_experts (nn.Module): Shared experts applied to all inputs."
  ],
  [
    "class Block(nn.Module):\n    \"\"\"\n    Transformer block combining attention and feed-forward layers.\n\n    Attributes:\n        attn (nn.Module): Attention layer (MLA).\n        ffn (nn.Module): Feed-forward network (MLP or MoE).\n        attn_norm (nn.Module): Layer normalization for attention.\n        ffn_norm (nn.Module): Layer normalization for feed-forward network.\n    \"\"\"\n    def __init__(self, layer_id: int, args: ModelArgs):\n        \"\"\"\n        Initializes the Transformer block.\n\n        Args:\n            layer_id (int): Layer index in the transformer.\n            args (ModelArgs): Model arguments containing block parameters.\n        \"\"\"\n        super().__init__()\n        self.attn = MLA(args)\n        self.ffn = MLP(args.dim, args.inter_dim) if layer_id < args.n_dense_layers else MoE(args)\n        self.attn_norm = RMSNorm(args.dim)\n        self.ffn_norm = RMSNorm(args.dim)\n\n    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the Transformer block.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n            start_pos (int): Starting position in the sequence.\n            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.\n\n        Returns:\n            torch.Tensor: Output tensor after block computation.\n        \"\"\"\n        x = x + self.attn(self.attn_norm(x), start_pos, freqs_cis, mask)\n        x = x + self.ffn(self.ffn_norm(x))\n        return x",
    "Transformer block combining attention and feed-forward layers.\n\nAttributes:\n    attn (nn.Module): Attention layer (MLA).\n    ffn (nn.Module): Feed-forward network (MLP or MoE).\n    attn_norm (nn.Module): Layer normalization for attention.\n    ffn_norm (nn.Module): Layer normalization for feed-forward network."
  ],
  [
    "class Transformer(nn.Module):\n    \"\"\"\n    Transformer model with positional embeddings, multiple layers, and output projection.\n\n    Attributes:\n        max_seq_len (int): Maximum sequence length for the transformer.\n        embed (nn.Module): Embedding layer for input tokens.\n        layers (torch.nn.ModuleList): List of transformer blocks.\n        norm (nn.Module): Layer normalization applied after all blocks.\n        head (nn.Module): Output projection layer mapping to vocabulary size.\n        freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n    \"\"\"\n    def __init__(self, args: ModelArgs):\n        \"\"\"\n        Initializes the Transformer model.\n\n        Args:\n            args (ModelArgs): Model arguments containing transformer parameters.\n        \"\"\"\n        global world_size, rank\n        world_size = dist.get_world_size() if dist.is_initialized() else 1\n        rank = dist.get_rank() if dist.is_initialized() else 0\n        Linear.dtype = torch.float8_e4m3fn if args.dtype == \"fp8\" else torch.bfloat16\n        super().__init__()\n        self.max_seq_len = args.max_seq_len\n        self.embed = ParallelEmbedding(args.vocab_size, args.dim)\n        self.layers = torch.nn.ModuleList()\n        for layer_id in range(args.n_layers):\n            self.layers.append(Block(layer_id, args))\n        self.norm = RMSNorm(args.dim)\n        self.head = ColumnParallelLinear(args.dim, args.vocab_size, dtype=torch.get_default_dtype())\n        self.register_buffer(\"freqs_cis\", precompute_freqs_cis(args), persistent=False)\n\n    @torch.inference_mode()\n    def forward(self, tokens: torch.Tensor, start_pos: int = 0):\n        \"\"\"\n        Forward pass for the Transformer model.\n\n        Args:\n            tokens (torch.Tensor): Input tensor of token IDs with shape (batch_size, seq_len).\n            start_pos (int, optional): Starting position in the sequence for rotary embeddings. Defaults to 0.\n\n        Returns:\n            torch.Tensor: Logits tensor of shape (batch_size, vocab_size).\n        \"\"\"\n        seqlen = tokens.size(1)\n        h = self.embed(tokens)\n        freqs_cis = self.freqs_cis[start_pos:start_pos+seqlen]\n        mask = None\n        if seqlen > 1:\n            mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device).triu_(1)\n        for layer in self.layers:\n            h = layer(h, start_pos, freqs_cis, mask)\n        h = self.norm(h)[:, -1]\n        logits = self.head(h)\n        if world_size > 1:\n            all_logits = [torch.empty_like(logits) for _ in range(world_size)]\n            dist.all_gather(all_logits, logits)\n            logits = torch.cat(all_logits, dim=-1)\n        return logits",
    "Transformer model with positional embeddings, multiple layers, and output projection.\n\nAttributes:\n    max_seq_len (int): Maximum sequence length for the transformer.\n    embed (nn.Module): Embedding layer for input tokens.\n    layers (torch.nn.ModuleList): List of transformer blocks.\n    norm (nn.Module): Layer normalization applied after all blocks.\n    head (nn.Module): Output projection layer mapping to vocabulary size.\n    freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings."
  ],
  [
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for parallel embedding layer.\n\n        Args:\n            x (torch.Tensor): Input tensor containing token indices.\n\n        Returns:\n            torch.Tensor: Embedded representations.\n\n        Raises:\n            ValueError: If `world_size` is not defined.\n        \"\"\"\n        if world_size > 1:\n            mask = (x < self.vocab_start_idx) | (x >= self.vocab_end_idx)\n            x = x - self.vocab_start_idx\n            x[mask] = 0\n        y = F.embedding(x, self.weight)\n        if world_size > 1:\n            y[mask] = 0\n            dist.all_reduce(y)\n        return y",
    "Forward pass for parallel embedding layer.\n\nArgs:\n    x (torch.Tensor): Input tensor containing token indices.\n\nReturns:\n    torch.Tensor: Embedded representations.\n\nRaises:\n    ValueError: If `world_size` is not defined."
  ],
  [
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the custom linear layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Transformed tensor after linear computation.\n        \"\"\"\n        return linear(x, self.weight, self.bias)",
    "Forward pass for the custom linear layer.\n\nArgs:\n    x (torch.Tensor): Input tensor.\n\nReturns:\n    torch.Tensor: Transformed tensor after linear computation."
  ],
  [
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for column parallel linear layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Transformed tensor with column-parallel computation.\n        \"\"\"\n        y = linear(x, self.weight, self.bias)\n        return y",
    "Forward pass for column parallel linear layer.\n\nArgs:\n    x (torch.Tensor): Input tensor.\n\nReturns:\n    torch.Tensor: Transformed tensor with column-parallel computation."
  ],
  [
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for row parallel linear layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Transformed tensor with row-parallel computation.\n        \"\"\"\n        y = linear(x, self.weight)\n        if world_size > 1:\n            dist.all_reduce(y)\n        if self.bias is not None:\n            y += self.bias\n        return y",
    "Forward pass for row parallel linear layer.\n\nArgs:\n    x (torch.Tensor): Input tensor.\n\nReturns:\n    torch.Tensor: Transformed tensor with row-parallel computation."
  ],
  [
    "def forward(self, x: torch.Tensor):\n        \"\"\"\n        Forward pass for RMSNorm.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Normalized tensor with the same shape as input.\n        \"\"\"\n        return F.rms_norm(x, (self.dim,), self.weight, self.eps)",
    "Forward pass for RMSNorm.\n\nArgs:\n    x (torch.Tensor): Input tensor.\n\nReturns:\n    torch.Tensor: Normalized tensor with the same shape as input."
  ],
  [
    "def find_correction_dim(num_rotations, dim, base, max_seq_len):\n        \"\"\"\n        Computes the correction dimension for a given number of rotations in the rotary positional embedding.\n\n        Args:\n            num_rotations (float): Number of rotations to compute the correction for.\n            dim (int): Dimensionality of the embedding space.\n            base (float): Base value for the exponential computation.\n            max_seq_len (int): Maximum sequence length.\n\n        Returns:\n            float: The correction dimension based on the input parameters.\n        \"\"\"\n        return dim * math.log(max_seq_len / (num_rotations * 2 * math.pi)) / (2 * math.log(base))",
    "Computes the correction dimension for a given number of rotations in the rotary positional embedding.\n\nArgs:\n    num_rotations (float): Number of rotations to compute the correction for.\n    dim (int): Dimensionality of the embedding space.\n    base (float): Base value for the exponential computation.\n    max_seq_len (int): Maximum sequence length.\n\nReturns:\n    float: The correction dimension based on the input parameters."
  ],
  [
    "def find_correction_range(low_rot, high_rot, dim, base, max_seq_len):\n        \"\"\"\n        Computes the range of correction dimensions for rotary positional embeddings.\n\n        Args:\n            low_rot (float): Lower bound for the number of rotations.\n            high_rot (float): Upper bound for the number of rotations.\n            dim (int): Dimensionality of the embedding space.\n            base (float): Base value for the exponential computation.\n            max_seq_len (int): Maximum sequence length.\n\n        Returns:\n            Tuple[int, int]: The range of correction dimensions (low, high), clamped to valid indices.\n        \"\"\"\n        low = math.floor(find_correction_dim(low_rot, dim, base, max_seq_len))\n        high = math.ceil(find_correction_dim(high_rot, dim, base, max_seq_len))\n        return max(low, 0), min(high, dim-1)",
    "Computes the range of correction dimensions for rotary positional embeddings.\n\nArgs:\n    low_rot (float): Lower bound for the number of rotations.\n    high_rot (float): Upper bound for the number of rotations.\n    dim (int): Dimensionality of the embedding space.\n    base (float): Base value for the exponential computation.\n    max_seq_len (int): Maximum sequence length.\n\nReturns:\n    Tuple[int, int]: The range of correction dimensions (low, high), clamped to valid indices."
  ],
  [
    "def linear_ramp_factor(min, max, dim):\n        \"\"\"\n        Computes a linear ramp function used to smooth values between a minimum and maximum range.\n\n        Args:\n            min (float): Minimum value for the ramp function.\n            max (float): Maximum value for the ramp function.\n            dim (int): Dimensionality of the ramp tensor.\n\n        Returns:\n            torch.Tensor: A tensor of shape (dim,) with values linearly interpolated between 0 and 1,\n                clamped to the range [0, 1].\n        \"\"\"\n        if min == max:\n            max += 0.001\n        linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n        ramp_func = torch.clamp(linear_func, 0, 1)\n        return ramp_func",
    "Computes a linear ramp function used to smooth values between a minimum and maximum range.\n\nArgs:\n    min (float): Minimum value for the ramp function.\n    max (float): Maximum value for the ramp function.\n    dim (int): Dimensionality of the ramp tensor.\n\nReturns:\n    torch.Tensor: A tensor of shape (dim,) with values linearly interpolated between 0 and 1,\n        clamped to the range [0, 1]."
  ],
  [
    "def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n        \"\"\"\n        Forward pass for the Multi-Head Latent Attention (MLA) Layer.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim).\n            start_pos (int): Starting position in the sequence for caching.\n            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.\n\n        Returns:\n            torch.Tensor: Output tensor with the same shape as the input.\n        \"\"\"\n        bsz, seqlen, _ = x.size()\n        end_pos = start_pos + seqlen\n        if self.q_lora_rank == 0:\n            q = self.wq(x)\n        else:\n            q = self.wq_b(self.q_norm(self.wq_a(x)))\n        q = q.view(bsz, seqlen, self.n_local_heads, self.qk_head_dim)\n        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n        q_pe = apply_rotary_emb(q_pe, freqs_cis)\n        kv = self.wkv_a(x)\n        kv, k_pe = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis)\n        if attn_impl == \"naive\":\n            q = torch.cat([q_nope, q_pe], dim=-1)\n            kv = self.wkv_b(self.kv_norm(kv))\n            kv = kv.view(bsz, seqlen, self.n_local_heads, self.qk_nope_head_dim + self.v_head_dim)\n            k_nope, v = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n            k = torch.cat([k_nope, k_pe.expand(-1, -1, self.n_local_heads, -1)], dim=-1)\n            self.k_cache[:bsz, start_pos:end_pos] = k\n            self.v_cache[:bsz, start_pos:end_pos] = v\n            scores = torch.einsum(\"bshd,bthd->bsht\", q, self.k_cache[:bsz, :end_pos]) * self.softmax_scale\n        else:\n            wkv_b = self.wkv_b.weight if self.wkv_b.scale is None else weight_dequant(self.wkv_b.weight, self.wkv_b.scale, block_size) \n            wkv_b = wkv_b.view(self.n_local_heads, -1, self.kv_lora_rank)\n            q_nope = torch.einsum(\"bshd,hdc->bshc\", q_nope, wkv_b[:, :self.qk_nope_head_dim])\n            self.kv_cache[:bsz, start_pos:end_pos] = self.kv_norm(kv)\n            self.pe_cache[:bsz, start_pos:end_pos] = k_pe.squeeze(2)\n            scores = (torch.einsum(\"bshc,btc->bsht\", q_nope, self.kv_cache[:bsz, :end_pos]) +\n                      torch.einsum(\"bshr,btr->bsht\", q_pe, self.pe_cache[:bsz, :end_pos])) * self.softmax_scale\n        if mask is not None:\n            scores += mask.unsqueeze(1)\n        scores = scores.softmax(dim=-1, dtype=torch.float32).type_as(x)\n        if attn_impl == \"naive\":\n            x = torch.einsum(\"bsht,bthd->bshd\", scores, self.v_cache[:bsz, :end_pos])\n        else:\n            x = torch.einsum(\"bsht,btc->bshc\", scores, self.kv_cache[:bsz, :end_pos])\n            x = torch.einsum(\"bshc,hdc->bshd\", x, wkv_b[:, -self.v_head_dim:])\n        x = self.wo(x.flatten(2))\n        return x",
    "Forward pass for the Multi-Head Latent Attention (MLA) Layer.\n\nArgs:\n    x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim).\n    start_pos (int): Starting position in the sequence for caching.\n    freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n    mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.\n\nReturns:\n    torch.Tensor: Output tensor with the same shape as the input."
  ],
  [
    "def __init__(self, dim: int, inter_dim: int):\n        \"\"\"\n        Initializes the MLP layer.\n\n        Args:\n            dim (int): Input and output dimensionality.\n            inter_dim (int): Hidden layer dimensionality.\n        \"\"\"\n        super().__init__()\n        self.w1 = ColumnParallelLinear(dim, inter_dim)\n        self.w2 = RowParallelLinear(inter_dim, dim)\n        self.w3 = ColumnParallelLinear(dim, inter_dim)",
    "Initializes the MLP layer.\n\nArgs:\n    dim (int): Input and output dimensionality.\n    inter_dim (int): Hidden layer dimensionality."
  ],
  [
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the MLP layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after MLP computation.\n        \"\"\"\n        return self.w2(F.silu(self.w1(x)) * self.w3(x))",
    "Forward pass for the MLP layer.\n\nArgs:\n    x (torch.Tensor): Input tensor.\n\nReturns:\n    torch.Tensor: Output tensor after MLP computation."
  ],
  [
    "def __init__(self, args: ModelArgs):\n        \"\"\"\n        Initializes the Gate module.\n\n        Args:\n            args (ModelArgs): Model arguments containing gating parameters.\n        \"\"\"\n        super().__init__()\n        self.dim = args.dim\n        self.topk = args.n_activated_experts\n        self.n_groups = args.n_expert_groups\n        self.topk_groups = args.n_limited_groups\n        self.score_func = args.score_func\n        self.route_scale = args.route_scale\n        self.weight = nn.Parameter(torch.empty(args.n_routed_experts, args.dim))\n        self.bias = nn.Parameter(torch.empty(args.n_routed_experts)) if self.dim == 7168 else None",
    "Initializes the Gate module.\n\nArgs:\n    args (ModelArgs): Model arguments containing gating parameters."
  ],
  [
    "def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass for the gating mechanism.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            Tuple[torch.Tensor, torch.Tensor]: Routing weights and selected expert indices.\n        \"\"\"\n        scores = linear(x, self.weight)\n        if self.score_func == \"softmax\":\n            scores = scores.softmax(dim=-1, dtype=torch.float32)\n        else:\n            scores = scores.sigmoid()\n        original_scores = scores\n        if self.bias is not None:\n            scores = scores + self.bias\n        if self.n_groups > 1:\n            scores = scores.view(x.size(0), self.n_groups, -1)\n            if self.bias is None:\n                group_scores = scores.amax(dim=-1)\n            else:\n                group_scores = scores.topk(2, dim=-1)[0].sum(dim=-1)\n            indices = group_scores.topk(self.topk_groups, dim=-1)[1]\n            mask = scores.new_ones(x.size(0), self.n_groups, dtype=bool).scatter_(1, indices, False)\n            scores = scores.masked_fill_(mask.unsqueeze(-1), float(\"-inf\")).flatten(1)\n        indices = torch.topk(scores, self.topk, dim=-1)[1]\n        weights = original_scores.gather(1, indices)\n        if self.score_func == \"sigmoid\":\n            weights /= weights.sum(dim=-1, keepdim=True)\n        weights *= self.route_scale\n        return weights.type_as(x), indices",
    "Forward pass for the gating mechanism.\n\nArgs:\n    x (torch.Tensor): Input tensor.\n\nReturns:\n    Tuple[torch.Tensor, torch.Tensor]: Routing weights and selected expert indices."
  ],
  [
    "def __init__(self, dim: int, inter_dim: int):\n        \"\"\"\n        Initializes the Expert layer.\n\n        Args:\n            dim (int): Input and output dimensionality.\n            inter_dim (int): Hidden layer dimensionality.\n        \"\"\"\n        super().__init__()\n        self.w1 = Linear(dim, inter_dim)\n        self.w2 = Linear(inter_dim, dim)\n        self.w3 = Linear(dim, inter_dim)",
    "Initializes the Expert layer.\n\nArgs:\n    dim (int): Input and output dimensionality.\n    inter_dim (int): Hidden layer dimensionality."
  ],
  [
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the Expert layer.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after expert computation.\n        \"\"\"\n        return self.w2(F.silu(self.w1(x)) * self.w3(x))",
    "Forward pass for the Expert layer.\n\nArgs:\n    x (torch.Tensor): Input tensor.\n\nReturns:\n    torch.Tensor: Output tensor after expert computation."
  ],
  [
    "def __init__(self, args: ModelArgs):\n        \"\"\"\n        Initializes the MoE module.\n\n        Args:\n            args (ModelArgs): Model arguments containing MoE parameters.\n        \"\"\"\n        super().__init__()\n        self.dim = args.dim\n        assert args.n_routed_experts % world_size == 0, f\"Number of experts must be divisible by world size (world_size={world_size})\"\n        self.n_routed_experts = args.n_routed_experts\n        self.n_local_experts = args.n_routed_experts // world_size\n        self.n_activated_experts = args.n_activated_experts\n        self.experts_start_idx = rank * self.n_local_experts\n        self.experts_end_idx = self.experts_start_idx + self.n_local_experts\n        self.gate = Gate(args)\n        self.experts = nn.ModuleList([Expert(args.dim, args.moe_inter_dim) if self.experts_start_idx <= i < self.experts_end_idx else None\n                                      for i in range(self.n_routed_experts)])\n        self.shared_experts = MLP(args.dim, args.n_shared_experts * args.moe_inter_dim)",
    "Initializes the MoE module.\n\nArgs:\n    args (ModelArgs): Model arguments containing MoE parameters."
  ],
  [
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the MoE module.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after expert routing and computation.\n        \"\"\"\n        shape = x.size()\n        x = x.view(-1, self.dim)\n        weights, indices = self.gate(x)\n        y = torch.zeros_like(x)\n        counts = torch.bincount(indices.flatten(), minlength=self.n_routed_experts).tolist()\n        for i in range(self.experts_start_idx, self.experts_end_idx):\n            if counts[i] == 0:\n                continue\n            expert = self.experts[i]\n            idx, top = torch.where(indices == i)\n            y[idx] += expert(x[idx]) * weights[idx, top, None]\n        z = self.shared_experts(x)\n        if world_size > 1:\n            dist.all_reduce(y)\n        return (y + z).view(shape)",
    "Forward pass for the MoE module.\n\nArgs:\n    x (torch.Tensor): Input tensor.\n\nReturns:\n    torch.Tensor: Output tensor after expert routing and computation."
  ],
  [
    "def __init__(self, layer_id: int, args: ModelArgs):\n        \"\"\"\n        Initializes the Transformer block.\n\n        Args:\n            layer_id (int): Layer index in the transformer.\n            args (ModelArgs): Model arguments containing block parameters.\n        \"\"\"\n        super().__init__()\n        self.attn = MLA(args)\n        self.ffn = MLP(args.dim, args.inter_dim) if layer_id < args.n_dense_layers else MoE(args)\n        self.attn_norm = RMSNorm(args.dim)\n        self.ffn_norm = RMSNorm(args.dim)",
    "Initializes the Transformer block.\n\nArgs:\n    layer_id (int): Layer index in the transformer.\n    args (ModelArgs): Model arguments containing block parameters."
  ],
  [
    "def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]) -> torch.Tensor:\n        \"\"\"\n        Forward pass for the Transformer block.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n            start_pos (int): Starting position in the sequence.\n            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.\n\n        Returns:\n            torch.Tensor: Output tensor after block computation.\n        \"\"\"\n        x = x + self.attn(self.attn_norm(x), start_pos, freqs_cis, mask)\n        x = x + self.ffn(self.ffn_norm(x))\n        return x",
    "Forward pass for the Transformer block.\n\nArgs:\n    x (torch.Tensor): Input tensor.\n    start_pos (int): Starting position in the sequence.\n    freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n    mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.\n\nReturns:\n    torch.Tensor: Output tensor after block computation."
  ],
  [
    "def __init__(self, args: ModelArgs):\n        \"\"\"\n        Initializes the Transformer model.\n\n        Args:\n            args (ModelArgs): Model arguments containing transformer parameters.\n        \"\"\"\n        global world_size, rank\n        world_size = dist.get_world_size() if dist.is_initialized() else 1\n        rank = dist.get_rank() if dist.is_initialized() else 0\n        Linear.dtype = torch.float8_e4m3fn if args.dtype == \"fp8\" else torch.bfloat16\n        super().__init__()\n        self.max_seq_len = args.max_seq_len\n        self.embed = ParallelEmbedding(args.vocab_size, args.dim)\n        self.layers = torch.nn.ModuleList()\n        for layer_id in range(args.n_layers):\n            self.layers.append(Block(layer_id, args))\n        self.norm = RMSNorm(args.dim)\n        self.head = ColumnParallelLinear(args.dim, args.vocab_size, dtype=torch.get_default_dtype())\n        self.register_buffer(\"freqs_cis\", precompute_freqs_cis(args), persistent=False)",
    "Initializes the Transformer model.\n\nArgs:\n    args (ModelArgs): Model arguments containing transformer parameters."
  ],
  [
    "def forward(self, tokens: torch.Tensor, start_pos: int = 0):\n        \"\"\"\n        Forward pass for the Transformer model.\n\n        Args:\n            tokens (torch.Tensor): Input tensor of token IDs with shape (batch_size, seq_len).\n            start_pos (int, optional): Starting position in the sequence for rotary embeddings. Defaults to 0.\n\n        Returns:\n            torch.Tensor: Logits tensor of shape (batch_size, vocab_size).\n        \"\"\"\n        seqlen = tokens.size(1)\n        h = self.embed(tokens)\n        freqs_cis = self.freqs_cis[start_pos:start_pos+seqlen]\n        mask = None\n        if seqlen > 1:\n            mask = torch.full((seqlen, seqlen), float(\"-inf\"), device=tokens.device).triu_(1)\n        for layer in self.layers:\n            h = layer(h, start_pos, freqs_cis, mask)\n        h = self.norm(h)[:, -1]\n        logits = self.head(h)\n        if world_size > 1:\n            all_logits = [torch.empty_like(logits) for _ in range(world_size)]\n            dist.all_gather(all_logits, logits)\n            logits = torch.cat(all_logits, dim=-1)\n        return logits",
    "Forward pass for the Transformer model.\n\nArgs:\n    tokens (torch.Tensor): Input tensor of token IDs with shape (batch_size, seq_len).\n    start_pos (int, optional): Starting position in the sequence for rotary embeddings. Defaults to 0.\n\nReturns:\n    torch.Tensor: Logits tensor of shape (batch_size, vocab_size)."
  ],
  [
    "async def generate_code_and_time(\n    image_url: str,\n    stack: Stack,\n    model: Llm,\n    original_input_filename: str,\n    attempt_idx: int,\n) -> Tuple[str, int, Optional[str], Optional[float], Optional[Exception]]:\n    \"\"\"\n    Generates code for an image, measures the time taken, and returns identifiers\n    along with success/failure status.\n    Returns a tuple: (original_input_filename, attempt_idx, content, duration, error_object)\n    content and duration are None if an error occurs during generation.\n    \"\"\"\n    start_time = time.perf_counter()\n    try:\n        content = await generate_code_for_image(\n            image_url=image_url, stack=stack, model=model\n        )\n        end_time = time.perf_counter()\n        duration = end_time - start_time\n        return original_input_filename, attempt_idx, content, duration, None\n    except Exception as e:\n        print(\n            f\"Error during code generation for {original_input_filename} (attempt {attempt_idx}): {e}\"\n        )\n        return original_input_filename, attempt_idx, None, None, e",
    "Generates code for an image, measures the time taken, and returns identifiers\nalong with success/failure status.\nReturns a tuple: (original_input_filename, attempt_idx, content, duration, error_object)\ncontent and duration are None if an error occurs during generation."
  ],
  [
    "async def convert_to_markdown(uri: str) -> str:\n    \n    return MarkItDown(enable_plugins=check_plugins_enabled()).convert_uri(uri).markdown",
    "Convert a resource described by an http:, https:, file: or data: URI to markdown"
  ],
  [
    "def register_converters(markitdown: MarkItDown, **kwargs):\n    \"\"\"\n    Called during construction of MarkItDown instances to register converters provided by plugins.\n    \"\"\"\n\n    # Simply create and attach an RtfConverter instance\n    markitdown.register_converter(RtfConverter())",
    "Called during construction of MarkItDown instances to register converters provided by plugins."
  ],
  [
    "class RtfConverter(DocumentConverter):\n    \"\"\"\n    Converts an RTF file to in the simplest possible way.\n    \"\"\"\n\n    def accepts(\n        self,\n        file_stream: BinaryIO,\n        stream_info: StreamInfo,\n        **kwargs: Any,\n    ) -> bool:\n        mimetype = (stream_info.mimetype or \"\").lower()\n        extension = (stream_info.extension or \"\").lower()\n\n        if extension in ACCEPTED_FILE_EXTENSIONS:\n            return True\n\n        for prefix in ACCEPTED_MIME_TYPE_PREFIXES:\n            if mimetype.startswith(prefix):\n                return True\n\n        return False\n\n    def convert(\n        self,\n        file_stream: BinaryIO,\n        stream_info: StreamInfo,\n        **kwargs: Any,\n    ) -> DocumentConverterResult:\n        # Read the file stream into an str using hte provided charset encoding, or using the system default\n        encoding = stream_info.charset or locale.getpreferredencoding()\n        stream_data = file_stream.read().decode(encoding)\n\n        # Return the result\n        return DocumentConverterResult(\n            title=None,\n            markdown=rtf_to_text(stream_data),\n        )",
    "Converts an RTF file to in the simplest possible way."
  ],
  [
    "def test_markitdown() -> None:\n    \n    md = MarkItDown(enable_plugins=True)\n    result = md.convert(os.path.join(TEST_FILES_DIR, \"test.rtf\"))\n\n    for test_string in RTF_TEST_STRINGS:\n        assert test_string in result.text_content",
    "Tests that MarkItDown correctly loads the plugin."
  ],
  [
    "def _handle_output(args, result: DocumentConverterResult):\n    \n    if args.output:\n        with open(args.output, \"w\", encoding=\"utf-8\") as f:\n            f.write(result.markdown)\n    else:\n        # Handle stdout encoding errors more gracefully\n        print(\n            result.markdown.encode(sys.stdout.encoding, errors=\"replace\").decode(\n                sys.stdout.encoding\n            )\n        )",
    "Handle output to stdout or file"
  ],
  [
    "class DocumentConverterResult:\n    \n\n    def __init__(\n        self,\n        markdown: str,\n        *,\n        title: Optional[str] = None,\n    ):\n        \"\"\"\n        Initialize the DocumentConverterResult.\n\n        The only required parameter is the converted Markdown text.\n        The title, and any other metadata that may be added in the future, are optional.\n\n        Parameters:\n        - markdown: The converted Markdown text.\n        - title: Optional title of the document.\n        \"\"\"\n        self.markdown = markdown\n        self.title = title\n\n    @property\n    def text_content(self) -> str:\n        \"\"\"Soft-deprecated alias for `markdown`. New code should migrate to using `markdown` or __str__.\"\"\"\n        return self.markdown\n\n    @text_content.setter\n    def text_content(self, markdown: str):\n        \"\"\"Soft-deprecated alias for `markdown`. New code should migrate to using `markdown` or __str__.\"\"\"\n        self.markdown = markdown\n\n    def __str__(self) -> str:\n        \"\"\"Return the converted Markdown text.\"\"\"\n        return self.markdown",
    "The result of converting a document to Markdown."
  ],
  [
    "def __init__(\n        self,\n        markdown: str,\n        *,\n        title: Optional[str] = None,\n    ):\n        \"\"\"\n        Initialize the DocumentConverterResult.\n\n        The only required parameter is the converted Markdown text.\n        The title, and any other metadata that may be added in the future, are optional.\n\n        Parameters:\n        - markdown: The converted Markdown text.\n        - title: Optional title of the document.\n        \"\"\"\n        self.markdown = markdown\n        self.title = title",
    "Initialize the DocumentConverterResult.\n\nThe only required parameter is the converted Markdown text.\nThe title, and any other metadata that may be added in the future, are optional.\n\nParameters:\n- markdown: The converted Markdown text.\n- title: Optional title of the document."
  ],
  [
    "def text_content(self) -> str:\n        \n        return self.markdown",
    "Soft-deprecated alias for `markdown`. New code should migrate to using `markdown` or __str__."
  ],
  [
    "def text_content(self, markdown: str):\n        \n        self.markdown = markdown",
    "Soft-deprecated alias for `markdown`. New code should migrate to using `markdown` or __str__."
  ],
  [
    "def accepts(\n        self,\n        file_stream: BinaryIO,\n        stream_info: StreamInfo,\n        **kwargs: Any,  # Options to pass to the converter\n    ) -> bool:\n        \"\"\"\n        Return a quick determination on if the converter should attempt converting the document.\n        This is primarily based `stream_info` (typically, `stream_info.mimetype`, `stream_info.extension`).\n        In cases where the data is retrieved via HTTP, the `steam_info.url` might also be referenced to\n        make a determination (e.g., special converters for Wikipedia, YouTube etc).\n        Finally, it is conceivable that the `stream_info.filename` might be used to in cases\n        where the filename is well-known (e.g., `Dockerfile`, `Makefile`, etc)\n\n        NOTE: The method signature is designed to match that of the convert() method. This provides some\n        assurance that, if accepts() returns True, the convert() method will also be able to handle the document.\n\n        IMPORTANT: In rare cases, (e.g., OutlookMsgConverter) we need to read more from the stream to make a final\n        determination. Read operations inevitably advances the position in file_stream. In these case, the position\n        MUST be reset it MUST be reset before returning. This is because the convert() method may be called immediately\n        after accepts(), and will expect the file_stream to be at the original position.\n\n        E.g.,\n        cur_pos = file_stream.tell() # Save the current position\n        data = file_stream.read(100) # ... peek at the first 100 bytes, etc.\n        file_stream.seek(cur_pos)    # Reset the position to the original position\n\n        Prameters:\n        - file_stream: The file-like object to convert. Must support seek(), tell(), and read() methods.\n        - stream_info: The StreamInfo object containing metadata about the file (mimetype, extension, charset, set)\n        - kwargs: Additional keyword arguments for the converter.\n\n        Returns:\n        - bool: True if the converter can handle the document, False otherwise.\n        \"\"\"\n        raise NotImplementedError(\n            f\"The subclass, {type(self).__name__}, must implement the accepts() method to determine if they can handle the document.\"\n        )",
    "Return a quick determination on if the converter should attempt converting the document.\nThis is primarily based `stream_info` (typically, `stream_info.mimetype`, `stream_info.extension`).\nIn cases where the data is retrieved via HTTP, the `steam_info.url` might also be referenced to\nmake a determination (e.g., special converters for Wikipedia, YouTube etc).\nFinally, it is conceivable that the `stream_info.filename` might be used to in cases\nwhere the filename is well-known (e.g., `Dockerfile`, `Makefile`, etc)\n\nNOTE: The method signature is designed to match that of the convert() method. This provides some\nassurance that, if accepts() returns True, the convert() method will also be able to handle the document.\n\nIMPORTANT: In rare cases, (e.g., OutlookMsgConverter) we need to read more from the stream to make a final\ndetermination. Read operations inevitably advances the position in file_stream. In these case, the position\nMUST be reset it MUST be reset before returning. This is because the convert() method may be called immediately\nafter accepts(), and will expect the file_stream to be at the original position.\n\nE.g.,\ncur_pos = file_stream.tell() # Save the current position\ndata = file_stream.read(100) # ... peek at the first 100 bytes, etc.\nfile_stream.seek(cur_pos)    # Reset the position to the original position\n\nPrameters:\n- file_stream: The file-like object to convert. Must support seek(), tell(), and read() methods.\n- stream_info: The StreamInfo object containing metadata about the file (mimetype, extension, charset, set)\n- kwargs: Additional keyword arguments for the converter.\n\nReturns:\n- bool: True if the converter can handle the document, False otherwise."
  ],
  [
    "def convert(\n        self,\n        file_stream: BinaryIO,\n        stream_info: StreamInfo,\n        **kwargs: Any,  # Options to pass to the converter\n    ) -> DocumentConverterResult:\n        \"\"\"\n        Convert a document to Markdown text.\n\n        Prameters:\n        - file_stream: The file-like object to convert. Must support seek(), tell(), and read() methods.\n        - stream_info: The StreamInfo object containing metadata about the file (mimetype, extension, charset, set)\n        - kwargs: Additional keyword arguments for the converter.\n\n        Returns:\n        - DocumentConverterResult: The result of the conversion, which includes the title and markdown content.\n\n        Raises:\n        - FileConversionException: If the mimetype is recognized, but the conversion fails for some other reason.\n        - MissingDependencyException: If the converter requires a dependency that is not installed.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement this method\")",
    "Convert a document to Markdown text.\n\nPrameters:\n- file_stream: The file-like object to convert. Must support seek(), tell(), and read() methods.\n- stream_info: The StreamInfo object containing metadata about the file (mimetype, extension, charset, set)\n- kwargs: Additional keyword arguments for the converter.\n\nReturns:\n- DocumentConverterResult: The result of the conversion, which includes the title and markdown content.\n\nRaises:\n- FileConversionException: If the mimetype is recognized, but the conversion fails for some other reason.\n- MissingDependencyException: If the converter requires a dependency that is not installed."
  ],
  [
    "class MissingDependencyException(MarkItDownException):\n    \"\"\"\n    Converters shipped with MarkItDown may depend on optional\n    dependencies. This exception is thrown when a converter's\n    convert() method is called, but the required dependency is not\n    installed. This is not necessarily a fatal error, as the converter\n    will simply be skipped (an error will bubble up only if no other\n    suitable converter is found).\n\n    Error messages should clearly indicate which dependency is missing.\n    \"\"\"\n\n    pass",
    "Converters shipped with MarkItDown may depend on optional\ndependencies. This exception is thrown when a converter's\nconvert() method is called, but the required dependency is not\ninstalled. This is not necessarily a fatal error, as the converter\nwill simply be skipped (an error will bubble up only if no other\nsuitable converter is found).\n\nError messages should clearly indicate which dependency is missing."
  ],
  [
    "class UnsupportedFormatException(MarkItDownException):\n    \"\"\"\n    Thrown when no suitable converter was found for the given file.\n    \"\"\"\n\n    pass",
    "Thrown when no suitable converter was found for the given file."
  ],
  [
    "class FailedConversionAttempt(object):\n    \"\"\"\n    Represents an a single attempt to convert a file.\n    \"\"\"\n\n    def __init__(self, converter: Any, exc_info: Optional[tuple] = None):\n        self.converter = converter\n        self.exc_info = exc_info",
    "Represents an a single attempt to convert a file."
  ],
  [
    "class FileConversionException(MarkItDownException):\n    \"\"\"\n    Thrown when a suitable converter was found, but the conversion\n    process fails for any reason.\n    \"\"\"\n\n    def __init__(\n        self,\n        message: Optional[str] = None,\n        attempts: Optional[List[FailedConversionAttempt]] = None,\n    ):\n        self.attempts = attempts\n\n        if message is None:\n            if attempts is None:\n                message = \"File conversion failed.\"\n            else:\n                message = f\"File conversion failed after {len(attempts)} attempts:\\n\"\n                for attempt in attempts:\n                    if attempt.exc_info is None:\n                        message += f\" -  {type(attempt.converter).__name__} provided no execution info.\"\n                    else:\n                        message += f\" - {type(attempt.converter).__name__} threw {attempt.exc_info[0].__name__} with message: {attempt.exc_info[1]}\\n\"\n\n        super().__init__(message)",
    "Thrown when a suitable converter was found, but the conversion\nprocess fails for any reason."
  ],
  [
    "def _load_plugins() -> Union[None, List[Any]]:\n    \n    global _plugins\n\n    # Skip if we've already loaded plugins\n    if _plugins is not None:\n        return _plugins\n\n    # Load plugins\n    _plugins = []\n    for entry_point in entry_points(group=\"markitdown.plugin\"):\n        try:\n            _plugins.append(entry_point.load())\n        except Exception:\n            tb = traceback.format_exc()\n            warn(f\"Plugin '{entry_point.name}' failed to load ... skipping:\\n{tb}\")\n\n    return _plugins",
    "Lazy load plugins, exiting early if already loaded."
  ],
  [
    "class ConverterRegistration:\n    \n\n    converter: DocumentConverter\n    priority: float",
    "A registration of a converter with its priority and other metadata."
  ],
  [
    "class MarkItDown:\n    \"\"\"(In preview) An extremely simple text-based document reader, suitable for LLM use.\n    This reader will convert common file-types or webpages to Markdown.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        enable_builtins: Union[None, bool] = None,\n        enable_plugins: Union[None, bool] = None,\n        **kwargs,\n    ):\n        self._builtins_enabled = False\n        self._plugins_enabled = False\n\n        requests_session = kwargs.get(\"requests_session\")\n        if requests_session is None:\n            self._requests_session = requests.Session()\n        else:\n            self._requests_session = requests_session\n\n        self._magika = magika.Magika()\n\n        # TODO - remove these (see enable_builtins)\n        self._llm_client: Any = None\n        self._llm_model: Union[str | None] = None\n        self._exiftool_path: Union[str | None] = None\n        self._style_map: Union[str | None] = None\n\n        # Register the converters\n        self._converters: List[ConverterRegistration] = []\n\n        if (\n            enable_builtins is None or enable_builtins\n        ):  # Default to True when not specified\n            self.enable_builtins(**kwargs)\n\n        if enable_plugins:\n            self.enable_plugins(**kwargs)\n\n    def enable_builtins(self, **kwargs) -> None:\n        \"\"\"\n        Enable and register built-in converters.\n        Built-in converters are enabled by default.\n        This method should only be called once, if built-ins were initially disabled.\n        \"\"\"\n        if not self._builtins_enabled:\n            # TODO: Move these into converter constructors\n            self._llm_client = kwargs.get(\"llm_client\")\n            self._llm_model = kwargs.get(\"llm_model\")\n            self._exiftool_path = kwargs.get(\"exiftool_path\")\n            self._style_map = kwargs.get(\"style_map\")\n\n            if self._exiftool_path is None:\n                self._exiftool_path = os.getenv(\"EXIFTOOL_PATH\")\n\n            # Still none? Check well-known paths\n            if self._exiftool_path is None:\n                candidate = shutil.which(\"exiftool\")\n                if candidate:\n                    candidate = os.path.abspath(candidate)\n                    if any(\n                        d == os.path.dirname(candidate)\n                        for d in [\n                            \"/usr/bin\",\n                            \"/usr/local/bin\",\n                            \"/opt\",\n                            \"/opt/bin\",\n                            \"/opt/local/bin\",\n                            \"/opt/homebrew/bin\",\n                            \"C:\\\\Windows\\\\System32\",\n                            \"C:\\\\Program Files\",\n                            \"C:\\\\Program Files (x86)\",\n                        ]\n                    ):\n                        self._exiftool_path = candidate\n\n            # Register converters for successful browsing operations\n            # Later registrations are tried first / take higher priority than earlier registrations\n            # To this end, the most specific converters should appear below the most generic converters\n            self.register_converter(\n                PlainTextConverter(), priority=PRIORITY_GENERIC_FILE_FORMAT\n            )\n            self.register_converter(\n                ZipConverter(markitdown=self), priority=PRIORITY_GENERIC_FILE_FORMAT\n            )\n            self.register_converter(\n                HtmlConverter(), priority=PRIORITY_GENERIC_FILE_FORMAT\n            )\n            self.register_converter(RssConverter())\n            self.register_converter(WikipediaConverter())\n            self.register_converter(YouTubeConverter())\n            self.register_converter(BingSerpConverter())\n            self.register_converter(DocxConverter())\n            self.register_converter(XlsxConverter())\n            self.register_converter(XlsConverter())\n            self.register_converter(PptxConverter())\n            self.register_converter(AudioConverter())\n            self.register_converter(ImageConverter())\n            self.register_converter(IpynbConverter())\n            self.register_converter(PdfConverter())\n            self.register_converter(OutlookMsgConverter())\n            self.register_converter(EpubConverter())\n            self.register_converter(CsvConverter())\n\n            # Register Document Intelligence converter at the top of the stack if endpoint is provided\n            docintel_endpoint = kwargs.get(\"docintel_endpoint\")\n            if docintel_endpoint is not None:\n                docintel_args: Dict[str, Any] = {}\n                docintel_args[\"endpoint\"] = docintel_endpoint\n\n                docintel_credential = kwargs.get(\"docintel_credential\")\n                if docintel_credential is not None:\n                    docintel_args[\"credential\"] = docintel_credential\n\n                docintel_types = kwargs.get(\"docintel_file_types\")\n                if docintel_types is not None:\n                    docintel_args[\"file_types\"] = docintel_types\n\n                docintel_version = kwargs.get(\"docintel_api_version\")\n                if docintel_version is not None:\n                    docintel_args[\"api_version\"] = docintel_version\n\n                self.register_converter(\n                    DocumentIntelligenceConverter(**docintel_args),\n                )\n\n            self._builtins_enabled = True\n        else:\n            warn(\"Built-in converters are already enabled.\", RuntimeWarning)\n\n    def enable_plugins(self, **kwargs) -> None:\n        \"\"\"\n        Enable and register converters provided by plugins.\n        Plugins are disabled by default.\n        This method should only be called once, if plugins were initially disabled.\n        \"\"\"\n        if not self._plugins_enabled:\n            # Load plugins\n            plugins = _load_plugins()\n            assert plugins is not None\n            for plugin in plugins:\n                try:\n                    plugin.register_converters(self, **kwargs)\n                except Exception:\n                    tb = traceback.format_exc()\n                    warn(f\"Plugin '{plugin}' failed to register converters:\\n{tb}\")\n            self._plugins_enabled = True\n        else:\n            warn(\"Plugins converters are already enabled.\", RuntimeWarning)\n\n    def convert(\n        self,\n        source: Union[str, requests.Response, Path, BinaryIO],\n        *,\n        stream_info: Optional[StreamInfo] = None,\n        **kwargs: Any,\n    ) -> DocumentConverterResult:  # TODO: deal with kwargs\n        \"\"\"\n        Args:\n            - source: can be a path (str or Path), url, or a requests.response object\n            - stream_info: optional stream info to use for the conversion. If None, infer from source\n            - kwargs: additional arguments to pass to the converter\n        \"\"\"\n\n        # Local path or url\n        if isinstance(source, str):\n            if (\n                source.startswith(\"http:\")\n                or source.startswith(\"https:\")\n                or source.startswith(\"file:\")\n                or source.startswith(\"data:\")\n            ):\n                # Rename the url argument to mock_url\n                # (Deprecated -- use stream_info)\n                _kwargs = {k: v for k, v in kwargs.items()}\n                if \"url\" in _kwargs:\n                    _kwargs[\"mock_url\"] = _kwargs[\"url\"]\n                    del _kwargs[\"url\"]\n\n                return self.convert_uri(source, stream_info=stream_info, **_kwargs)\n            else:\n                return self.convert_local(source, stream_info=stream_info, **kwargs)\n        # Path object\n        elif isinstance(source, Path):\n            return self.convert_local(source, stream_info=stream_info, **kwargs)\n        # Request response\n        elif isinstance(source, requests.Response):\n            return self.convert_response(source, stream_info=stream_info, **kwargs)\n        # Binary stream\n        elif (\n            hasattr(source, \"read\")\n            and callable(source.read)\n            and not isinstance(source, io.TextIOBase)\n        ):\n            return self.convert_stream(source, stream_info=stream_info, **kwargs)\n        else:\n            raise TypeError(\n                f\"Invalid source type: {type(source)}. Expected str, requests.Response, BinaryIO.\"\n            )\n\n    def convert_local(\n        self,\n        path: Union[str, Path],\n        *,\n        stream_info: Optional[StreamInfo] = None,\n        file_extension: Optional[str] = None,  # Deprecated -- use stream_info\n        url: Optional[str] = None,  # Deprecated -- use stream_info\n        **kwargs: Any,\n    ) -> DocumentConverterResult:\n        if isinstance(path, Path):\n            path = str(path)\n\n        # Build a base StreamInfo object from which to start guesses\n        base_guess = StreamInfo(\n            local_path=path,\n            extension=os.path.splitext(path)[1],\n            filename=os.path.basename(path),\n        )\n\n        # Extend the base_guess with any additional info from the arguments\n        if stream_info is not None:\n            base_guess = base_guess.copy_and_update(stream_info)\n\n        if file_extension is not None:\n            # Deprecated -- use stream_info\n            base_guess = base_guess.copy_and_update(extension=file_extension)\n\n        if url is not None:\n            # Deprecated -- use stream_info\n            base_guess = base_guess.copy_and_update(url=url)\n\n        with open(path, \"rb\") as fh:\n            guesses = self._get_stream_info_guesses(\n                file_stream=fh, base_guess=base_guess\n            )\n            return self._convert(file_stream=fh, stream_info_guesses=guesses, **kwargs)\n\n    def convert_stream(\n        self,\n        stream: BinaryIO,\n        *,\n        stream_info: Optional[StreamInfo] = None,\n        file_extension: Optional[str] = None,  # Deprecated -- use stream_info\n        url: Optional[str] = None,  # Deprecated -- use stream_info\n        **kwargs: Any,\n    ) -> DocumentConverterResult:\n        guesses: List[StreamInfo] = []\n\n        # Do we have anything on which to base a guess?\n        base_guess = None\n        if stream_info is not None or file_extension is not None or url is not None:\n            # Start with a non-Null base guess\n            if stream_info is None:\n                base_guess = StreamInfo()\n            else:\n                base_guess = stream_info\n\n            if file_extension is not None:\n                # Deprecated -- use stream_info\n                assert base_guess is not None  # for mypy\n                base_guess = base_guess.copy_and_update(extension=file_extension)\n\n            if url is not None:\n                # Deprecated -- use stream_info\n                assert base_guess is not None  # for mypy\n                base_guess = base_guess.copy_and_update(url=url)\n\n        # Check if we have a seekable stream. If not, load the entire stream into memory.\n        if not stream.seekable():\n            buffer = io.BytesIO()\n            while True:\n                chunk = stream.read(4096)\n                if not chunk:\n                    break\n                buffer.write(chunk)\n            buffer.seek(0)\n            stream = buffer\n\n        # Add guesses based on stream content\n        guesses = self._get_stream_info_guesses(\n            file_stream=stream, base_guess=base_guess or StreamInfo()\n        )\n        return self._convert(file_stream=stream, stream_info_guesses=guesses, **kwargs)\n\n    def convert_url(\n        self,\n        url: str,\n        *,\n        stream_info: Optional[StreamInfo] = None,\n        file_extension: Optional[str] = None,\n        mock_url: Optional[str] = None,\n        **kwargs: Any,\n    ) -> DocumentConverterResult:\n        \"\"\"Alias for convert_uri()\"\"\"\n        # convert_url will likely be deprecated in the future in favor of convert_uri\n        return self.convert_uri(\n            url,\n            stream_info=stream_info,\n            file_extension=file_extension,\n            mock_url=mock_url,\n            **kwargs,\n        )\n\n    def convert_uri(\n        self,\n        uri: str,\n        *,\n        stream_info: Optional[StreamInfo] = None,\n        file_extension: Optional[str] = None,  # Deprecated -- use stream_info\n        mock_url: Optional[\n            str\n        ] = None,  # Mock the request as if it came from a different URL\n        **kwargs: Any,\n    ) -> DocumentConverterResult:\n        uri = uri.strip()\n\n        # File URIs\n        if uri.startswith(\"file:\"):\n            netloc, path = file_uri_to_path(uri)\n            if netloc and netloc != \"localhost\":\n                raise ValueError(\n                    f\"Unsupported file URI: {uri}. Netloc must be empty or localhost.\"\n                )\n            return self.convert_local(\n                path,\n                stream_info=stream_info,\n                file_extension=file_extension,\n                url=mock_url,\n                **kwargs,\n            )\n        # Data URIs\n        elif uri.startswith(\"data:\"):\n            mimetype, attributes, data = parse_data_uri(uri)\n\n            base_guess = StreamInfo(\n                mimetype=mimetype,\n                charset=attributes.get(\"charset\"),\n            )\n            if stream_info is not None:\n                base_guess = base_guess.copy_and_update(stream_info)\n\n            return self.convert_stream(\n                io.BytesIO(data),\n                stream_info=base_guess,\n                file_extension=file_extension,\n                url=mock_url,\n                **kwargs,\n            )\n        # HTTP/HTTPS URIs\n        elif uri.startswith(\"http:\") or uri.startswith(\"https:\"):\n            response = self._requests_session.get(uri, stream=True)\n            response.raise_for_status()\n            return self.convert_response(\n                response,\n                stream_info=stream_info,\n                file_extension=file_extension,\n                url=mock_url,\n                **kwargs,\n            )\n        else:\n            raise ValueError(\n                f\"Unsupported URI scheme: {uri.split(':')[0]}. Supported schemes are: file:, data:, http:, https:\"\n            )\n\n    def convert_response(\n        self,\n        response: requests.Response,\n        *,\n        stream_info: Optional[StreamInfo] = None,\n        file_extension: Optional[str] = None,  # Deprecated -- use stream_info\n        url: Optional[str] = None,  # Deprecated -- use stream_info\n        **kwargs: Any,\n    ) -> DocumentConverterResult:\n        # If there is a content-type header, get the mimetype and charset (if present)\n        mimetype: Optional[str] = None\n        charset: Optional[str] = None\n\n        if \"content-type\" in response.headers:\n            parts = response.headers[\"content-type\"].split(\";\")\n            mimetype = parts.pop(0).strip()\n            for part in parts:\n                if part.strip().startswith(\"charset=\"):\n                    _charset = part.split(\"=\")[1].strip()\n                    if len(_charset) > 0:\n                        charset = _charset\n\n        # If there is a content-disposition header, get the filename and possibly the extension\n        filename: Optional[str] = None\n        extension: Optional[str] = None\n        if \"content-disposition\" in response.headers:\n            m = re.search(r\"filename=([^;]+)\", response.headers[\"content-disposition\"])\n            if m:\n                filename = m.group(1).strip(\"\\\"'\")\n                _, _extension = os.path.splitext(filename)\n                if len(_extension) > 0:\n                    extension = _extension\n\n        # If there is still no filename, try to read it from the url\n        if filename is None:\n            parsed_url = urlparse(response.url)\n            _, _extension = os.path.splitext(parsed_url.path)\n            if len(_extension) > 0:  # Looks like this might be a file!\n                filename = os.path.basename(parsed_url.path)\n                extension = _extension\n\n        # Create an initial guess from all this information\n        base_guess = StreamInfo(\n            mimetype=mimetype,\n            charset=charset,\n            filename=filename,\n            extension=extension,\n            url=response.url,\n        )\n\n        # Update with any additional info from the arguments\n        if stream_info is not None:\n            base_guess = base_guess.copy_and_update(stream_info)\n        if file_extension is not None:\n            # Deprecated -- use stream_info\n            base_guess = base_guess.copy_and_update(extension=file_extension)\n        if url is not None:\n            # Deprecated -- use stream_info\n            base_guess = base_guess.copy_and_update(url=url)\n\n        # Read into BytesIO\n        buffer = io.BytesIO()\n        for chunk in response.iter_content(chunk_size=512):\n            buffer.write(chunk)\n        buffer.seek(0)\n\n        # Convert\n        guesses = self._get_stream_info_guesses(\n            file_stream=buffer, base_guess=base_guess\n        )\n        return self._convert(file_stream=buffer, stream_info_guesses=guesses, **kwargs)\n\n    def _convert(\n        self, *, file_stream: BinaryIO, stream_info_guesses: List[StreamInfo], **kwargs\n    ) -> DocumentConverterResult:\n        res: Union[None, DocumentConverterResult] = None\n\n        # Keep track of which converters throw exceptions\n        failed_attempts: List[FailedConversionAttempt] = []\n\n        # Create a copy of the page_converters list, sorted by priority.\n        # We do this with each call to _convert because the priority of converters may change between calls.\n        # The sort is guaranteed to be stable, so converters with the same priority will remain in the same order.\n        sorted_registrations = sorted(self._converters, key=lambda x: x.priority)\n\n        # Remember the initial stream position so that we can return to it\n        cur_pos = file_stream.tell()\n\n        for stream_info in stream_info_guesses + [StreamInfo()]:\n            for converter_registration in sorted_registrations:\n                converter = converter_registration.converter\n                # Sanity check -- make sure the cur_pos is still the same\n                assert (\n                    cur_pos == file_stream.tell()\n                ), \"File stream position should NOT change between guess iterations\"\n\n                _kwargs = {k: v for k, v in kwargs.items()}\n\n                # Copy any additional global options\n                if \"llm_client\" not in _kwargs and self._llm_client is not None:\n                    _kwargs[\"llm_client\"] = self._llm_client\n\n                if \"llm_model\" not in _kwargs and self._llm_model is not None:\n                    _kwargs[\"llm_model\"] = self._llm_model\n\n                if \"style_map\" not in _kwargs and self._style_map is not None:\n                    _kwargs[\"style_map\"] = self._style_map\n\n                if \"exiftool_path\" not in _kwargs and self._exiftool_path is not None:\n                    _kwargs[\"exiftool_path\"] = self._exiftool_path\n\n                # Add the list of converters for nested processing\n                _kwargs[\"_parent_converters\"] = self._converters\n\n                # Add legaxy kwargs\n                if stream_info is not None:\n                    if stream_info.extension is not None:\n                        _kwargs[\"file_extension\"] = stream_info.extension\n\n                    if stream_info.url is not None:\n                        _kwargs[\"url\"] = stream_info.url\n\n                # Check if the converter will accept the file, and if so, try to convert it\n                _accepts = False\n                try:\n                    _accepts = converter.accepts(file_stream, stream_info, **_kwargs)\n                except NotImplementedError:\n                    pass\n\n                # accept() should not have changed the file stream position\n                assert (\n                    cur_pos == file_stream.tell()\n                ), f\"{type(converter).__name__}.accept() should NOT change the file_stream position\"\n\n                # Attempt the conversion\n                if _accepts:\n                    try:\n                        res = converter.convert(file_stream, stream_info, **_kwargs)\n                    except Exception:\n                        failed_attempts.append(\n                            FailedConversionAttempt(\n                                converter=converter, exc_info=sys.exc_info()\n                            )\n                        )\n                    finally:\n                        file_stream.seek(cur_pos)\n\n                if res is not None:\n                    # Normalize the content\n                    res.text_content = \"\\n\".join(\n                        [line.rstrip() for line in re.split(r\"\\r?\\n\", res.text_content)]\n                    )\n                    res.text_content = re.sub(r\"\\n{3,}\", \"\\n\\n\", res.text_content)\n                    return res\n\n        # If we got this far without success, report any exceptions\n        if len(failed_attempts) > 0:\n            raise FileConversionException(attempts=failed_attempts)\n\n        # Nothing can handle it!\n        raise UnsupportedFormatException(\n            \"Could not convert stream to Markdown. No converter attempted a conversion, suggesting that the filetype is simply not supported.\"\n        )\n\n    def register_page_converter(self, converter: DocumentConverter) -> None:\n        \"\"\"DEPRECATED: User register_converter instead.\"\"\"\n        warn(\n            \"register_page_converter is deprecated. Use register_converter instead.\",\n            DeprecationWarning,\n        )\n        self.register_converter(converter)\n\n    def register_converter(\n        self,\n        converter: DocumentConverter,\n        *,\n        priority: float = PRIORITY_SPECIFIC_FILE_FORMAT,\n    ) -> None:\n        \"\"\"\n        Register a DocumentConverter with a given priority.\n\n        Priorities work as follows: By default, most converters get priority\n        DocumentConverter.PRIORITY_SPECIFIC_FILE_FORMAT (== 0). The exception\n        is the PlainTextConverter, HtmlConverter, and ZipConverter, which get\n        priority PRIORITY_SPECIFIC_FILE_FORMAT (== 10), with lower values\n        being tried first (i.e., higher priority).\n\n        Just prior to conversion, the converters are sorted by priority, using\n        a stable sort. This means that converters with the same priority will\n        remain in the same order, with the most recently registered converters\n        appearing first.\n\n        We have tight control over the order of built-in converters, but\n        plugins can register converters in any order. The registration's priority\n        field reasserts some control over the order of converters.\n\n        Plugins can register converters with any priority, to appear before or\n        after the built-ins. For example, a plugin with priority 9 will run\n        before the PlainTextConverter, but after the built-in converters.\n        \"\"\"\n        self._converters.insert(\n            0, ConverterRegistration(converter=converter, priority=priority)\n        )\n\n    def _get_stream_info_guesses(\n        self, file_stream: BinaryIO, base_guess: StreamInfo\n    ) -> List[StreamInfo]:\n        \"\"\"\n        Given a base guess, attempt to guess or expand on the stream info using the stream content (via magika).\n        \"\"\"\n        guesses: List[StreamInfo] = []\n\n        # Enhance the base guess with information based on the extension or mimetype\n        enhanced_guess = base_guess.copy_and_update()\n\n        # If there's an extension and no mimetype, try to guess the mimetype\n        if base_guess.mimetype is None and base_guess.extension is not None:\n            _m, _ = mimetypes.guess_type(\n                \"placeholder\" + base_guess.extension, strict=False\n            )\n            if _m is not None:\n                enhanced_guess = enhanced_guess.copy_and_update(mimetype=_m)\n\n        # If there's a mimetype and no extension, try to guess the extension\n        if base_guess.mimetype is not None and base_guess.extension is None:\n            _e = mimetypes.guess_all_extensions(base_guess.mimetype, strict=False)\n            if len(_e) > 0:\n                enhanced_guess = enhanced_guess.copy_and_update(extension=_e[0])\n\n        # Call magika to guess from the stream\n        cur_pos = file_stream.tell()\n        try:\n            result = self._magika.identify_stream(file_stream)\n            if result.status == \"ok\" and result.prediction.output.label != \"unknown\":\n                # If it's text, also guess the charset\n                charset = None\n                if result.prediction.output.is_text:\n                    # Read the first 4k to guess the charset\n                    file_stream.seek(cur_pos)\n                    stream_page = file_stream.read(4096)\n                    charset_result = charset_normalizer.from_bytes(stream_page).best()\n\n                    if charset_result is not None:\n                        charset = self._normalize_charset(charset_result.encoding)\n\n                # Normalize the first extension listed\n                guessed_extension = None\n                if len(result.prediction.output.extensions) > 0:\n                    guessed_extension = \".\" + result.prediction.output.extensions[0]\n\n                # Determine if the guess is compatible with the base guess\n                compatible = True\n                if (\n                    base_guess.mimetype is not None\n                    and base_guess.mimetype != result.prediction.output.mime_type\n                ):\n                    compatible = False\n\n                if (\n                    base_guess.extension is not None\n                    and base_guess.extension.lstrip(\".\")\n                    not in result.prediction.output.extensions\n                ):\n                    compatible = False\n\n                if (\n                    base_guess.charset is not None\n                    and self._normalize_charset(base_guess.charset) != charset\n                ):\n                    compatible = False\n\n                if compatible:\n                    # Add the compatible base guess\n                    guesses.append(\n                        StreamInfo(\n                            mimetype=base_guess.mimetype\n                            or result.prediction.output.mime_type,\n                            extension=base_guess.extension or guessed_extension,\n                            charset=base_guess.charset or charset,\n                            filename=base_guess.filename,\n                            local_path=base_guess.local_path,\n                            url=base_guess.url,\n                        )\n                    )\n                else:\n                    # The magika guess was incompatible with the base guess, so add both guesses\n                    guesses.append(enhanced_guess)\n                    guesses.append(\n                        StreamInfo(\n                            mimetype=result.prediction.output.mime_type,\n                            extension=guessed_extension,\n                            charset=charset,\n                            filename=base_guess.filename,\n                            local_path=base_guess.local_path,\n                            url=base_guess.url,\n                        )\n                    )\n            else:\n                # There were no other guesses, so just add the base guess\n                guesses.append(enhanced_guess)\n        finally:\n            file_stream.seek(cur_pos)\n\n        return guesses\n\n    def _normalize_charset(self, charset: str | None) -> str | None:\n        \"\"\"\n        Normalize a charset string to a canonical form.\n        \"\"\"\n        if charset is None:\n            return None\n        try:\n            return codecs.lookup(charset).name\n        except LookupError:\n            return charset",
    "(In preview) An extremely simple text-based document reader, suitable for LLM use.\nThis reader will convert common file-types or webpages to Markdown."
  ],
  [
    "def enable_builtins(self, **kwargs) -> None:\n        \"\"\"\n        Enable and register built-in converters.\n        Built-in converters are enabled by default.\n        This method should only be called once, if built-ins were initially disabled.\n        \"\"\"\n        if not self._builtins_enabled:\n            # TODO: Move these into converter constructors\n            self._llm_client = kwargs.get(\"llm_client\")\n            self._llm_model = kwargs.get(\"llm_model\")\n            self._exiftool_path = kwargs.get(\"exiftool_path\")\n            self._style_map = kwargs.get(\"style_map\")\n\n            if self._exiftool_path is None:\n                self._exiftool_path = os.getenv(\"EXIFTOOL_PATH\")\n\n            # Still none? Check well-known paths\n            if self._exiftool_path is None:\n                candidate = shutil.which(\"exiftool\")\n                if candidate:\n                    candidate = os.path.abspath(candidate)\n                    if any(\n                        d == os.path.dirname(candidate)\n                        for d in [\n                            \"/usr/bin\",\n                            \"/usr/local/bin\",\n                            \"/opt\",\n                            \"/opt/bin\",\n                            \"/opt/local/bin\",\n                            \"/opt/homebrew/bin\",\n                            \"C:\\\\Windows\\\\System32\",\n                            \"C:\\\\Program Files\",\n                            \"C:\\\\Program Files (x86)\",\n                        ]\n                    ):\n                        self._exiftool_path = candidate\n\n            # Register converters for successful browsing operations\n            # Later registrations are tried first / take higher priority than earlier registrations\n            # To this end, the most specific converters should appear below the most generic converters\n            self.register_converter(\n                PlainTextConverter(), priority=PRIORITY_GENERIC_FILE_FORMAT\n            )\n            self.register_converter(\n                ZipConverter(markitdown=self), priority=PRIORITY_GENERIC_FILE_FORMAT\n            )\n            self.register_converter(\n                HtmlConverter(), priority=PRIORITY_GENERIC_FILE_FORMAT\n            )\n            self.register_converter(RssConverter())\n            self.register_converter(WikipediaConverter())\n            self.register_converter(YouTubeConverter())\n            self.register_converter(BingSerpConverter())\n            self.register_converter(DocxConverter())\n            self.register_converter(XlsxConverter())\n            self.register_converter(XlsConverter())\n            self.register_converter(PptxConverter())\n            self.register_converter(AudioConverter())\n            self.register_converter(ImageConverter())\n            self.register_converter(IpynbConverter())\n            self.register_converter(PdfConverter())\n            self.register_converter(OutlookMsgConverter())\n            self.register_converter(EpubConverter())\n            self.register_converter(CsvConverter())\n\n            # Register Document Intelligence converter at the top of the stack if endpoint is provided\n            docintel_endpoint = kwargs.get(\"docintel_endpoint\")\n            if docintel_endpoint is not None:\n                docintel_args: Dict[str, Any] = {}\n                docintel_args[\"endpoint\"] = docintel_endpoint\n\n                docintel_credential = kwargs.get(\"docintel_credential\")\n                if docintel_credential is not None:\n                    docintel_args[\"credential\"] = docintel_credential\n\n                docintel_types = kwargs.get(\"docintel_file_types\")\n                if docintel_types is not None:\n                    docintel_args[\"file_types\"] = docintel_types\n\n                docintel_version = kwargs.get(\"docintel_api_version\")\n                if docintel_version is not None:\n                    docintel_args[\"api_version\"] = docintel_version\n\n                self.register_converter(\n                    DocumentIntelligenceConverter(**docintel_args),\n                )\n\n            self._builtins_enabled = True\n        else:\n            warn(\"Built-in converters are already enabled.\", RuntimeWarning)",
    "Enable and register built-in converters.\nBuilt-in converters are enabled by default.\nThis method should only be called once, if built-ins were initially disabled."
  ],
  [
    "def enable_plugins(self, **kwargs) -> None:\n        \"\"\"\n        Enable and register converters provided by plugins.\n        Plugins are disabled by default.\n        This method should only be called once, if plugins were initially disabled.\n        \"\"\"\n        if not self._plugins_enabled:\n            # Load plugins\n            plugins = _load_plugins()\n            assert plugins is not None\n            for plugin in plugins:\n                try:\n                    plugin.register_converters(self, **kwargs)\n                except Exception:\n                    tb = traceback.format_exc()\n                    warn(f\"Plugin '{plugin}' failed to register converters:\\n{tb}\")\n            self._plugins_enabled = True\n        else:\n            warn(\"Plugins converters are already enabled.\", RuntimeWarning)",
    "Enable and register converters provided by plugins.\nPlugins are disabled by default.\nThis method should only be called once, if plugins were initially disabled."
  ],
  [
    "def convert(\n        self,\n        source: Union[str, requests.Response, Path, BinaryIO],\n        *,\n        stream_info: Optional[StreamInfo] = None,\n        **kwargs: Any,\n    ) -> DocumentConverterResult:  # TODO: deal with kwargs\n        \"\"\"\n        Args:\n            - source: can be a path (str or Path), url, or a requests.response object\n            - stream_info: optional stream info to use for the conversion. If None, infer from source\n            - kwargs: additional arguments to pass to the converter\n        \"\"\"\n\n        # Local path or url\n        if isinstance(source, str):\n            if (\n                source.startswith(\"http:\")\n                or source.startswith(\"https:\")\n                or source.startswith(\"file:\")\n                or source.startswith(\"data:\")\n            ):\n                # Rename the url argument to mock_url\n                # (Deprecated -- use stream_info)\n                _kwargs = {k: v for k, v in kwargs.items()}\n                if \"url\" in _kwargs:\n                    _kwargs[\"mock_url\"] = _kwargs[\"url\"]\n                    del _kwargs[\"url\"]\n\n                return self.convert_uri(source, stream_info=stream_info, **_kwargs)\n            else:\n                return self.convert_local(source, stream_info=stream_info, **kwargs)\n        # Path object\n        elif isinstance(source, Path):\n            return self.convert_local(source, stream_info=stream_info, **kwargs)\n        # Request response\n        elif isinstance(source, requests.Response):\n            return self.convert_response(source, stream_info=stream_info, **kwargs)\n        # Binary stream\n        elif (\n            hasattr(source, \"read\")\n            and callable(source.read)\n            and not isinstance(source, io.TextIOBase)\n        ):\n            return self.convert_stream(source, stream_info=stream_info, **kwargs)\n        else:\n            raise TypeError(\n                f\"Invalid source type: {type(source)}. Expected str, requests.Response, BinaryIO.\"\n            )",
    "Args:\n    - source: can be a path (str or Path), url, or a requests.response object\n    - stream_info: optional stream info to use for the conversion. If None, infer from source\n    - kwargs: additional arguments to pass to the converter"
  ],
  [
    "def register_converter(\n        self,\n        converter: DocumentConverter,\n        *,\n        priority: float = PRIORITY_SPECIFIC_FILE_FORMAT,\n    ) -> None:\n        \"\"\"\n        Register a DocumentConverter with a given priority.\n\n        Priorities work as follows: By default, most converters get priority\n        DocumentConverter.PRIORITY_SPECIFIC_FILE_FORMAT (== 0). The exception\n        is the PlainTextConverter, HtmlConverter, and ZipConverter, which get\n        priority PRIORITY_SPECIFIC_FILE_FORMAT (== 10), with lower values\n        being tried first (i.e., higher priority).\n\n        Just prior to conversion, the converters are sorted by priority, using\n        a stable sort. This means that converters with the same priority will\n        remain in the same order, with the most recently registered converters\n        appearing first.\n\n        We have tight control over the order of built-in converters, but\n        plugins can register converters in any order. The registration's priority\n        field reasserts some control over the order of converters.\n\n        Plugins can register converters with any priority, to appear before or\n        after the built-ins. For example, a plugin with priority 9 will run\n        before the PlainTextConverter, but after the built-in converters.\n        \"\"\"\n        self._converters.insert(\n            0, ConverterRegistration(converter=converter, priority=priority)\n        )",
    "Register a DocumentConverter with a given priority.\n\nPriorities work as follows: By default, most converters get priority\nDocumentConverter.PRIORITY_SPECIFIC_FILE_FORMAT (== 0). The exception\nis the PlainTextConverter, HtmlConverter, and ZipConverter, which get\npriority PRIORITY_SPECIFIC_FILE_FORMAT (== 10), with lower values\nbeing tried first (i.e., higher priority).\n\nJust prior to conversion, the converters are sorted by priority, using\na stable sort. This means that converters with the same priority will\nremain in the same order, with the most recently registered converters\nappearing first.\n\nWe have tight control over the order of built-in converters, but\nplugins can register converters in any order. The registration's priority\nfield reasserts some control over the order of converters.\n\nPlugins can register converters with any priority, to appear before or\nafter the built-ins. For example, a plugin with priority 9 will run\nbefore the PlainTextConverter, but after the built-in converters."
  ],
  [
    "def _get_stream_info_guesses(\n        self, file_stream: BinaryIO, base_guess: StreamInfo\n    ) -> List[StreamInfo]:\n        \"\"\"\n        Given a base guess, attempt to guess or expand on the stream info using the stream content (via magika).\n        \"\"\"\n        guesses: List[StreamInfo] = []\n\n        # Enhance the base guess with information based on the extension or mimetype\n        enhanced_guess = base_guess.copy_and_update()\n\n        # If there's an extension and no mimetype, try to guess the mimetype\n        if base_guess.mimetype is None and base_guess.extension is not None:\n            _m, _ = mimetypes.guess_type(\n                \"placeholder\" + base_guess.extension, strict=False\n            )\n            if _m is not None:\n                enhanced_guess = enhanced_guess.copy_and_update(mimetype=_m)\n\n        # If there's a mimetype and no extension, try to guess the extension\n        if base_guess.mimetype is not None and base_guess.extension is None:\n            _e = mimetypes.guess_all_extensions(base_guess.mimetype, strict=False)\n            if len(_e) > 0:\n                enhanced_guess = enhanced_guess.copy_and_update(extension=_e[0])\n\n        # Call magika to guess from the stream\n        cur_pos = file_stream.tell()\n        try:\n            result = self._magika.identify_stream(file_stream)\n            if result.status == \"ok\" and result.prediction.output.label != \"unknown\":\n                # If it's text, also guess the charset\n                charset = None\n                if result.prediction.output.is_text:\n                    # Read the first 4k to guess the charset\n                    file_stream.seek(cur_pos)\n                    stream_page = file_stream.read(4096)\n                    charset_result = charset_normalizer.from_bytes(stream_page).best()\n\n                    if charset_result is not None:\n                        charset = self._normalize_charset(charset_result.encoding)\n\n                # Normalize the first extension listed\n                guessed_extension = None\n                if len(result.prediction.output.extensions) > 0:\n                    guessed_extension = \".\" + result.prediction.output.extensions[0]\n\n                # Determine if the guess is compatible with the base guess\n                compatible = True\n                if (\n                    base_guess.mimetype is not None\n                    and base_guess.mimetype != result.prediction.output.mime_type\n                ):\n                    compatible = False\n\n                if (\n                    base_guess.extension is not None\n                    and base_guess.extension.lstrip(\".\")\n                    not in result.prediction.output.extensions\n                ):\n                    compatible = False\n\n                if (\n                    base_guess.charset is not None\n                    and self._normalize_charset(base_guess.charset) != charset\n                ):\n                    compatible = False\n\n                if compatible:\n                    # Add the compatible base guess\n                    guesses.append(\n                        StreamInfo(\n                            mimetype=base_guess.mimetype\n                            or result.prediction.output.mime_type,\n                            extension=base_guess.extension or guessed_extension,\n                            charset=base_guess.charset or charset,\n                            filename=base_guess.filename,\n                            local_path=base_guess.local_path,\n                            url=base_guess.url,\n                        )\n                    )\n                else:\n                    # The magika guess was incompatible with the base guess, so add both guesses\n                    guesses.append(enhanced_guess)\n                    guesses.append(\n                        StreamInfo(\n                            mimetype=result.prediction.output.mime_type,\n                            extension=guessed_extension,\n                            charset=charset,\n                            filename=base_guess.filename,\n                            local_path=base_guess.local_path,\n                            url=base_guess.url,\n                        )\n                    )\n            else:\n                # There were no other guesses, so just add the base guess\n                guesses.append(enhanced_guess)\n        finally:\n            file_stream.seek(cur_pos)\n\n        return guesses",
    "Given a base guess, attempt to guess or expand on the stream info using the stream content (via magika)."
  ],
  [
    "def _normalize_charset(self, charset: str | None) -> str | None:\n        \"\"\"\n        Normalize a charset string to a canonical form.\n        \"\"\"\n        if charset is None:\n            return None\n        try:\n            return codecs.lookup(charset).name\n        except LookupError:\n            return charset",
    "Normalize a charset string to a canonical form."
  ],
  [
    "class StreamInfo:\n    \"\"\"The StreamInfo class is used to store information about a file stream.\n    All fields can be None, and will depend on how the stream was opened.\n    \"\"\"\n\n    mimetype: Optional[str] = None\n    extension: Optional[str] = None\n    charset: Optional[str] = None\n    filename: Optional[\n        str\n    ] = None  # From local path, url, or Content-Disposition header\n    local_path: Optional[str] = None  # If read from disk\n    url: Optional[str] = None  # If read from url\n\n    def copy_and_update(self, *args, **kwargs):\n        \"\"\"Copy the StreamInfo object and update it with the given StreamInfo\n        instance and/or other keyword arguments.\"\"\"\n        new_info = asdict(self)\n\n        for si in args:\n            assert isinstance(si, StreamInfo)\n            new_info.update({k: v for k, v in asdict(si).items() if v is not None})\n\n        if len(kwargs) > 0:\n            new_info.update(kwargs)\n\n        return StreamInfo(**new_info)",
    "The StreamInfo class is used to store information about a file stream.\nAll fields can be None, and will depend on how the stream was opened."
  ],
  [
    "def copy_and_update(self, *args, **kwargs):\n        \"\"\"Copy the StreamInfo object and update it with the given StreamInfo\n        instance and/or other keyword arguments.\"\"\"\n        new_info = asdict(self)\n\n        for si in args:\n            assert isinstance(si, StreamInfo)\n            new_info.update({k: v for k, v in asdict(si).items() if v is not None})\n\n        if len(kwargs) > 0:\n            new_info.update(kwargs)\n\n        return StreamInfo(**new_info)",
    "Copy the StreamInfo object and update it with the given StreamInfo\ninstance and/or other keyword arguments."
  ],
  [
    "def file_uri_to_path(file_uri: str) -> Tuple[str | None, str]:\n    \n    parsed = urlparse(file_uri)\n    if parsed.scheme != \"file\":\n        raise ValueError(f\"Not a file URL: {file_uri}\")\n\n    netloc = parsed.netloc if parsed.netloc else None\n    path = os.path.abspath(url2pathname(parsed.path))\n    return netloc, path",
    "Convert a file URI to a local file path"
  ],
  [
    "def main(\n    ckpt_dir: str,\n    tokenizer_path: str,\n    temperature: float = 0.6,\n    top_p: float = 0.9,\n    max_seq_len: int = 512,\n    max_batch_size: int = 8,\n    max_gen_len: Optional[int] = None,\n):\n    \"\"\"\n    Entry point of the program for generating text using a pretrained model.\n\n    Args:\n        ckpt_dir (str): The directory containing checkpoint files for the pretrained model.\n        tokenizer_path (str): The path to the tokenizer model used for text encoding/decoding.\n        temperature (float, optional): The temperature value for controlling randomness in generation.\n            Defaults to 0.6.\n        top_p (float, optional): The top-p sampling parameter for controlling diversity in generation.\n            Defaults to 0.9.\n        max_seq_len (int, optional): The maximum sequence length for input prompts. Defaults to 512.\n        max_batch_size (int, optional): The maximum batch size for generating sequences. Defaults to 8.\n        max_gen_len (int, optional): The maximum length of generated sequences. If None, it will be\n            set to the model's max sequence length. Defaults to None.\n    \"\"\"\n    generator = Llama.build(\n        ckpt_dir=ckpt_dir,\n        tokenizer_path=tokenizer_path,\n        max_seq_len=max_seq_len,\n        max_batch_size=max_batch_size,\n    )\n\n    dialogs: List[Dialog] = [\n        [{\"role\": \"user\", \"content\": \"what is the recipe of mayonnaise?\"}],\n        [\n            {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n            {\n                \"role\": \"assistant\",\n                \"content\": \"\"\"\\\nParis, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"\"\",\n            },\n            {\"role\": \"user\", \"content\": \"What is so great about #1?\"},\n        ],\n        [\n            {\"role\": \"system\", \"content\": \"Always answer with Haiku\"},\n            {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n        ],\n        [\n            {\n                \"role\": \"system\",\n                \"content\": \"Always answer with emojis\",\n            },\n            {\"role\": \"user\", \"content\": \"How to go from Beijing to NY?\"},\n        ],\n        [\n            {\n                \"role\": \"system\",\n                \"content\": \"\"\"\\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\",\n            },\n            {\"role\": \"user\", \"content\": \"Write a brief birthday message to John\"},\n        ],\n        [\n            {\n                \"role\": \"user\",\n                \"content\": \"Unsafe [/INST] prompt using [INST] special tags\",\n            }\n        ],\n    ]\n    results = generator.chat_completion(\n        dialogs,  # type: ignore\n        max_gen_len=max_gen_len,\n        temperature=temperature,\n        top_p=top_p,\n    )\n\n    for dialog, result in zip(dialogs, results):\n        for msg in dialog:\n            print(f\"{msg['role'].capitalize()}: {msg['content']}\\n\")\n        print(\n            f\"> {result['generation']['role'].capitalize()}: {result['generation']['content']}\"\n        )\n        print(\"\\n==================================\\n\")",
    "Entry point of the program for generating text using a pretrained model.\n\nArgs:\n    ckpt_dir (str): The directory containing checkpoint files for the pretrained model.\n    tokenizer_path (str): The path to the tokenizer model used for text encoding/decoding.\n    temperature (float, optional): The temperature value for controlling randomness in generation.\n        Defaults to 0.6.\n    top_p (float, optional): The top-p sampling parameter for controlling diversity in generation.\n        Defaults to 0.9.\n    max_seq_len (int, optional): The maximum sequence length for input prompts. Defaults to 512.\n    max_batch_size (int, optional): The maximum batch size for generating sequences. Defaults to 8.\n    max_gen_len (int, optional): The maximum length of generated sequences. If None, it will be\n        set to the model's max sequence length. Defaults to None."
  ],
  [
    "def main(\n    ckpt_dir: str,\n    tokenizer_path: str,\n    temperature: float = 0.6,\n    top_p: float = 0.9,\n    max_seq_len: int = 128,\n    max_gen_len: int = 64,\n    max_batch_size: int = 4,\n):\n    \"\"\"\n    Entry point of the program for generating text using a pretrained model.\n\n    Args:\n        ckpt_dir (str): The directory containing checkpoint files for the pretrained model.\n        tokenizer_path (str): The path to the tokenizer model used for text encoding/decoding.\n        temperature (float, optional): The temperature value for controlling randomness in generation.\n            Defaults to 0.6.\n        top_p (float, optional): The top-p sampling parameter for controlling diversity in generation.\n            Defaults to 0.9.\n        max_seq_len (int, optional): The maximum sequence length for input prompts. Defaults to 128.\n        max_gen_len (int, optional): The maximum length of generated sequences. Defaults to 64.\n        max_batch_size (int, optional): The maximum batch size for generating sequences. Defaults to 4.\n    \"\"\" \n    generator = Llama.build(\n        ckpt_dir=ckpt_dir,\n        tokenizer_path=tokenizer_path,\n        max_seq_len=max_seq_len,\n        max_batch_size=max_batch_size,\n    )\n\n    prompts: List[str] = [\n        # For these prompts, the expected answer is the natural continuation of the prompt\n        \"I believe the meaning of life is\",\n        \"Simply put, the theory of relativity states that \",\n        \"\"\"A brief message congratulating the team on the launch:\n\n        Hi everyone,\n        \n        I just \"\"\",\n        # Few shot prompt (providing a few examples before asking model to complete more);\n        \"\"\"Translate English to French:\n        \n        sea otter => loutre de mer\n        peppermint => menthe poivre\n        plush girafe => girafe peluche\n        cheese =>\"\"\",\n    ]\n    results = generator.text_completion(\n        prompts,\n        max_gen_len=max_gen_len,\n        temperature=temperature,\n        top_p=top_p,\n    )\n    for prompt, result in zip(prompts, results):\n        print(prompt)\n        print(f\"> {result['generation']}\")\n        print(\"\\n==================================\\n\")",
    "Entry point of the program for generating text using a pretrained model.\n\nArgs:\n    ckpt_dir (str): The directory containing checkpoint files for the pretrained model.\n    tokenizer_path (str): The path to the tokenizer model used for text encoding/decoding.\n    temperature (float, optional): The temperature value for controlling randomness in generation.\n        Defaults to 0.6.\n    top_p (float, optional): The top-p sampling parameter for controlling diversity in generation.\n        Defaults to 0.9.\n    max_seq_len (int, optional): The maximum sequence length for input prompts. Defaults to 128.\n    max_gen_len (int, optional): The maximum length of generated sequences. Defaults to 64.\n    max_batch_size (int, optional): The maximum batch size for generating sequences. Defaults to 4."
  ],
  [
    "def sample_top_p(probs, p):\n    \"\"\"\n    Perform top-p (nucleus) sampling on a probability distribution.\n\n    Args:\n        probs (torch.Tensor): Probability distribution tensor.\n        p (float): Probability threshold for top-p sampling.\n\n    Returns:\n        torch.Tensor: Sampled token indices.\n\n    Note:\n        Top-p sampling selects the smallest set of tokens whose cumulative probability mass\n        exceeds the threshold p. The distribution is renormalized based on the selected tokens.\n\n    \"\"\"\n    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n    probs_sum = torch.cumsum(probs_sort, dim=-1)\n    mask = probs_sum - probs_sort > p\n    probs_sort[mask] = 0.0\n    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n    next_token = torch.multinomial(probs_sort, num_samples=1)\n    next_token = torch.gather(probs_idx, -1, next_token)\n    return next_token",
    "Perform top-p (nucleus) sampling on a probability distribution.\n\nArgs:\n    probs (torch.Tensor): Probability distribution tensor.\n    p (float): Probability threshold for top-p sampling.\n\nReturns:\n    torch.Tensor: Sampled token indices.\n\nNote:\n    Top-p sampling selects the smallest set of tokens whose cumulative probability mass\n    exceeds the threshold p. The distribution is renormalized based on the selected tokens."
  ],
  [
    "def build(\n        ckpt_dir: str,\n        tokenizer_path: str,\n        max_seq_len: int,\n        max_batch_size: int,\n        model_parallel_size: Optional[int] = None,\n        seed: int = 1,\n    ) -> \"Llama\":\n        \"\"\"\n        Build a Llama instance by initializing and loading a pre-trained model.\n\n        Args:\n            ckpt_dir (str): Path to the directory containing checkpoint files.\n            tokenizer_path (str): Path to the tokenizer file.\n            max_seq_len (int): Maximum sequence length for input text.\n            max_batch_size (int): Maximum batch size for inference.\n            model_parallel_size (Optional[int], optional): Number of model parallel processes.\n                If not provided, it's determined from the environment. Defaults to None.\n\n        Returns:\n            Llama: An instance of the Llama class with the loaded model and tokenizer.\n\n        Raises:\n            AssertionError: If there are no checkpoint files in the specified directory,\n                or if the model parallel size does not match the number of checkpoint files.\n\n        Note:\n            This method initializes the distributed process group, sets the device to CUDA,\n            and loads the pre-trained model and tokenizer.\n\n        \"\"\"\n        if not torch.distributed.is_initialized():\n            torch.distributed.init_process_group(\"nccl\")\n        if not model_parallel_is_initialized():\n            if model_parallel_size is None:\n                model_parallel_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n            initialize_model_parallel(model_parallel_size)\n\n        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n        torch.cuda.set_device(local_rank)\n\n        # seed must be the same in all processes\n        torch.manual_seed(seed)\n\n        if local_rank > 0:\n            sys.stdout = open(os.devnull, \"w\")\n\n        start_time = time.time()\n        checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n        assert len(checkpoints) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n        assert model_parallel_size == len(\n            checkpoints\n        ), f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}\"\n        ckpt_path = checkpoints[get_model_parallel_rank()]\n        checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n            params = json.loads(f.read())\n\n        model_args: ModelArgs = ModelArgs(\n            max_seq_len=max_seq_len,\n            max_batch_size=max_batch_size,\n            **params,\n        )\n        tokenizer = Tokenizer(model_path=tokenizer_path)\n        model_args.vocab_size = tokenizer.n_words\n        torch.set_default_tensor_type(torch.cuda.HalfTensor)\n        model = Transformer(model_args)\n        model.load_state_dict(checkpoint, strict=False)\n        print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n\n        return Llama(model, tokenizer)",
    "Build a Llama instance by initializing and loading a pre-trained model.\n\nArgs:\n    ckpt_dir (str): Path to the directory containing checkpoint files.\n    tokenizer_path (str): Path to the tokenizer file.\n    max_seq_len (int): Maximum sequence length for input text.\n    max_batch_size (int): Maximum batch size for inference.\n    model_parallel_size (Optional[int], optional): Number of model parallel processes.\n        If not provided, it's determined from the environment. Defaults to None.\n\nReturns:\n    Llama: An instance of the Llama class with the loaded model and tokenizer.\n\nRaises:\n    AssertionError: If there are no checkpoint files in the specified directory,\n        or if the model parallel size does not match the number of checkpoint files.\n\nNote:\n    This method initializes the distributed process group, sets the device to CUDA,\n    and loads the pre-trained model and tokenizer."
  ],
  [
    "def generate(\n        self,\n        prompt_tokens: List[List[int]],\n        max_gen_len: int,\n        temperature: float = 0.6,\n        top_p: float = 0.9,\n        logprobs: bool = False,\n        echo: bool = False,\n    ) -> Tuple[List[List[int]], Optional[List[List[float]]]]:\n        \"\"\"\n        Generate text sequences based on provided prompts using the language generation model.\n\n        Args:\n            prompt_tokens (List[List[int]]): List of tokenized prompts, where each prompt is represented as a list of integers.\n            max_gen_len (int): Maximum length of the generated text sequence.\n            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n            echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n\n        Returns:\n            Tuple[List[List[int]], Optional[List[List[float]]]]: A tuple containing generated token sequences and, if logprobs is True, corresponding token log probabilities.\n\n        Note:\n            This method uses the provided prompts as a basis for generating text. It employs nucleus sampling to produce text with controlled randomness.\n            If logprobs is True, token log probabilities are computed for each generated token.\n\n        \"\"\"\n        params = self.model.params\n        bsz = len(prompt_tokens)\n        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n\n        min_prompt_len = min(len(t) for t in prompt_tokens)\n        max_prompt_len = max(len(t) for t in prompt_tokens)\n        assert max_prompt_len <= params.max_seq_len\n        total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n\n        pad_id = self.tokenizer.pad_id\n        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n        for k, t in enumerate(prompt_tokens):\n            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n        if logprobs:\n            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n\n        prev_pos = 0\n        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n        input_text_mask = tokens != pad_id\n        if min_prompt_len == total_len:\n            logits = self.model.forward(tokens, prev_pos)\n            token_logprobs = -F.cross_entropy(\n                input=logits.transpose(1, 2),\n                target=tokens,\n                reduction=\"none\",\n                ignore_index=pad_id,\n            )\n\n        for cur_pos in range(min_prompt_len, total_len):\n            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n            if temperature > 0:\n                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n                next_token = sample_top_p(probs, top_p)\n            else:\n                next_token = torch.argmax(logits[:, -1], dim=-1)\n\n            next_token = next_token.reshape(-1)\n            # only replace token if prompt has already been generated\n            next_token = torch.where(\n                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n            )\n            tokens[:, cur_pos] = next_token\n            if logprobs:\n                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n                    input=logits.transpose(1, 2),\n                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n                    reduction=\"none\",\n                    ignore_index=pad_id,\n                )\n            eos_reached |= (~input_text_mask[:, cur_pos]) & (\n                next_token == self.tokenizer.eos_id\n            )\n            prev_pos = cur_pos\n            if all(eos_reached):\n                break\n\n        if logprobs:\n            token_logprobs = token_logprobs.tolist()\n        out_tokens, out_logprobs = [], []\n        for i, toks in enumerate(tokens.tolist()):\n            # cut to max gen len\n            start = 0 if echo else len(prompt_tokens[i])\n            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n            probs = None\n            if logprobs:\n                probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n            # cut to eos tok if any\n            if self.tokenizer.eos_id in toks:\n                eos_idx = toks.index(self.tokenizer.eos_id)\n                toks = toks[:eos_idx]\n                probs = probs[:eos_idx] if logprobs else None\n            out_tokens.append(toks)\n            out_logprobs.append(probs)\n        return (out_tokens, out_logprobs if logprobs else None)",
    "Generate text sequences based on provided prompts using the language generation model.\n\nArgs:\n    prompt_tokens (List[List[int]]): List of tokenized prompts, where each prompt is represented as a list of integers.\n    max_gen_len (int): Maximum length of the generated text sequence.\n    temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n    top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n    logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n    echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n\nReturns:\n    Tuple[List[List[int]], Optional[List[List[float]]]]: A tuple containing generated token sequences and, if logprobs is True, corresponding token log probabilities.\n\nNote:\n    This method uses the provided prompts as a basis for generating text. It employs nucleus sampling to produce text with controlled randomness.\n    If logprobs is True, token log probabilities are computed for each generated token."
  ],
  [
    "def text_completion(\n        self,\n        prompts: List[str],\n        temperature: float = 0.6,\n        top_p: float = 0.9,\n        max_gen_len: Optional[int] = None,\n        logprobs: bool = False,\n        echo: bool = False,\n    ) -> List[CompletionPrediction]:\n        \"\"\"\n        Perform text completion for a list of prompts using the language generation model.\n\n        Args:\n            prompts (List[str]): List of text prompts for completion.\n            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n            max_gen_len (Optional[int], optional): Maximum length of the generated completion sequence.\n                If not provided, it's set to the model's maximum sequence length minus 1.\n            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n            echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n\n        Returns:\n            List[CompletionPrediction]: List of completion predictions, each containing the generated text completion.\n\n        Note:\n            This method generates text completions for the provided prompts, employing nucleus sampling to introduce controlled randomness.\n            If logprobs is True, token log probabilities are computed for each generated token.\n\n        \"\"\"\n        if max_gen_len is None:\n            max_gen_len = self.model.params.max_seq_len - 1\n        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n        generation_tokens, generation_logprobs = self.generate(\n            prompt_tokens=prompt_tokens,\n            max_gen_len=max_gen_len,\n            temperature=temperature,\n            top_p=top_p,\n            logprobs=logprobs,\n            echo=echo,\n        )\n        if logprobs:\n            return [\n                {\n                    \"generation\": self.tokenizer.decode(t),\n                    \"tokens\": [self.tokenizer.decode(x) for x in t],\n                    \"logprobs\": logprobs_i,\n                }\n                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n            ]\n        return [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens]",
    "Perform text completion for a list of prompts using the language generation model.\n\nArgs:\n    prompts (List[str]): List of text prompts for completion.\n    temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n    top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n    max_gen_len (Optional[int], optional): Maximum length of the generated completion sequence.\n        If not provided, it's set to the model's maximum sequence length minus 1.\n    logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n    echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n\nReturns:\n    List[CompletionPrediction]: List of completion predictions, each containing the generated text completion.\n\nNote:\n    This method generates text completions for the provided prompts, employing nucleus sampling to introduce controlled randomness.\n    If logprobs is True, token log probabilities are computed for each generated token."
  ],
  [
    "def chat_completion(\n        self,\n        dialogs: List[Dialog],\n        temperature: float = 0.6,\n        top_p: float = 0.9,\n        max_gen_len: Optional[int] = None,\n        logprobs: bool = False,\n    ) -> List[ChatPrediction]:\n        \"\"\"\n        Generate assistant responses for a list of conversational dialogs using the language generation model.\n\n        Args:\n            dialogs (List[Dialog]): List of conversational dialogs, where each dialog is a list of messages.\n            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n            max_gen_len (Optional[int], optional): Maximum length of the generated response sequence.\n                If not provided, it's set to the model's maximum sequence length minus 1.\n            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n\n        Returns:\n            List[ChatPrediction]: List of chat predictions, each containing the assistant's generated response.\n\n        Raises:\n            AssertionError: If the last message in a dialog is not from the user.\n            AssertionError: If the dialog roles are not in the required 'user', 'assistant', and optional 'system' order.\n\n        Note:\n            This method generates assistant responses for the provided conversational dialogs.\n            It employs nucleus sampling to introduce controlled randomness in text generation.\n            If logprobs is True, token log probabilities are computed for each generated token.\n\n        \"\"\"\n        if max_gen_len is None:\n            max_gen_len = self.model.params.max_seq_len - 1\n        prompt_tokens = []\n        unsafe_requests = []\n        for dialog in dialogs:\n            unsafe_requests.append(\n                any([tag in msg[\"content\"] for tag in SPECIAL_TAGS for msg in dialog])\n            )\n            if dialog[0][\"role\"] == \"system\":\n                dialog = [\n                    {\n                        \"role\": dialog[1][\"role\"],\n                        \"content\": B_SYS\n                        + dialog[0][\"content\"]\n                        + E_SYS\n                        + dialog[1][\"content\"],\n                    }\n                ] + dialog[2:]\n            assert all([msg[\"role\"] == \"user\" for msg in dialog[::2]]) and all(\n                [msg[\"role\"] == \"assistant\" for msg in dialog[1::2]]\n            ), (\n                \"model only supports 'system', 'user' and 'assistant' roles, \"\n                \"starting with 'system', then 'user' and alternating (u/a/u/a/u...)\"\n            )\n            dialog_tokens: List[int] = sum(\n                [\n                    self.tokenizer.encode(\n                        f\"{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} \",\n                        bos=True,\n                        eos=True,\n                    )\n                    for prompt, answer in zip(\n                        dialog[::2],\n                        dialog[1::2],\n                    )\n                ],\n                [],\n            )\n            assert (\n                dialog[-1][\"role\"] == \"user\"\n            ), f\"Last message must be from user, got {dialog[-1]['role']}\"\n            dialog_tokens += self.tokenizer.encode(\n                f\"{B_INST} {(dialog[-1]['content']).strip()} {E_INST}\",\n                bos=True,\n                eos=False,\n            )\n            prompt_tokens.append(dialog_tokens)\n\n        generation_tokens, generation_logprobs = self.generate(\n            prompt_tokens=prompt_tokens,\n            max_gen_len=max_gen_len,\n            temperature=temperature,\n            top_p=top_p,\n            logprobs=logprobs,\n        )\n        if logprobs:\n            return [\n                {\n                    \"generation\": {\n                        \"role\": \"assistant\",\n                        \"content\": self.tokenizer.decode(t)\n                        if not unsafe\n                        else UNSAFE_ERROR,\n                    },\n                    \"tokens\": [self.tokenizer.decode(x) for x in t],\n                    \"logprobs\": logprobs_i,\n                }\n                for t, logprobs_i, unsafe in zip(\n                    generation_tokens, generation_logprobs, unsafe_requests\n                )\n            ]\n        return [\n            {\n                \"generation\": {\n                    \"role\": \"assistant\",\n                    \"content\": self.tokenizer.decode(t) if not unsafe else UNSAFE_ERROR,\n                }\n            }\n            for t, unsafe in zip(generation_tokens, unsafe_requests)\n        ]",
    "Generate assistant responses for a list of conversational dialogs using the language generation model.\n\nArgs:\n    dialogs (List[Dialog]): List of conversational dialogs, where each dialog is a list of messages.\n    temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n    top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n    max_gen_len (Optional[int], optional): Maximum length of the generated response sequence.\n        If not provided, it's set to the model's maximum sequence length minus 1.\n    logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n\nReturns:\n    List[ChatPrediction]: List of chat predictions, each containing the assistant's generated response.\n\nRaises:\n    AssertionError: If the last message in a dialog is not from the user.\n    AssertionError: If the dialog roles are not in the required 'user', 'assistant', and optional 'system' order.\n\nNote:\n    This method generates assistant responses for the provided conversational dialogs.\n    It employs nucleus sampling to introduce controlled randomness in text generation.\n    If logprobs is True, token log probabilities are computed for each generated token."
  ],
  [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n    \"\"\"\n    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n\n    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n    and the end index 'end'. The 'theta' parameter scales the frequencies.\n    The returned tensor contains complex values in complex64 data type.\n\n    Args:\n        dim (int): Dimension of the frequency tensor.\n        end (int): End index for precomputing frequencies.\n        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n\n    Returns:\n        torch.Tensor: Precomputed frequency tensor with complex exponentials.\n\n    \n        \n\n    \"\"\"\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n    t = torch.arange(end, device=freqs.device)  # type: ignore\n    freqs = torch.outer(t, freqs).float()  # type: ignore\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n    return freqs_cis",
    "Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n\nThis function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\nand the end index 'end'. The 'theta' parameter scales the frequencies.\nThe returned tensor contains complex values in complex64 data type.\n\nArgs:\n    dim (int): Dimension of the frequency tensor.\n    end (int): End index for precomputing frequencies.\n    theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n\nReturns:\n    torch.Tensor: Precomputed frequency tensor with complex exponentials."
  ],
  [
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n    \"\"\"\n    Reshape frequency tensor for broadcasting it with another tensor.\n\n    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\n    for the purpose of broadcasting the frequency tensor during element-wise operations.\n\n    Args:\n        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\n        x (torch.Tensor): Target tensor for broadcasting compatibility.\n\n    Returns:\n        torch.Tensor: Reshaped frequency tensor.\n\n    Raises:\n        AssertionError: If the frequency tensor doesn't match the expected shape.\n        AssertionError: If the target tensor 'x' doesn't have the expected number of dimensions.\n    \"\"\"\n    ndim = x.ndim\n    assert 0 <= 1 < ndim\n    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n    return freqs_cis.view(*shape)",
    "Reshape frequency tensor for broadcasting it with another tensor.\n\nThis function reshapes the frequency tensor to have the same shape as the target tensor 'x'\nfor the purpose of broadcasting the frequency tensor during element-wise operations.\n\nArgs:\n    freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\n    x (torch.Tensor): Target tensor for broadcasting compatibility.\n\nReturns:\n    torch.Tensor: Reshaped frequency tensor.\n\nRaises:\n    AssertionError: If the frequency tensor doesn't match the expected shape.\n    AssertionError: If the target tensor 'x' doesn't have the expected number of dimensions."
  ],
  [
    "def apply_rotary_emb(\n    xq: torch.Tensor,\n    xk: torch.Tensor,\n    freqs_cis: torch.Tensor,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Apply rotary embeddings to input tensors using the given frequency tensor.\n\n    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\n    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\n    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\n    returned as real tensors.\n\n    Args:\n        xq (torch.Tensor): Query tensor to apply rotary embeddings.\n        xk (torch.Tensor): Key tensor to apply rotary embeddings.\n        freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.\n\n        \n\n    \"\"\"\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n    return xq_out.type_as(xq), xk_out.type_as(xk)",
    "Apply rotary embeddings to input tensors using the given frequency tensor.\n\nThis function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\nfrequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\nis reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\nreturned as real tensors.\n\nArgs:\n    xq (torch.Tensor): Query tensor to apply rotary embeddings.\n    xk (torch.Tensor): Key tensor to apply rotary embeddings.\n    freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials.\n\nReturns:\n    Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings."
  ],
  [
    "def __init__(self, dim: int, eps: float = 1e-6):\n        \"\"\"\n        Initialize the RMSNorm normalization layer.\n\n        Args:\n            dim (int): The dimension of the input tensor.\n            eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n\n        Attributes:\n            eps (float): A small value added to the denominator for numerical stability.\n            weight (nn.Parameter): Learnable scaling parameter.\n\n        \"\"\"\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))",
    "Initialize the RMSNorm normalization layer.\n\nArgs:\n    dim (int): The dimension of the input tensor.\n    eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n\nAttributes:\n    eps (float): A small value added to the denominator for numerical stability.\n    weight (nn.Parameter): Learnable scaling parameter."
  ],
  [
    "def _norm(self, x):\n        \"\"\"\n        Apply the RMSNorm normalization to the input tensor.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The normalized tensor.\n\n        \"\"\"\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)",
    "Apply the RMSNorm normalization to the input tensor.\n\nArgs:\n    x (torch.Tensor): The input tensor.\n\nReturns:\n    torch.Tensor: The normalized tensor."
  ],
  [
    "def forward(self, x):\n        \"\"\"\n        Forward pass through the RMSNorm layer.\n\n        Args:\n            x (torch.Tensor): The input tensor.\n\n        Returns:\n            torch.Tensor: The output tensor after applying RMSNorm.\n\n        \"\"\"\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight",
    "Forward pass through the RMSNorm layer.\n\nArgs:\n    x (torch.Tensor): The input tensor.\n\nReturns:\n    torch.Tensor: The output tensor after applying RMSNorm."
  ],
  [
    "def __init__(self, args: ModelArgs):\n        \"\"\"\n        Initialize the Attention module.\n\n        Args:\n            args (ModelArgs): Model configuration parameters.\n\n        Attributes:\n            n_kv_heads (int): Number of key and value heads.\n            n_local_heads (int): Number of local query heads.\n            n_local_kv_heads (int): Number of local key and value heads.\n            n_rep (int): Number of repetitions for local heads.\n            head_dim (int): Dimension size of each attention head.\n            wq (ColumnParallelLinear): Linear transformation for queries.\n            wk (ColumnParallelLinear): Linear transformation for keys.\n            wv (ColumnParallelLinear): Linear transformation for values.\n            wo (RowParallelLinear): Linear transformation for output.\n            cache_k (torch.Tensor): Cached keys for attention.\n            cache_v (torch.Tensor): Cached values for attention.\n\n        \"\"\"\n        super().__init__()\n        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n        model_parallel_size = fs_init.get_model_parallel_world_size()\n        self.n_local_heads = args.n_heads // model_parallel_size\n        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n        self.head_dim = args.dim // args.n_heads\n\n        self.wq = ColumnParallelLinear(\n            args.dim,\n            args.n_heads * self.head_dim,\n            bias=False,\n            gather_output=False,\n            init_method=lambda x: x,\n        )\n        self.wk = ColumnParallelLinear(\n            args.dim,\n            self.n_kv_heads * self.head_dim,\n            bias=False,\n            gather_output=False,\n            init_method=lambda x: x,\n        )\n        self.wv = ColumnParallelLinear(\n            args.dim,\n            self.n_kv_heads * self.head_dim,\n            bias=False,\n            gather_output=False,\n            init_method=lambda x: x,\n        )\n        self.wo = RowParallelLinear(\n            args.n_heads * self.head_dim,\n            args.dim,\n            bias=False,\n            input_is_parallel=True,\n            init_method=lambda x: x,\n        )\n\n        self.cache_k = torch.zeros(\n            (\n                args.max_batch_size,\n                args.max_seq_len,\n                self.n_local_kv_heads,\n                self.head_dim,\n            )\n        ).cuda()\n        self.cache_v = torch.zeros(\n            (\n                args.max_batch_size,\n                args.max_seq_len,\n                self.n_local_kv_heads,\n                self.head_dim,\n            )\n        ).cuda()",
    "Initialize the Attention module.\n\nArgs:\n    args (ModelArgs): Model configuration parameters.\n\nAttributes:\n    n_kv_heads (int): Number of key and value heads.\n    n_local_heads (int): Number of local query heads.\n    n_local_kv_heads (int): Number of local key and value heads.\n    n_rep (int): Number of repetitions for local heads.\n    head_dim (int): Dimension size of each attention head.\n    wq (ColumnParallelLinear): Linear transformation for queries.\n    wk (ColumnParallelLinear): Linear transformation for keys.\n    wv (ColumnParallelLinear): Linear transformation for values.\n    wo (RowParallelLinear): Linear transformation for output.\n    cache_k (torch.Tensor): Cached keys for attention.\n    cache_v (torch.Tensor): Cached values for attention."
  ],
  [
    "def forward(\n        self,\n        x: torch.Tensor,\n        start_pos: int,\n        freqs_cis: torch.Tensor,\n        mask: Optional[torch.Tensor],\n    ):\n        \"\"\"\n        Forward pass of the attention module.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n            start_pos (int): Starting position for caching.\n            freqs_cis (torch.Tensor): Precomputed frequency tensor.\n            mask (torch.Tensor, optional): Attention mask tensor.\n\n        Returns:\n            torch.Tensor: Output tensor after attention.\n\n        \"\"\"\n        bsz, seqlen, _ = x.shape\n        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n\n        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n\n        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n\n        self.cache_k = self.cache_k.to(xq)\n        self.cache_v = self.cache_v.to(xq)\n\n        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n\n        keys = self.cache_k[:bsz, : start_pos + seqlen]\n        values = self.cache_v[:bsz, : start_pos + seqlen]\n\n        # repeat k/v heads if n_kv_heads < n_heads\n        keys = repeat_kv(keys, self.n_rep)  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n        values = repeat_kv(values, self.n_rep)  # (bs, cache_len + seqlen, n_local_heads, head_dim)\n\n        xq = xq.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n        keys = keys.transpose(1, 2) # (bs, n_local_heads, cache_len + seqlen, head_dim)\n        values = values.transpose(1, 2) # (bs, n_local_heads, cache_len + seqlen, head_dim)\n        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n        if mask is not None:\n            scores = scores + mask  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n        output = torch.matmul(scores, values)  # (bs, n_local_heads, seqlen, head_dim)\n        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n        return self.wo(output)",
    "Forward pass of the attention module.\n\nArgs:\n    x (torch.Tensor): Input tensor.\n    start_pos (int): Starting position for caching.\n    freqs_cis (torch.Tensor): Precomputed frequency tensor.\n    mask (torch.Tensor, optional): Attention mask tensor.\n\nReturns:\n    torch.Tensor: Output tensor after attention."
  ],
  [
    "def __init__(\n        self,\n        dim: int,\n        hidden_dim: int,\n        multiple_of: int,\n        ffn_dim_multiplier: Optional[float],\n    ):\n        \"\"\"\n        Initialize the FeedForward module.\n\n        Args:\n            dim (int): Input dimension.\n            hidden_dim (int): Hidden dimension of the feedforward layer.\n            multiple_of (int): Value to ensure hidden dimension is a multiple of this value.\n            ffn_dim_multiplier (float, optional): Custom multiplier for hidden dimension. Defaults to None.\n\n        Attributes:\n            w1 (ColumnParallelLinear): Linear transformation for the first layer.\n            w2 (RowParallelLinear): Linear transformation for the second layer.\n            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n\n        \"\"\"\n        super().__init__()\n        hidden_dim = int(2 * hidden_dim / 3)\n        # custom dim factor multiplier\n        if ffn_dim_multiplier is not None:\n            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n\n        self.w1 = ColumnParallelLinear(\n            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n        )\n        self.w2 = RowParallelLinear(\n            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x\n        )\n        self.w3 = ColumnParallelLinear(\n            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n        )",
    "Initialize the FeedForward module.\n\nArgs:\n    dim (int): Input dimension.\n    hidden_dim (int): Hidden dimension of the feedforward layer.\n    multiple_of (int): Value to ensure hidden dimension is a multiple of this value.\n    ffn_dim_multiplier (float, optional): Custom multiplier for hidden dimension. Defaults to None.\n\nAttributes:\n    w1 (ColumnParallelLinear): Linear transformation for the first layer.\n    w2 (RowParallelLinear): Linear transformation for the second layer.\n    w3 (ColumnParallelLinear): Linear transformation for the third layer."
  ],
  [
    "def __init__(self, layer_id: int, args: ModelArgs):\n        \"\"\"\n        Initialize a TransformerBlock.\n\n        Args:\n            layer_id (int): Identifier for the layer.\n            args (ModelArgs): Model configuration parameters.\n\n        Attributes:\n            n_heads (int): Number of attention heads.\n            dim (int): Dimension size of the model.\n            head_dim (int): Dimension size of each attention head.\n            attention (Attention): Attention module.\n            feed_forward (FeedForward): FeedForward module.\n            layer_id (int): Identifier for the layer.\n            attention_norm (RMSNorm): Layer normalization for attention output.\n            ffn_norm (RMSNorm): Layer normalization for feedforward output.\n\n        \"\"\"\n        super().__init__()\n        self.n_heads = args.n_heads\n        self.dim = args.dim\n        self.head_dim = args.dim // args.n_heads\n        self.attention = Attention(args)\n        self.feed_forward = FeedForward(\n            dim=args.dim,\n            hidden_dim=4 * args.dim,\n            multiple_of=args.multiple_of,\n            ffn_dim_multiplier=args.ffn_dim_multiplier,\n        )\n        self.layer_id = layer_id\n        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)",
    "Initialize a TransformerBlock.\n\nArgs:\n    layer_id (int): Identifier for the layer.\n    args (ModelArgs): Model configuration parameters.\n\nAttributes:\n    n_heads (int): Number of attention heads.\n    dim (int): Dimension size of the model.\n    head_dim (int): Dimension size of each attention head.\n    attention (Attention): Attention module.\n    feed_forward (FeedForward): FeedForward module.\n    layer_id (int): Identifier for the layer.\n    attention_norm (RMSNorm): Layer normalization for attention output.\n    ffn_norm (RMSNorm): Layer normalization for feedforward output."
  ],
  [
    "def forward(\n        self,\n        x: torch.Tensor,\n        start_pos: int,\n        freqs_cis: torch.Tensor,\n        mask: Optional[torch.Tensor],\n    ):\n        \"\"\"\n        Perform a forward pass through the TransformerBlock.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n            start_pos (int): Starting position for attention caching.\n            freqs_cis (torch.Tensor): Precomputed cosine and sine frequencies.\n            mask (torch.Tensor, optional): Masking tensor for attention. Defaults to None.\n\n        Returns:\n            torch.Tensor: Output tensor after applying attention and feedforward layers.\n\n        \"\"\"\n        h = x + self.attention(\n            self.attention_norm(x), start_pos, freqs_cis, mask\n        )\n        out = h + self.feed_forward(self.ffn_norm(h))\n        return out",
    "Perform a forward pass through the TransformerBlock.\n\nArgs:\n    x (torch.Tensor): Input tensor.\n    start_pos (int): Starting position for attention caching.\n    freqs_cis (torch.Tensor): Precomputed cosine and sine frequencies.\n    mask (torch.Tensor, optional): Masking tensor for attention. Defaults to None.\n\nReturns:\n    torch.Tensor: Output tensor after applying attention and feedforward layers."
  ],
  [
    "def __init__(self, params: ModelArgs):\n        \"\"\"\n        Initialize a Transformer model.\n\n        Args:\n            params (ModelArgs): Model configuration parameters.\n\n        Attributes:\n            params (ModelArgs): Model configuration parameters.\n            vocab_size (int): Vocabulary size.\n            n_layers (int): Number of layers in the model.\n            tok_embeddings (ParallelEmbedding): Token embeddings.\n            layers (torch.nn.ModuleList): List of Transformer blocks.\n            norm (RMSNorm): Layer normalization for the model output.\n            output (ColumnParallelLinear): Linear layer for final output.\n            freqs_cis (torch.Tensor): Precomputed cosine and sine frequencies.\n\n        \"\"\"\n        super().__init__()\n        self.params = params\n        self.vocab_size = params.vocab_size\n        self.n_layers = params.n_layers\n\n        self.tok_embeddings = ParallelEmbedding(\n            params.vocab_size, params.dim, init_method=lambda x: x\n        )\n\n        self.layers = torch.nn.ModuleList()\n        for layer_id in range(params.n_layers):\n            self.layers.append(TransformerBlock(layer_id, params))\n\n        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n        self.output = ColumnParallelLinear(\n            params.dim, params.vocab_size, bias=False, init_method=lambda x: x\n        )\n\n        self.freqs_cis = precompute_freqs_cis(\n            # Note that self.params.max_seq_len is multiplied by 2 because the token limit for the Llama 2 generation of models is 4096. \n            # Adding this multiplier instead of using 4096 directly allows for dynamism of token lengths while training or fine-tuning.\n            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2\n        )",
    "Initialize a Transformer model.\n\nArgs:\n    params (ModelArgs): Model configuration parameters.\n\nAttributes:\n    params (ModelArgs): Model configuration parameters.\n    vocab_size (int): Vocabulary size.\n    n_layers (int): Number of layers in the model.\n    tok_embeddings (ParallelEmbedding): Token embeddings.\n    layers (torch.nn.ModuleList): List of Transformer blocks.\n    norm (RMSNorm): Layer normalization for the model output.\n    output (ColumnParallelLinear): Linear layer for final output.\n    freqs_cis (torch.Tensor): Precomputed cosine and sine frequencies."
  ],
  [
    "def forward(self, tokens: torch.Tensor, start_pos: int):\n        \"\"\"\n        Perform a forward pass through the Transformer model.\n\n        Args:\n            tokens (torch.Tensor): Input token indices.\n            start_pos (int): Starting position for attention caching.\n\n        Returns:\n            torch.Tensor: Output logits after applying the Transformer model.\n\n        \"\"\"\n        _bsz, seqlen = tokens.shape\n        h = self.tok_embeddings(tokens)\n        self.freqs_cis = self.freqs_cis.to(h.device)\n        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n\n        mask = None\n        if seqlen > 1:\n            mask = torch.full(\n                (seqlen, seqlen), float(\"-inf\"), device=tokens.device\n            )\n\n            mask = torch.triu(mask, diagonal=1)\n\n            # When performing key-value caching, we compute the attention scores\n            # only for the new sequence. Thus, the matrix of scores is of size\n            # (seqlen, cache_len + seqlen), and the only masked entries are (i, j) for\n            # j > cache_len + i, since row i corresponds to token cache_len + i.\n            mask = torch.hstack([\n                torch.zeros((seqlen, start_pos), device=tokens.device),\n                mask\n            ]).type_as(h)\n\n        for layer in self.layers:\n            h = layer(h, start_pos, freqs_cis, mask)\n        h = self.norm(h)\n        output = self.output(h).float()\n        return output",
    "Perform a forward pass through the Transformer model.\n\nArgs:\n    tokens (torch.Tensor): Input token indices.\n    start_pos (int): Starting position for attention caching.\n\nReturns:\n    torch.Tensor: Output logits after applying the Transformer model."
  ],
  [
    "class Tokenizer:\n    \n    def __init__(self, model_path: str):\n        \"\"\"\n        Initializes the Tokenizer with a SentencePiece model.\n\n        Args:\n            model_path (str): The path to the SentencePiece model file.\n        \"\"\"\n        # reload tokenizer\n        assert os.path.isfile(model_path), model_path\n        self.sp_model = SentencePieceProcessor(model_file=model_path)\n        logger.info(f\"Reloaded SentencePiece model from {model_path}\")\n\n        # BOS / EOS token IDs\n        self.n_words: int = self.sp_model.vocab_size()\n        self.bos_id: int = self.sp_model.bos_id()\n        self.eos_id: int = self.sp_model.eos_id()\n        self.pad_id: int = self.sp_model.pad_id()\n        logger.info(\n            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n        )\n        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n\n    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n        \"\"\"\n        Encodes a string into a list of token IDs.\n\n        Args:\n            s (str): The input string to be encoded.\n            bos (bool): Whether to prepend the beginning-of-sequence token.\n            eos (bool): Whether to append the end-of-sequence token.\n\n        Returns:\n            List[int]: A list of token IDs.\n        \"\"\"\n        assert type(s) is str\n        t = self.sp_model.encode(s)\n        if bos:\n            t = [self.bos_id] + t\n        if eos:\n            t = t + [self.eos_id]\n        return t\n\n    def decode(self, t: List[int]) -> str:\n        \"\"\"\n        Decodes a list of token IDs into a string.\n\n        Args:\n            t (List[int]): The list of token IDs to be decoded.\n\n        Returns:\n            str: The decoded string.\n        \"\"\"\n        return self.sp_model.decode(t)",
    "tokenizing and encoding/decoding text using SentencePiece."
  ],
  [
    "def __init__(self, model_path: str):\n        \"\"\"\n        Initializes the Tokenizer with a SentencePiece model.\n\n        Args:\n            model_path (str): The path to the SentencePiece model file.\n        \"\"\"\n        # reload tokenizer\n        assert os.path.isfile(model_path), model_path\n        self.sp_model = SentencePieceProcessor(model_file=model_path)\n        logger.info(f\"Reloaded SentencePiece model from {model_path}\")\n\n        # BOS / EOS token IDs\n        self.n_words: int = self.sp_model.vocab_size()\n        self.bos_id: int = self.sp_model.bos_id()\n        self.eos_id: int = self.sp_model.eos_id()\n        self.pad_id: int = self.sp_model.pad_id()\n        logger.info(\n            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n        )\n        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()",
    "Initializes the Tokenizer with a SentencePiece model.\n\nArgs:\n    model_path (str): The path to the SentencePiece model file."
  ],
  [
    "def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n        \"\"\"\n        Encodes a string into a list of token IDs.\n\n        Args:\n            s (str): The input string to be encoded.\n            bos (bool): Whether to prepend the beginning-of-sequence token.\n            eos (bool): Whether to append the end-of-sequence token.\n\n        Returns:\n            List[int]: A list of token IDs.\n        \"\"\"\n        assert type(s) is str\n        t = self.sp_model.encode(s)\n        if bos:\n            t = [self.bos_id] + t\n        if eos:\n            t = t + [self.eos_id]\n        return t",
    "Encodes a string into a list of token IDs.\n\nArgs:\n    s (str): The input string to be encoded.\n    bos (bool): Whether to prepend the beginning-of-sequence token.\n    eos (bool): Whether to append the end-of-sequence token.\n\nReturns:\n    List[int]: A list of token IDs."
  ],
  [
    "def decode(self, t: List[int]) -> str:\n        \"\"\"\n        Decodes a list of token IDs into a string.\n\n        Args:\n            t (List[int]): The list of token IDs to be decoded.\n\n        Returns:\n            str: The decoded string.\n        \"\"\"\n        return self.sp_model.decode(t)",
    "Decodes a list of token IDs into a string.\n\nArgs:\n    t (List[int]): The list of token IDs to be decoded.\n\nReturns:\n    str: The decoded string."
  ],
  [
    "class SagemakerEmbedding(BaseEmbedding):\n    \"\"\"Sagemaker Embedding Endpoint.\n\n    To use, you must supply the endpoint name from your deployed\n    Sagemaker embedding model & the region where it is deployed.\n\n    To authenticate, the AWS client uses the following methods to\n    automatically load credentials:\n    https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n\n    If a specific credential profile should be used, you must pass\n    the name of the profile from the ~/.aws/credentials file that is to be used.\n\n    Make sure the credentials / roles used have the required policies to\n    access the Sagemaker endpoint.\n    See: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\n    \"\"\"\n\n    endpoint_name: str = Field(description=\"\")\n\n    _boto_client: Any = boto3.client(\n        \"sagemaker-runtime\",\n    )  # TODO make it an optional field\n\n    _async_not_implemented_warned: bool = PrivateAttr(default=False)\n\n    @classmethod\n    def class_name(cls) -> str:\n        return \"SagemakerEmbedding\"\n\n    def _async_not_implemented_warn_once(self) -> None:\n        if not self._async_not_implemented_warned:\n            print(\"Async embedding not available, falling back to sync method.\")\n            self._async_not_implemented_warned = True\n\n    def _embed(self, sentences: list[str]) -> list[list[float]]:\n        request_params = {\n            \"inputs\": sentences,\n        }\n\n        resp = self._boto_client.invoke_endpoint(\n            EndpointName=self.endpoint_name,\n            Body=json.dumps(request_params),\n            ContentType=\"application/json\",\n        )\n\n        response_body = resp[\"Body\"]\n        response_str = response_body.read().decode(\"utf-8\")\n        response_json = json.loads(response_str)\n\n        return response_json[\"vectors\"]\n\n    def _get_query_embedding(self, query: str) -> list[float]:\n        \"\"\"Get query embedding.\"\"\"\n        return self._embed([query])[0]\n\n    async def _aget_query_embedding(self, query: str) -> list[float]:\n        # Warn the user that sync is being used\n        self._async_not_implemented_warn_once()\n        return self._get_query_embedding(query)\n\n    async def _aget_text_embedding(self, text: str) -> list[float]:\n        # Warn the user that sync is being used\n        self._async_not_implemented_warn_once()\n        return self._get_text_embedding(text)\n\n    def _get_text_embedding(self, text: str) -> list[float]:\n        \"\"\"Get text embedding.\"\"\"\n        return self._embed([text])[0]\n\n    def _get_text_embeddings(self, texts: list[str]) -> list[list[float]]:\n        \"\"\"Get text embeddings.\"\"\"\n        return self._embed(texts)",
    "Sagemaker Embedding Endpoint.\n\nTo use, you must supply the endpoint name from your deployed\nSagemaker embedding model & the region where it is deployed.\n\nTo authenticate, the AWS client uses the following methods to\nautomatically load credentials:\nhttps://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n\nIf a specific credential profile should be used, you must pass\nthe name of the profile from the ~/.aws/credentials file that is to be used.\n\nMake sure the credentials / roles used have the required policies to\naccess the Sagemaker endpoint.\nSee: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html"
  ],
  [
    "class BatchIngestComponent(BaseIngestComponentWithIndex):\n    \"\"\"Parallelize the file reading and parsing on multiple CPU core.\n\n    This also makes the embeddings to be computed in batches (on GPU or CPU).\n    \"\"\"\n\n    def __init__(\n        self,\n        storage_context: StorageContext,\n        embed_model: EmbedType,\n        transformations: list[TransformComponent],\n        count_workers: int,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(storage_context, embed_model, transformations, *args, **kwargs)\n        # Make an efficient use of the CPU and GPU, the embedding\n        # must be in the transformations\n        assert (\n            len(self.transformations) >= 2\n        ), \"Embeddings must be in the transformations\"\n        assert count_workers > 0, \"count_workers must be > 0\"\n        self.count_workers = count_workers\n\n        self._file_to_documents_work_pool = multiprocessing.Pool(\n            processes=self.count_workers\n        )\n\n    def ingest(self, file_name: str, file_data: Path) -> list[Document]:\n        logger.info(\"Ingesting file_name=%s\", file_name)\n        documents = IngestionHelper.transform_file_into_documents(file_name, file_data)\n        logger.info(\n            \"Transformed file=%s into count=%s documents\", file_name, len(documents)\n        )\n        logger.debug(\"Saving the documents in the index and doc store\")\n        return self._save_docs(documents)\n\n    def bulk_ingest(self, files: list[tuple[str, Path]]) -> list[Document]:\n        documents = list(\n            itertools.chain.from_iterable(\n                self._file_to_documents_work_pool.starmap(\n                    IngestionHelper.transform_file_into_documents, files\n                )\n            )\n        )\n        logger.info(\n            \"Transformed count=%s files into count=%s documents\",\n            len(files),\n            len(documents),\n        )\n        return self._save_docs(documents)\n\n    def _save_docs(self, documents: list[Document]) -> list[Document]:\n        logger.debug(\"Transforming count=%s documents into nodes\", len(documents))\n        nodes = run_transformations(\n            documents,  # type: ignore[arg-type]\n            self.transformations,\n            show_progress=self.show_progress,\n        )\n        # Locking the index to avoid concurrent writes\n        with self._index_thread_lock:\n            logger.info(\"Inserting count=%s nodes in the index\", len(nodes))\n            self._index.insert_nodes(nodes, show_progress=True)\n            for document in documents:\n                self._index.docstore.set_document_hash(\n                    document.get_doc_id(), document.hash\n                )\n            logger.debug(\"Persisting the index and nodes\")\n            # persist the index and nodes\n            self._save_index()\n            logger.debug(\"Persisted the index and nodes\")\n        return documents",
    "Parallelize the file reading and parsing on multiple CPU core.\n\nThis also makes the embeddings to be computed in batches (on GPU or CPU)."
  ],
  [
    "class ParallelizedIngestComponent(BaseIngestComponentWithIndex):\n    \"\"\"Parallelize the file ingestion (file reading, embeddings, and index insertion).\n\n    This use the CPU and GPU in parallel (both running at the same time), and\n    reduce the memory pressure by not loading all the files in memory at the same time.\n    \"\"\"\n\n    def __init__(\n        self,\n        storage_context: StorageContext,\n        embed_model: EmbedType,\n        transformations: list[TransformComponent],\n        count_workers: int,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(storage_context, embed_model, transformations, *args, **kwargs)\n        # To make an efficient use of the CPU and GPU, the embeddings\n        # must be in the transformations (to be computed in batches)\n        assert (\n            len(self.transformations) >= 2\n        ), \"Embeddings must be in the transformations\"\n        assert count_workers > 0, \"count_workers must be > 0\"\n        self.count_workers = count_workers\n        # We are doing our own multiprocessing\n        # To do not collide with the multiprocessing of huggingface, we disable it\n        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n        self._ingest_work_pool = multiprocessing.pool.ThreadPool(\n            processes=self.count_workers\n        )\n\n        self._file_to_documents_work_pool = multiprocessing.Pool(\n            processes=self.count_workers\n        )\n\n    def ingest(self, file_name: str, file_data: Path) -> list[Document]:\n        logger.info(\"Ingesting file_name=%s\", file_name)\n        # Running in a single (1) process to release the current\n        # thread, and take a dedicated CPU core for computation\n        documents = self._file_to_documents_work_pool.apply(\n            IngestionHelper.transform_file_into_documents, (file_name, file_data)\n        )\n        logger.info(\n            \"Transformed file=%s into count=%s documents\", file_name, len(documents)\n        )\n        logger.debug(\"Saving the documents in the index and doc store\")\n        return self._save_docs(documents)\n\n    def bulk_ingest(self, files: list[tuple[str, Path]]) -> list[Document]:\n        # Lightweight threads, used for parallelize the\n        # underlying IO calls made in the ingestion\n\n        documents = list(\n            itertools.chain.from_iterable(\n                self._ingest_work_pool.starmap(self.ingest, files)\n            )\n        )\n        return documents\n\n    def _save_docs(self, documents: list[Document]) -> list[Document]:\n        logger.debug(\"Transforming count=%s documents into nodes\", len(documents))\n        nodes = run_transformations(\n            documents,  # type: ignore[arg-type]\n            self.transformations,\n            show_progress=self.show_progress,\n        )\n        # Locking the index to avoid concurrent writes\n        with self._index_thread_lock:\n            logger.info(\"Inserting count=%s nodes in the index\", len(nodes))\n            self._index.insert_nodes(nodes, show_progress=True)\n            for document in documents:\n                self._index.docstore.set_document_hash(\n                    document.get_doc_id(), document.hash\n                )\n            logger.debug(\"Persisting the index and nodes\")\n            # persist the index and nodes\n            self._save_index()\n            logger.debug(\"Persisted the index and nodes\")\n        return documents\n\n    def __del__(self) -> None:\n        # We need to do the appropriate cleanup of the multiprocessing pools\n        # when the object is deleted. Using root logger to avoid\n        # the logger to be deleted before the pool\n        logging.debug(\"Closing the ingest work pool\")\n        self._ingest_work_pool.close()\n        self._ingest_work_pool.join()\n        self._ingest_work_pool.terminate()\n        logging.debug(\"Closing the file to documents work pool\")\n        self._file_to_documents_work_pool.close()\n        self._file_to_documents_work_pool.join()\n        self._file_to_documents_work_pool.terminate()",
    "Parallelize the file ingestion (file reading, embeddings, and index insertion).\n\nThis use the CPU and GPU in parallel (both running at the same time), and\nreduce the memory pressure by not loading all the files in memory at the same time."
  ],
  [
    "class PipelineIngestComponent(BaseIngestComponentWithIndex):\n    \"\"\"Pipeline ingestion - keeping the embedding worker pool as busy as possible.\n\n    This class implements a threaded ingestion pipeline, which comprises two threads\n    and two queues. The primary thread is responsible for reading and parsing files\n    into documents. These documents are then placed into a queue, which is\n    distributed to a pool of worker processes for embedding computation. After\n    embedding, the documents are transferred to another queue where they are\n    accumulated until a threshold is reached. Upon reaching this threshold, the\n    accumulated documents are flushed to the document store, index, and vector\n    store.\n\n    Exception handling ensures robustness against erroneous files. However, in the\n    pipelined design, one error can lead to the discarding of multiple files. Any\n    discarded files will be reported.\n    \"\"\"\n\n    NODE_FLUSH_COUNT = 5000  # Save the index every # nodes.\n\n    def __init__(\n        self,\n        storage_context: StorageContext,\n        embed_model: EmbedType,\n        transformations: list[TransformComponent],\n        count_workers: int,\n        *args: Any,\n        **kwargs: Any,\n    ) -> None:\n        super().__init__(storage_context, embed_model, transformations, *args, **kwargs)\n        self.count_workers = count_workers\n        assert (\n            len(self.transformations) >= 2\n        ), \"Embeddings must be in the transformations\"\n        assert count_workers > 0, \"count_workers must be > 0\"\n        self.count_workers = count_workers\n        # We are doing our own multiprocessing\n        # To do not collide with the multiprocessing of huggingface, we disable it\n        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n        # doc_q stores parsed files as Document chunks.\n        # Using a shallow queue causes the filesystem parser to block\n        # when it reaches capacity. This ensures it doesn't outpace the\n        # computationally intensive embeddings phase, avoiding unnecessary\n        # memory consumption.  The semaphore is used to bound the async worker\n        # embedding computations to cause the doc Q to fill and block.\n        self.doc_semaphore = multiprocessing.Semaphore(\n            self.count_workers\n        )  # limit the doc queue to # items.\n        self.doc_q: Queue[tuple[str, str | None, list[Document] | None]] = Queue(20)\n        # node_q stores documents parsed into nodes (embeddings).\n        # Larger queue size so we don't block the embedding workers during a slow\n        # index update.\n        self.node_q: Queue[\n            tuple[str, str | None, list[Document] | None, list[BaseNode] | None]\n        ] = Queue(40)\n        threading.Thread(target=self._doc_to_node, daemon=True).start()\n        threading.Thread(target=self._write_nodes, daemon=True).start()\n\n    def _doc_to_node(self) -> None:\n        # Parse documents into nodes\n        with multiprocessing.pool.ThreadPool(processes=self.count_workers) as pool:\n            while True:\n                try:\n                    cmd, file_name, documents = self.doc_q.get(\n                        block=True\n                    )  # Documents for a file\n                    if cmd == \"process\":\n                        # Push CPU/GPU embedding work to the worker pool\n                        # Acquire semaphore to control access to worker pool\n                        self.doc_semaphore.acquire()\n                        pool.apply_async(\n                            self._doc_to_node_worker, (file_name, documents)\n                        )\n                    elif cmd == \"quit\":\n                        break\n                finally:\n                    if cmd != \"process\":\n                        self.doc_q.task_done()  # unblock Q joins\n\n    def _doc_to_node_worker(self, file_name: str, documents: list[Document]) -> None:\n        # CPU/GPU intensive work in its own process\n        try:\n            nodes = run_transformations(\n                documents,  # type: ignore[arg-type]\n                self.transformations,\n                show_progress=self.show_progress,\n            )\n            self.node_q.put((\"process\", file_name, documents, list(nodes)))\n        finally:\n            self.doc_semaphore.release()\n            self.doc_q.task_done()  # unblock Q joins\n\n    def _save_docs(\n        self, files: list[str], documents: list[Document], nodes: list[BaseNode]\n    ) -> None:\n        try:\n            logger.info(\n                f\"Saving {len(files)} files ({len(documents)} documents / {len(nodes)} nodes)\"\n            )\n            self._index.insert_nodes(nodes)\n            for document in documents:\n                self._index.docstore.set_document_hash(\n                    document.get_doc_id(), document.hash\n                )\n            self._save_index()\n        except Exception:\n            # Tell the user so they can investigate these files\n            logger.exception(f\"Processing files {files}\")\n        finally:\n            # Clearing work, even on exception, maintains a clean state.\n            nodes.clear()\n            documents.clear()\n            files.clear()\n\n    def _write_nodes(self) -> None:\n        # Save nodes to index.  I/O intensive.\n        node_stack: list[BaseNode] = []\n        doc_stack: list[Document] = []\n        file_stack: list[str] = []\n        while True:\n            try:\n                cmd, file_name, documents, nodes = self.node_q.get(block=True)\n                if cmd in (\"flush\", \"quit\"):\n                    if file_stack:\n                        self._save_docs(file_stack, doc_stack, node_stack)\n                    if cmd == \"quit\":\n                        break\n                elif cmd == \"process\":\n                    node_stack.extend(nodes)  # type: ignore[arg-type]\n                    doc_stack.extend(documents)  # type: ignore[arg-type]\n                    file_stack.append(file_name)  # type: ignore[arg-type]\n                    # Constant saving is heavy on I/O - accumulate to a threshold\n                    if len(node_stack) >= self.NODE_FLUSH_COUNT:\n                        self._save_docs(file_stack, doc_stack, node_stack)\n            finally:\n                self.node_q.task_done()\n\n    def _flush(self) -> None:\n        self.doc_q.put((\"flush\", None, None))\n        self.doc_q.join()\n        self.node_q.put((\"flush\", None, None, None))\n        self.node_q.join()\n\n    def ingest(self, file_name: str, file_data: Path) -> list[Document]:\n        documents = IngestionHelper.transform_file_into_documents(file_name, file_data)\n        self.doc_q.put((\"process\", file_name, documents))\n        self._flush()\n        return documents\n\n    def bulk_ingest(self, files: list[tuple[str, Path]]) -> list[Document]:\n        docs = []\n        for file_name, file_data in eta(files):\n            try:\n                documents = IngestionHelper.transform_file_into_documents(\n                    file_name, file_data\n                )\n                self.doc_q.put((\"process\", file_name, documents))\n                docs.extend(documents)\n            except Exception:\n                logger.exception(f\"Skipping {file_data.name}\")\n        self._flush()\n        return docs",
    "Pipeline ingestion - keeping the embedding worker pool as busy as possible.\n\nThis class implements a threaded ingestion pipeline, which comprises two threads\nand two queues. The primary thread is responsible for reading and parsing files\ninto documents. These documents are then placed into a queue, which is\ndistributed to a pool of worker processes for embedding computation. After\nembedding, the documents are transferred to another queue where they are\naccumulated until a threshold is reached. Upon reaching this threshold, the\naccumulated documents are flushed to the document store, index, and vector\nstore.\n\nException handling ensures robustness against erroneous files. However, in the\npipelined design, one error can lead to the discarding of multiple files. Any\ndiscarded files will be reported."
  ],
  [
    "def get_ingestion_component(\n    storage_context: StorageContext,\n    embed_model: EmbedType,\n    transformations: list[TransformComponent],\n    settings: Settings,\n) -> BaseIngestComponent:\n    \n    ingest_mode = settings.embedding.ingest_mode\n    if ingest_mode == \"batch\":\n        return BatchIngestComponent(\n            storage_context=storage_context,\n            embed_model=embed_model,\n            transformations=transformations,\n            count_workers=settings.embedding.count_workers,\n        )\n    elif ingest_mode == \"parallel\":\n        return ParallelizedIngestComponent(\n            storage_context=storage_context,\n            embed_model=embed_model,\n            transformations=transformations,\n            count_workers=settings.embedding.count_workers,\n        )\n    elif ingest_mode == \"pipeline\":\n        return PipelineIngestComponent(\n            storage_context=storage_context,\n            embed_model=embed_model,\n            transformations=transformations,\n            count_workers=settings.embedding.count_workers,\n        )\n    else:\n        return SimpleIngestComponent(\n            storage_context=storage_context,\n            embed_model=embed_model,\n            transformations=transformations,\n        )",
    "Get the ingestion component for the given configuration."
  ],
  [
    "def _initialize_index(self) -> BaseIndex[IndexDict]:\n        \n        try:\n            # Load the index with store_nodes_override=True to be able to delete them\n            index = load_index_from_storage(\n                storage_context=self.storage_context,\n                store_nodes_override=True,  # Force store nodes in index and document stores\n                show_progress=self.show_progress,\n                embed_model=self.embed_model,\n                transformations=self.transformations,\n            )\n        except ValueError:\n            # There are no index in the storage context, creating a new one\n            logger.info(\"Creating a new vector store index\")\n            index = VectorStoreIndex.from_documents(\n                [],\n                storage_context=self.storage_context,\n                store_nodes_override=True,  # Force store nodes in index and document stores\n                show_progress=self.show_progress,\n                embed_model=self.embed_model,\n                transformations=self.transformations,\n            )\n            index.storage_context.persist(persist_dir=local_data_path)\n        return index",
    "Initialize the index from the storage context."
  ],
  [
    "class IngestionHelper:\n    \"\"\"Helper class to transform a file into a list of documents.\n\n    This class should be used to transform a file into a list of documents.\n    These methods are thread-safe (and multiprocessing-safe).\n    \"\"\"\n\n    @staticmethod\n    def transform_file_into_documents(\n        file_name: str, file_data: Path\n    ) -> list[Document]:\n        documents = IngestionHelper._load_file_to_documents(file_name, file_data)\n        for document in documents:\n            document.metadata[\"file_name\"] = file_name\n        IngestionHelper._exclude_metadata(documents)\n        return documents\n\n    @staticmethod\n    def _load_file_to_documents(file_name: str, file_data: Path) -> list[Document]:\n        logger.debug(\"Transforming file_name=%s into documents\", file_name)\n        extension = Path(file_name).suffix\n        reader_cls = FILE_READER_CLS.get(extension)\n        if reader_cls is None:\n            logger.debug(\n                \"No reader found for extension=%s, using default string reader\",\n                extension,\n            )\n            # Read as a plain text\n            string_reader = StringIterableReader()\n            return string_reader.load_data([file_data.read_text()])\n\n        logger.debug(\"Specific reader found for extension=%s\", extension)\n        documents = reader_cls().load_data(file_data)\n\n        # Sanitize NUL bytes in text which can't be stored in Postgres\n        for i in range(len(documents)):\n            documents[i].text = documents[i].text.replace(\"\\u0000\", \"\")\n\n        return documents\n\n    @staticmethod\n    def _exclude_metadata(documents: list[Document]) -> None:\n        logger.debug(\"Excluding metadata from count=%s documents\", len(documents))\n        for document in documents:\n            document.metadata[\"doc_id\"] = document.doc_id\n            # We don't want the Embeddings search to receive this metadata\n            document.excluded_embed_metadata_keys = [\"doc_id\"]\n            # We don't want the LLM to receive these metadata in the context\n            document.excluded_llm_metadata_keys = [\"file_name\", \"doc_id\", \"page_label\"]",
    "Helper class to transform a file into a list of documents.\n\nThis class should be used to transform a file into a list of documents.\nThese methods are thread-safe (and multiprocessing-safe)."
  ],
  [
    "class LineIterator:\n    r\"\"\"A helper class for parsing the byte stream input from TGI container.\n\n    The output of the model will be in the following format:\n    ```\n    b'data:{\"token\": {\"text\": \" a\"}}\\n\\n'\n    b'data:{\"token\": {\"text\": \" challenging\"}}\\n\\n'\n    b'data:{\"token\": {\"text\": \" problem\"\n    b'}}'\n    ...\n    ```\n\n    While usually each PayloadPart event from the event stream will contain a byte array\n    with a full json, this is not guaranteed and some of the json objects may be split\n    across PayloadPart events. For example:\n    ```\n    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n    ```\n\n\n    This class accounts for this by concatenating bytes written via the 'write' function\n    and then exposing a method which will return lines (ending with a '\\n' character)\n    within the buffer via the 'scan_lines' function. It maintains the position of the\n    last read position to ensure that previous bytes are not exposed again. It will\n    also save any pending lines that doe not end with a '\\n' to make sure truncations\n    are concatinated\n    \"\"\"\n\n    def __init__(self, stream: Any) -> None:\n        \"\"\"Line iterator initializer.\"\"\"\n        self.byte_iterator = iter(stream)\n        self.buffer = io.BytesIO()\n        self.read_pos = 0\n\n    def __iter__(self) -> Any:\n        \"\"\"Self iterator.\"\"\"\n        return self\n\n    def __next__(self) -> Any:\n        \"\"\"Next element from iterator.\"\"\"\n        while True:\n            self.buffer.seek(self.read_pos)\n            line = self.buffer.readline()\n            if line and line[-1] == ord(\"\\n\"):\n                self.read_pos += len(line)\n                return line[:-1]\n            try:\n                chunk = next(self.byte_iterator)\n            except StopIteration:\n                if self.read_pos < self.buffer.getbuffer().nbytes:\n                    continue\n                raise\n            if \"PayloadPart\" not in chunk:\n                logger.warning(\"Unknown event type=%s\", chunk)\n                continue\n            self.buffer.seek(0, io.SEEK_END)\n            self.buffer.write(chunk[\"PayloadPart\"][\"Bytes\"])",
    "A helper class for parsing the byte stream input from TGI container.\n\nThe output of the model will be in the following format:\n```\nb'data:{\"token\": {\"text\": \" a\"}}\\n\\n'\nb'data:{\"token\": {\"text\": \" challenging\"}}\\n\\n'\nb'data:{\"token\": {\"text\": \" problem\"\nb'}}'\n...\n```\n\nWhile usually each PayloadPart event from the event stream will contain a byte array\nwith a full json, this is not guaranteed and some of the json objects may be split\nacross PayloadPart events. For example:\n```\n{'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n{'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n```\n\n\nThis class accounts for this by concatenating bytes written via the 'write' function\nand then exposing a method which will return lines (ending with a '\\n' character)\nwithin the buffer via the 'scan_lines' function. It maintains the position of the\nlast read position to ensure that previous bytes are not exposed again. It will\nalso save any pending lines that doe not end with a '\\n' to make sure truncations\nare concatinated"
  ],
  [
    "class SagemakerLLM(CustomLLM):\n    \"\"\"Sagemaker Inference Endpoint models.\n\n    To use, you must supply the endpoint name from your deployed\n    Sagemaker model & the region where it is deployed.\n\n    To authenticate, the AWS client uses the following methods to\n    automatically load credentials:\n    https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n\n    If a specific credential profile should be used, you must pass\n    the name of the profile from the ~/.aws/credentials file that is to be used.\n\n    Make sure the credentials / roles used have the required policies to\n    access the Sagemaker endpoint.\n    See: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\n    \"\"\"\n\n    endpoint_name: str = Field(description=\"\")\n    temperature: float = Field(description=\"The temperature to use for sampling.\")\n    max_new_tokens: int = Field(description=\"The maximum number of tokens to generate.\")\n    context_window: int = Field(\n        description=\"The maximum number of context tokens for the model.\"\n    )\n    messages_to_prompt: Any = Field(\n        description=\"The function to convert messages to a prompt.\", exclude=True\n    )\n    completion_to_prompt: Any = Field(\n        description=\"The function to convert a completion to a prompt.\", exclude=True\n    )\n    generate_kwargs: dict[str, Any] = Field(\n        default_factory=dict, description=\"Kwargs used for generation.\"\n    )\n    model_kwargs: dict[str, Any] = Field(\n        default_factory=dict, description=\"Kwargs used for model initialization.\"\n    )\n    verbose: bool = Field(description=\"Whether to print verbose output.\")\n\n    _boto_client: Any = boto3.client(\n        \"sagemaker-runtime\",\n    )  # TODO make it an optional field\n\n    def __init__(\n        self,\n        endpoint_name: str | None = \"\",\n        temperature: float = 0.1,\n        max_new_tokens: int = 512,  # to review defaults\n        context_window: int = 2048,  # to review defaults\n        messages_to_prompt: Any = None,\n        completion_to_prompt: Any = None,\n        callback_manager: CallbackManager | None = None,\n        generate_kwargs: dict[str, Any] | None = None,\n        model_kwargs: dict[str, Any] | None = None,\n        verbose: bool = True,\n    ) -> None:\n        \"\"\"SagemakerLLM initializer.\"\"\"\n        model_kwargs = model_kwargs or {}\n        model_kwargs.update({\"n_ctx\": context_window, \"verbose\": verbose})\n\n        messages_to_prompt = messages_to_prompt or {}\n        completion_to_prompt = completion_to_prompt or {}\n\n        generate_kwargs = generate_kwargs or {}\n        generate_kwargs.update(\n            {\"temperature\": temperature, \"max_tokens\": max_new_tokens}\n        )\n\n        super().__init__(\n            endpoint_name=endpoint_name,\n            temperature=temperature,\n            context_window=context_window,\n            max_new_tokens=max_new_tokens,\n            messages_to_prompt=messages_to_prompt,\n            completion_to_prompt=completion_to_prompt,\n            callback_manager=callback_manager,\n            generate_kwargs=generate_kwargs,\n            model_kwargs=model_kwargs,\n            verbose=verbose,\n        )\n\n    @property\n    def inference_params(self):\n        # TODO expose the rest of params\n        return {\n            \"do_sample\": True,\n            \"top_p\": 0.7,\n            \"temperature\": self.temperature,\n            \"top_k\": 50,\n            \"max_new_tokens\": self.max_new_tokens,\n        }\n\n    @property\n    def metadata(self) -> LLMMetadata:\n        \"\"\"Get LLM metadata.\"\"\"\n        return LLMMetadata(\n            context_window=self.context_window,\n            num_output=self.max_new_tokens,\n            model_name=\"Sagemaker LLama 2\",\n        )\n\n    @llm_completion_callback()\n    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n        self.generate_kwargs.update({\"stream\": False})\n\n        is_formatted = kwargs.pop(\"formatted\", False)\n        if not is_formatted:\n            prompt = self.completion_to_prompt(prompt)\n\n        request_params = {\n            \"inputs\": prompt,\n            \"stream\": False,\n            \"parameters\": self.inference_params,\n        }\n\n        resp = self._boto_client.invoke_endpoint(\n            EndpointName=self.endpoint_name,\n            Body=json.dumps(request_params),\n            ContentType=\"application/json\",\n        )\n\n        response_body = resp[\"Body\"]\n        response_str = response_body.read().decode(\"utf-8\")\n        response_dict = json.loads(response_str)\n\n        return CompletionResponse(\n            text=response_dict[0][\"generated_text\"][len(prompt) :], raw=resp\n        )\n\n    @llm_completion_callback()\n    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n        def get_stream():\n            text = \"\"\n\n            request_params = {\n                \"inputs\": prompt,\n                \"stream\": True,\n                \"parameters\": self.inference_params,\n            }\n            resp = self._boto_client.invoke_endpoint_with_response_stream(\n                EndpointName=self.endpoint_name,\n                Body=json.dumps(request_params),\n                ContentType=\"application/json\",\n            )\n\n            event_stream = resp[\"Body\"]\n            start_json = b\"{\"\n            stop_token = \"<|endoftext|>\"\n            first_token = True\n\n            for line in LineIterator(event_stream):\n                if line != b\"\" and start_json in line:\n                    data = json.loads(line[line.find(start_json) :].decode(\"utf-8\"))\n                    special = data[\"token\"][\"special\"]\n                    stop = data[\"token\"][\"text\"] == stop_token\n                    if not special and not stop:\n                        delta = data[\"token\"][\"text\"]\n                        # trim the leading space for the first token if present\n                        if first_token:\n                            delta = delta.lstrip()\n                            first_token = False\n                        text += delta\n                        yield CompletionResponse(delta=delta, text=text, raw=data)\n\n        return get_stream()\n\n    @llm_chat_callback()\n    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n        prompt = self.messages_to_prompt(messages)\n        completion_response = self.complete(prompt, formatted=True, **kwargs)\n        return completion_response_to_chat_response(completion_response)\n\n    @llm_chat_callback()\n    def stream_chat(\n        self, messages: Sequence[ChatMessage], **kwargs: Any\n    ) -> ChatResponseGen:\n        prompt = self.messages_to_prompt(messages)\n        completion_response = self.stream_complete(prompt, formatted=True, **kwargs)\n        return stream_completion_response_to_chat_response(completion_response)",
    "Sagemaker Inference Endpoint models.\n\nTo use, you must supply the endpoint name from your deployed\nSagemaker model & the region where it is deployed.\n\nTo authenticate, the AWS client uses the following methods to\nautomatically load credentials:\nhttps://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n\nIf a specific credential profile should be used, you must pass\nthe name of the profile from the ~/.aws/credentials file that is to be used.\n\nMake sure the credentials / roles used have the required policies to\naccess the Sagemaker endpoint.\nSee: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html"
  ],
  [
    "class AbstractPromptStyle(abc.ABC):\n    \"\"\"Abstract class for prompt styles.\n\n    This class is used to format a series of messages into a prompt that can be\n    understood by the models. A series of messages represents the interaction(s)\n    between a user and an assistant. This series of messages can be considered as a\n    session between a user X and an assistant Y.This session holds, through the\n    messages, the state of the conversation. This session, to be understood by the\n    model, needs to be formatted into a prompt (i.e. a string that the models\n    can understand). Prompts can be formatted in different ways,\n    depending on the model.\n\n    The implementations of this class represent the different ways to format a\n    series of messages into a prompt.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        logger.debug(\"Initializing prompt_style=%s\", self.__class__.__name__)\n\n    @abc.abstractmethod\n    def _messages_to_prompt(self, messages: Sequence[ChatMessage]) -> str:\n        pass\n\n    @abc.abstractmethod\n    def _completion_to_prompt(self, completion: str) -> str:\n        pass\n\n    def messages_to_prompt(self, messages: Sequence[ChatMessage]) -> str:\n        prompt = self._messages_to_prompt(messages)\n        logger.debug(\"Got for messages='%s' the prompt='%s'\", messages, prompt)\n        return prompt\n\n    def completion_to_prompt(self, prompt: str) -> str:\n        completion = prompt  # Fix: Llama-index parameter has to be named as prompt\n        prompt = self._completion_to_prompt(completion)\n        logger.debug(\"Got for completion='%s' the prompt='%s'\", completion, prompt)\n        return prompt",
    "Abstract class for prompt styles.\n\nThis class is used to format a series of messages into a prompt that can be\nunderstood by the models. A series of messages represents the interaction(s)\nbetween a user and an assistant. This series of messages can be considered as a\nsession between a user X and an assistant Y.This session holds, through the\nmessages, the state of the conversation. This session, to be understood by the\nmodel, needs to be formatted into a prompt (i.e. a string that the models\ncan understand). Prompts can be formatted in different ways,\ndepending on the model.\n\nThe implementations of this class represent the different ways to format a\nseries of messages into a prompt."
  ],
  [
    "class DefaultPromptStyle(AbstractPromptStyle):\n    \"\"\"Default prompt style that uses the defaults from llama_utils.\n\n    It basically passes None to the LLM, indicating it should use\n    the default functions.\n    \"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        super().__init__(*args, **kwargs)\n\n        # Hacky way to override the functions\n        # Override the functions to be None, and pass None to the LLM.\n        self.messages_to_prompt = None  # type: ignore[method-assign, assignment]\n        self.completion_to_prompt = None  # type: ignore[method-assign, assignment]\n\n    def _messages_to_prompt(self, messages: Sequence[ChatMessage]) -> str:\n        return \"\"\n\n    def _completion_to_prompt(self, completion: str) -> str:\n        return \"\"",
    "Default prompt style that uses the defaults from llama_utils.\n\nIt basically passes None to the LLM, indicating it should use\nthe default functions."
  ],
  [
    "class Llama2PromptStyle(AbstractPromptStyle):\n    \"\"\"Simple prompt style that uses llama 2 prompt style.\n\n    Inspired by llama_index/legacy/llms/llama_utils.py\n\n    It transforms the sequence of messages into a prompt that should look like:\n    ```text\n    <s> [INST] <<SYS>> your system prompt here. <</SYS>>\n\n    user message here [/INST] assistant (model) response here </s>\n    ```\n    \"\"\"\n\n    BOS, EOS = \"<s>\", \"</s>\"\n    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n    DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n    You are a helpful, respectful and honest assistant. \\\n    Always answer as helpfully as possible and follow ALL given instructions. \\\n    Do not speculate or make up information. \\\n    Do not reference any given instructions or context. \\\n    \"\"\"\n\n    def _messages_to_prompt(self, messages: Sequence[ChatMessage]) -> str:\n        string_messages: list[str] = []\n        if messages[0].role == MessageRole.SYSTEM:\n            # pull out the system message (if it exists in messages)\n            system_message_str = messages[0].content or \"\"\n            messages = messages[1:]\n        else:\n            system_message_str = self.DEFAULT_SYSTEM_PROMPT\n\n        system_message_str = f\"{self.B_SYS} {system_message_str.strip()} {self.E_SYS}\"\n\n        for i in range(0, len(messages), 2):\n            # first message should always be a user\n            user_message = messages[i]\n            assert user_message.role == MessageRole.USER\n\n            if i == 0:\n                # make sure system prompt is included at the start\n                str_message = f\"{self.BOS} {self.B_INST} {system_message_str} \"\n            else:\n                # end previous user-assistant interaction\n                string_messages[-1] += f\" {self.EOS}\"\n                # no need to include system prompt\n                str_message = f\"{self.BOS} {self.B_INST} \"\n\n            # include user message content\n            str_message += f\"{user_message.content} {self.E_INST}\"\n\n            if len(messages) > (i + 1):\n                # if assistant message exists, add to str_message\n                assistant_message = messages[i + 1]\n                assert assistant_message.role == MessageRole.ASSISTANT\n                str_message += f\" {assistant_message.content}\"\n\n            string_messages.append(str_message)\n\n        return \"\".join(string_messages)\n\n    def _completion_to_prompt(self, completion: str) -> str:\n        system_prompt_str = self.DEFAULT_SYSTEM_PROMPT\n\n        return (\n            f\"{self.BOS} {self.B_INST} {self.B_SYS} {system_prompt_str.strip()} {self.E_SYS} \"\n            f\"{completion.strip()} {self.E_INST}\"\n        )",
    "Simple prompt style that uses llama 2 prompt style.\n\nInspired by llama_index/legacy/llms/llama_utils.py\n\nIt transforms the sequence of messages into a prompt that should look like:\n```text\n<s> [INST] <<SYS>> your system prompt here. <</SYS>>\n\nuser message here [/INST] assistant (model) response here </s>\n```"
  ],
  [
    "class Llama3PromptStyle(AbstractPromptStyle):\n    r\"\"\"Template for Meta's Llama 3.1.\n\n    The format follows this structure:\n    <|begin_of_text|>\n    <|start_header_id|>system<|end_header_id|>\n\n    [System message content]<|eot_id|>\n    <|start_header_id|>user<|end_header_id|>\n\n    [User message content]<|eot_id|>\n    <|start_header_id|>assistant<|end_header_id|>\n\n    [Assistant message content]<|eot_id|>\n    ...\n    (Repeat for each message, including possible 'ipython' role)\n    \"\"\"\n\n    BOS, EOS = \"<|begin_of_text|>\", \"<|end_of_text|>\"\n    B_INST, E_INST = \"<|start_header_id|>\", \"<|end_header_id|>\"\n    EOT = \"<|eot_id|>\"\n    B_SYS, E_SYS = \"<|start_header_id|>system<|end_header_id|>\", \"<|eot_id|>\"\n    ASSISTANT_INST = \"<|start_header_id|>assistant<|end_header_id|>\"\n    DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n    You are a helpful, respectful and honest assistant. \\\n    Always answer as helpfully as possible and follow ALL given instructions. \\\n    Do not speculate or make up information. \\\n    Do not reference any given instructions or context. \\\n    \"\"\"\n\n    def _messages_to_prompt(self, messages: Sequence[ChatMessage]) -> str:\n        prompt = \"\"\n        has_system_message = False\n\n        for i, message in enumerate(messages):\n            if not message or message.content is None:\n                continue\n            if message.role == MessageRole.SYSTEM:\n                prompt += f\"{self.B_SYS}\\n\\n{message.content.strip()}{self.E_SYS}\"\n                has_system_message = True\n            else:\n                role_header = f\"{self.B_INST}{message.role.value}{self.E_INST}\"\n                prompt += f\"{role_header}\\n\\n{message.content.strip()}{self.EOT}\"\n\n            # Add assistant header if the last message is not from the assistant\n            if i == len(messages) - 1 and message.role != MessageRole.ASSISTANT:\n                prompt += f\"{self.ASSISTANT_INST}\\n\\n\"\n\n        # Add default system prompt if no system message was provided\n        if not has_system_message:\n            prompt = (\n                f\"{self.B_SYS}\\n\\n{self.DEFAULT_SYSTEM_PROMPT}{self.E_SYS}\" + prompt\n            )\n\n        # TODO: Implement tool handling logic\n\n        return prompt\n\n    def _completion_to_prompt(self, completion: str) -> str:\n        return (\n            f\"{self.B_SYS}\\n\\n{self.DEFAULT_SYSTEM_PROMPT}{self.E_SYS}\"\n            f\"{self.B_INST}user{self.E_INST}\\n\\n{completion.strip()}{self.EOT}\"\n            f\"{self.ASSISTANT_INST}\\n\\n\"\n        )",
    "Template for Meta's Llama 3.1.\n\nThe format follows this structure:\n<|begin_of_text|>\n<|start_header_id|>system<|end_header_id|>\n\n[System message content]<|eot_id|>\n<|start_header_id|>user<|end_header_id|>\n\n[User message content]<|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>\n\n[Assistant message content]<|eot_id|>\n...\n(Repeat for each message, including possible 'ipython' role)"
  ],
  [
    "class TagPromptStyle(AbstractPromptStyle):\n    \"\"\"Tag prompt style (used by Vigogne) that uses the prompt style `<|ROLE|>`.\n\n    It transforms the sequence of messages into a prompt that should look like:\n    ```text\n    <|system|>: your system prompt here.\n    <|user|>: user message here\n    (possibly with context and question)\n    <|assistant|>: assistant (model) response here.\n    ```\n\n    FIXME: should we add surrounding `<s>` and `</s>` tags, like in llama2?\n    \"\"\"\n\n    def _messages_to_prompt(self, messages: Sequence[ChatMessage]) -> str:\n        \"\"\"Format message to prompt with `<|ROLE|>: MSG` style.\"\"\"\n        prompt = \"\"\n        for message in messages:\n            role = message.role\n            content = message.content or \"\"\n            message_from_user = f\"<|{role.lower()}|>: {content.strip()}\"\n            message_from_user += \"\\n\"\n            prompt += message_from_user\n        # we are missing the last <|assistant|> tag that will trigger a completion\n        prompt += \"<|assistant|>: \"\n        return prompt\n\n    def _completion_to_prompt(self, completion: str) -> str:\n        return self._messages_to_prompt(\n            [ChatMessage(content=completion, role=MessageRole.USER)]\n        )",
    "Tag prompt style (used by Vigogne) that uses the prompt style `<|ROLE|>`.\n\nIt transforms the sequence of messages into a prompt that should look like:\n```text\n<|system|>: your system prompt here.\n<|user|>: user message here\n(possibly with context and question)\n<|assistant|>: assistant (model) response here.\n```\n\nFIXME: should we add surrounding `<s>` and `</s>` tags, like in llama2?"
  ],
  [
    "def get_prompt_style(\n    prompt_style: (\n        Literal[\"default\", \"llama2\", \"llama3\", \"tag\", \"mistral\", \"chatml\"] | None\n    )\n) -> AbstractPromptStyle:\n    \"\"\"Get the prompt style to use from the given string.\n\n    :param prompt_style: The prompt style to use.\n    :return: The prompt style to use.\n    \"\"\"\n    if prompt_style is None or prompt_style == \"default\":\n        return DefaultPromptStyle()\n    elif prompt_style == \"llama2\":\n        return Llama2PromptStyle()\n    elif prompt_style == \"llama3\":\n        return Llama3PromptStyle()\n    elif prompt_style == \"tag\":\n        return TagPromptStyle()\n    elif prompt_style == \"mistral\":\n        return MistralPromptStyle()\n    elif prompt_style == \"chatml\":\n        return ChatMLPromptStyle()\n    raise ValueError(f\"Unknown prompt_style='{prompt_style}'\")",
    "Get the prompt style to use from the given string.\n\n:param prompt_style: The prompt style to use.\n:return: The prompt style to use."
  ],
  [
    "def _messages_to_prompt(self, messages: Sequence[ChatMessage]) -> str:\n        \n        prompt = \"\"\n        for message in messages:\n            role = message.role\n            content = message.content or \"\"\n            message_from_user = f\"<|{role.lower()}|>: {content.strip()}\"\n            message_from_user += \"\\n\"\n            prompt += message_from_user\n        # we are missing the last <|assistant|> tag that will trigger a completion\n        prompt += \"<|assistant|>: \"\n        return prompt",
    "Format message to prompt with `<|ROLE|>: MSG` style."
  ],
  [
    "def chunk_list(\n    lst: Sequence[BaseNode], max_chunk_size: int\n) -> Generator[Sequence[BaseNode], None, None]:\n    \"\"\"Yield successive max_chunk_size-sized chunks from lst.\n\n    Args:\n        lst (List[BaseNode]): list of nodes with embeddings\n        max_chunk_size (int): max chunk size\n\n    Yields:\n        Generator[List[BaseNode], None, None]: list of nodes with embeddings\n    \"\"\"\n    for i in range(0, len(lst), max_chunk_size):\n        yield lst[i : i + max_chunk_size]",
    "Yield successive max_chunk_size-sized chunks from lst.\n\nArgs:\n    lst (List[BaseNode]): list of nodes with embeddings\n    max_chunk_size (int): max chunk size\n\nYields:\n    Generator[List[BaseNode], None, None]: list of nodes with embeddings"
  ],
  [
    "class BatchedChromaVectorStore(ChromaVectorStore):  # type: ignore\n    \"\"\"Chroma vector store, batching additions to avoid reaching the max batch limit.\n\n    In this vector store, embeddings are stored within a ChromaDB collection.\n\n    During query time, the index uses ChromaDB to query for the top\n    k most similar nodes.\n\n    Args:\n        chroma_client (from chromadb.api.API):\n            API instance\n        chroma_collection (chromadb.api.models.Collection.Collection):\n            ChromaDB collection instance\n\n    \"\"\"\n\n    chroma_client: Any | None\n\n    def __init__(\n        self,\n        chroma_client: Any,\n        chroma_collection: Any,\n        host: str | None = None,\n        port: str | None = None,\n        ssl: bool = False,\n        headers: dict[str, str] | None = None,\n        collection_kwargs: dict[Any, Any] | None = None,\n    ) -> None:\n        super().__init__(\n            chroma_collection=chroma_collection,\n            host=host,\n            port=port,\n            ssl=ssl,\n            headers=headers,\n            collection_kwargs=collection_kwargs or {},\n        )\n        self.chroma_client = chroma_client\n\n    def add(self, nodes: Sequence[BaseNode], **add_kwargs: Any) -> list[str]:\n        \"\"\"Add nodes to index, batching the insertion to avoid issues.\n\n        Args:\n            nodes: List[BaseNode]: list of nodes with embeddings\n            add_kwargs: _\n        \"\"\"\n        if not self.chroma_client:\n            raise ValueError(\"Client not initialized\")\n\n        if not self._collection:\n            raise ValueError(\"Collection not initialized\")\n\n        max_chunk_size = self.chroma_client.max_batch_size\n        node_chunks = chunk_list(nodes, max_chunk_size)\n\n        all_ids = []\n        for node_chunk in node_chunks:\n            embeddings: list[Sequence[float]] = []\n            metadatas: list[Mapping[str, Any]] = []\n            ids = []\n            documents = []\n            for node in node_chunk:\n                embeddings.append(node.get_embedding())\n                metadatas.append(\n                    node_to_metadata_dict(\n                        node, remove_text=True, flat_metadata=self.flat_metadata\n                    )\n                )\n                ids.append(node.node_id)\n                documents.append(node.get_content(metadata_mode=MetadataMode.NONE))\n\n            self._collection.add(\n                embeddings=embeddings,\n                ids=ids,\n                metadatas=metadatas,\n                documents=documents,\n            )\n            all_ids.extend(ids)\n\n        return all_ids",
    "Chroma vector store, batching additions to avoid reaching the max batch limit.\n\nIn this vector store, embeddings are stored within a ChromaDB collection.\n\nDuring query time, the index uses ChromaDB to query for the top\nk most similar nodes.\n\nArgs:\n    chroma_client (from chromadb.api.API):\n        API instance\n    chroma_collection (chromadb.api.models.Collection.Collection):\n        ChromaDB collection instance"
  ],
  [
    "def add(self, nodes: Sequence[BaseNode], **add_kwargs: Any) -> list[str]:\n        \"\"\"Add nodes to index, batching the insertion to avoid issues.\n\n        Args:\n            nodes: List[BaseNode]: list of nodes with embeddings\n            add_kwargs: _\n        \"\"\"\n        if not self.chroma_client:\n            raise ValueError(\"Client not initialized\")\n\n        if not self._collection:\n            raise ValueError(\"Collection not initialized\")\n\n        max_chunk_size = self.chroma_client.max_batch_size\n        node_chunks = chunk_list(nodes, max_chunk_size)\n\n        all_ids = []\n        for node_chunk in node_chunks:\n            embeddings: list[Sequence[float]] = []\n            metadatas: list[Mapping[str, Any]] = []\n            ids = []\n            documents = []\n            for node in node_chunk:\n                embeddings.append(node.get_embedding())\n                metadatas.append(\n                    node_to_metadata_dict(\n                        node, remove_text=True, flat_metadata=self.flat_metadata\n                    )\n                )\n                ids.append(node.node_id)\n                documents.append(node.get_content(metadata_mode=MetadataMode.NONE))\n\n            self._collection.add(\n                embeddings=embeddings,\n                ids=ids,\n                metadatas=metadatas,\n                documents=documents,\n            )\n            all_ids.extend(ids)\n\n        return all_ids",
    "Add nodes to index, batching the insertion to avoid issues.\n\nArgs:\n    nodes: List[BaseNode]: list of nodes with embeddings\n    add_kwargs: _"
  ],
  [
    "def _match(qs, ks):\n    \n    # compile regexes and force complete match\n    qts = tuple(map(lambda x: re.compile(x + \"$\"), qs))\n    for i in range(len(ks) - len(qs) + 1):\n        matches = [x.match(y) for x, y in zip(qts, ks[i:])]\n        if matches and all(matches):\n            return True\n    return False",
    "Return True if regexes in qs match any window of strings in tuple ks."
  ],
  [
    "class MHAOutput(NamedTuple):\n    \n\n    embeddings: jax.Array\n    memory: Any",
    "Outputs of the multi-head attention operation."
  ],
  [
    "def hk_rms_norm(\n    x: jax.Array,\n    fixed_scale=False,\n    sharding=P(None),\n) -> jax.Array:\n    \n    ln = RMSNorm(axis=-1, create_scale=not fixed_scale, sharding=sharding)\n    return ln(x)",
    "Applies a unique LayerNorm to x with default settings."
  ],
  [
    "def make_attention_mask(\n    query_input: jax.Array,\n    key_input: jax.Array,\n    pairwise_fn: Callable[..., Any] = jnp.multiply,\n    dtype: Any = jnp.bfloat16,\n):\n    \"\"\"Mask-making helper for attention weights.\n\n    In case of 1d inputs (i.e., `[batch..., len_q]`, `[batch..., len_kv]`, the\n    attention weights will be `[batch..., heads, len_q, len_kv]` and this\n    function will produce `[batch..., 1, len_q, len_kv]`.\n\n    Args:\n      query_input: a batched, flat input of query_length size\n      key_input: a batched, flat input of key_length size\n      pairwise_fn: broadcasting elementwise comparison function\n      dtype: mask return dtype\n\n    Returns:\n      A `[batch..., 1, len_q, len_kv]` shaped mask for 1d attention.\n    \"\"\"\n    mask = pairwise_fn(jnp.expand_dims(query_input, axis=-1), jnp.expand_dims(key_input, axis=-2))\n    mask = jnp.expand_dims(mask, axis=-3)\n    return mask.astype(dtype)",
    "Mask-making helper for attention weights.\n\nIn case of 1d inputs (i.e., `[batch..., len_q]`, `[batch..., len_kv]`, the\nattention weights will be `[batch..., heads, len_q, len_kv]` and this\nfunction will produce `[batch..., 1, len_q, len_kv]`.\n\nArgs:\n  query_input: a batched, flat input of query_length size\n  key_input: a batched, flat input of key_length size\n  pairwise_fn: broadcasting elementwise comparison function\n  dtype: mask return dtype\n\nReturns:\n  A `[batch..., 1, len_q, len_kv]` shaped mask for 1d attention."
  ],
  [
    "def rotate_half(\n    x: jax.Array,\n) -> jax.Array:\n    \n    x1, x2 = jnp.split(x, 2, axis=-1)\n    return jnp.concatenate((-x2, x1), axis=-1)",
    "Obtain the rotated counterpart of each feature"
  ],
  [
    "class RotaryEmbedding(hk.Module):\n    \"\"\"Applies rotary embeddings (RoPE) to the input sequence tensor,\n    as described in https://arxiv.org/abs/2104.09864.\n\n    Attributes:\n        dim (int): Dimensionality of the feature vectors\n        base_exponent (int): Base exponent to compute embeddings from\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        name: Optional[str] = None,\n        base_exponent: int = 10000,\n    ):\n        super().__init__(name)\n        self.dim = dim\n        self.base_exponent = base_exponent\n        assert self.dim % 2 == 0\n\n    def __call__(\n        self,\n        x: jax.Array,\n        seq_dim: int,\n        offset: jax.Array,\n        const_position: Optional[int] = None,\n        t: Optional[jax.Array] = None,\n    ) -> jax.Array:\n        fprop_dtype = x.dtype\n        # Compute the per-dimension frequencies\n        exponents = jnp.arange(0, self.dim, 2, dtype=jnp.float32)\n        inv_freq = jnp.asarray(\n            1.0 / (self.base_exponent ** (exponents / self.dim)), dtype=jnp.float32\n        )\n\n        if jnp.shape(offset) == ():\n            # Offset can be a scalar or one offset per batch element.\n            offset = jnp.expand_dims(offset, 0)\n\n        # Compute the per element phase (to pass into sin and cos)\n        if const_position:\n            t = const_position * jnp.ones(\n                (\n                    1,\n                    x.shape[seq_dim],\n                ),\n                dtype=jnp.float32,\n            )\n        elif t is None:\n            t = jnp.arange(x.shape[seq_dim], dtype=jnp.float32) + jnp.expand_dims(offset, -1)\n        phase = jnp.einsum(\"bi,j->bij\", t, inv_freq)\n        phase = jnp.tile(phase, reps=(1, 2))[:, :, None, :]\n\n        x = x * jnp.cos(phase) + rotate_half(x) * jnp.sin(phase)\n        x = x.astype(fprop_dtype)\n\n        return x",
    "Applies rotary embeddings (RoPE) to the input sequence tensor,\nas described in https://arxiv.org/abs/2104.09864.\n\nAttributes:\n    dim (int): Dimensionality of the feature vectors\n    base_exponent (int): Base exponent to compute embeddings from"
  ],
  [
    "class InOutEmbed(hk.Embed):\n    \n\n    def __init__(\n        self,\n        vocab_size: Optional[int] = None,\n        embed_dim: Optional[int] = None,\n        sharding: Optional[P] = None,\n        name: Optional[str] = None,\n    ):\n        super().__init__(\n            vocab_size=vocab_size,\n            embed_dim=embed_dim,\n            name=name,\n        )\n        self.sharding = sharding\n\n    @property\n    def embeddings(self):\n        embed_mat = hk.get_parameter(\n            \"embeddings\",\n            [self.vocab_size, self.embed_dim],\n            dtype=jnp.float32,\n            init=hk.initializers.Constant(0),\n        )\n        if self.sharding:\n            embed_mat = with_sharding_constraint(embed_mat, self.sharding)\n        return embed_mat\n\n    def decode(\n        self,\n        inputs: jax.Array,\n    ) -> jax.Array:\n        return jnp.dot(inputs, self.embeddings.T.astype(inputs.dtype))",
    "Module for embedding tokens in a low-dimensional space."
  ],
  [
    "def __call__(\n        self,\n        inputs: jax.Array,\n    ) -> jax.Array:\n        \n\n        fprop_dtype = inputs.dtype\n        if not inputs.shape:\n            raise ValueError(\"Input must not be scalar.\")\n\n        input_size = self.input_size = inputs.shape[-1]\n        output_size = self.output_size\n\n        w = hk.get_parameter(\n            \"w\", [input_size, output_size], jnp.float32, init=hk.initializers.Constant(0)\n        )\n\n        if hasattr(w, \"scales\"):\n            shape = inputs.shape\n            inputs = jnp.reshape(inputs, (-1, shape[-1]))\n\n            @functools.partial(\n                shard_map,\n                mesh=self.mesh,\n                in_specs=(self.sharding, self.sharding),\n                out_specs=self.sharding,\n                check_rep=False,\n            )\n            def mul(w, s):\n                return w.astype(s.dtype) * s\n\n            w = mul(w.weight, w.scales)\n        out = jnp.dot(inputs, w.astype(fprop_dtype))\n        if self.with_bias:\n            b = hk.get_parameter(\n                \"b\", [self.output_size], jnp.float32, init=hk.initializers.Constant(0)\n            )\n            b = jnp.broadcast_to(b, out.shape)\n            out = out + b.astype(fprop_dtype)\n\n        return out",
    "Computes a linear transform of the input."
  ],
  [
    "def __call__(\n        self,\n        inputs: jax.Array,  # [B, T, D]\n        mask: jax.Array,  # [B, 1, T, T] or [B, 1, 1, T]\n        padding_mask: Optional[jax.Array],\n        layer_memory: Optional[KVMemory],\n    ) -> DecoderOutput:\n        \n\n        def layer_norm(x):\n            return hk_rms_norm(x)\n\n        if self.shard_activations:\n            sharding = P(self.data_axis, None, self.model_axis)\n        else:\n            sharding = P(self.data_axis, None)\n        h = with_sharding_constraint(inputs, sharding)\n\n        attn_output = MHABlock(\n            num_q_heads=self.num_q_heads,\n            num_kv_heads=self.num_kv_heads,\n            key_size=self.key_size,\n            attn_output_multiplier=self.attn_output_multiplier,\n            mesh=self.mesh,\n            data_axis=self.data_axis,\n            model_axis=self.model_axis,\n        )(layer_norm(h), mask, layer_memory)\n        h_attn = attn_output.embeddings\n\n        h_attn = layer_norm(h_attn)\n        h += h_attn\n        h = with_sharding_constraint(h, sharding)\n\n        def base_dense_block(h):\n            h = DenseBlock(\n                num_q_heads=self.num_q_heads,\n                num_kv_heads=self.num_kv_heads,\n                key_size=self.key_size,\n                widening_factor=self.widening_factor,\n                sharding_constraint=False,\n                mesh=self.mesh,\n            )(h)\n            return h\n\n        if self.num_experts > 1:\n            rank_logger.debug(\"Using MoE!\")\n            router = Router(\n                num_selected_experts=self.num_selected_experts,\n                shard_activations=self.shard_activations,\n                data_axis=self.data_axis,\n                model_axis=self.model_axis,\n                mesh=self.mesh,\n            )\n            h_dense = MoELayer(\n                num_experts=self.num_experts,\n                mesh=self.mesh,\n                layer_fn=base_dense_block,\n                router=router,\n                shard_activations=self.shard_activations,\n                data_axis=self.data_axis,\n                model_axis=self.model_axis,\n            )(layer_norm(h), padding_mask)\n        else:\n            h_dense = base_dense_block(layer_norm(h))\n\n        h_dense = layer_norm(h_dense)\n        h += h_dense\n        h = with_sharding_constraint(h, sharding)\n\n        return DecoderOutput(\n            embeddings=h,\n            memory=attn_output.memory,\n        )",
    "Transforms input embedding sequences to output embedding sequences."
  ],
  [
    "def __call__(\n        self,\n        tokens: jax.Array,\n        memory: Optional[Memory] = None,\n        *,\n        batch: Dict[str, jax.Array] = {},\n        last_hid_only: bool = False,\n        length: Optional[jax.Array] = None,\n    ) -> LanguageModelOutput:\n        \n        del batch  # Unused.\n\n        config = self.config\n\n        input_mask = jnp.greater(tokens, config.pad_token)\n\n        # Embed the input tokens and positions.\n        in_out_embed = InOutEmbed(\n            self.config.vocab_size,\n            embed_dim=self.config.model_size,\n            sharding=P(None, (\"data\", \"model\")),\n        )\n        input_embeddings = in_out_embed(tokens).astype(config.fprop_dtype)\n        input_embeddings = with_sharding_constraint(\n            input_embeddings, P(\"data\", None, self.model.model_axis)\n        )\n        input_embeddings *= config.embedding_multiplier_scale\n\n        model_output = self.model(\n            input_embeddings,\n            input_mask,\n            memory=memory,\n        )  # [B, T, D]\n        embeddings, model_state = model_output.embeddings, model_output.memory\n        if self.model.shard_activations:\n            embeddings = with_sharding_constraint(\n                embeddings, P(\"data\", None, self.model.model_axis)\n            )\n        else:\n            embeddings = with_sharding_constraint(embeddings, P(\"data\", None))\n        rank_logger.debug(f\"Final embedding shape: {embeddings.shape}\")\n        embeddings = layer_norm(embeddings, self.model)\n        assert embeddings.dtype == self.fprop_dtype\n\n        if last_hid_only:\n            last_step = jnp.maximum(jnp.sum(input_mask.astype(jnp.int32), axis=1) - 1, 0)\n            last_hid = jax.vmap(lambda x, i: x[i], in_axes=0, out_axes=0)(embeddings, last_step)\n            return last_hid\n\n        if length is not None:\n            last_step = jnp.maximum(length.astype(jnp.int32) - 1, 0)\n            embeddings = jax.vmap(lambda x, i: x[i], in_axes=0, out_axes=0)(embeddings, last_step)\n            embeddings = jnp.expand_dims(embeddings, axis=1)\n\n        # Decode the embeddings (here, we use tied weights).\n        rank_logger.info(embeddings.shape)\n        out = in_out_embed.decode(embeddings)\n        rank_logger.info(out.shape)\n        out *= config.output_multiplier_scale\n\n        if self.model.shard_activations:\n            out = with_sharding_constraint(out, P(\"data\", None, self.model.model_axis))\n        else:\n            out = with_sharding_constraint(out, P(\"data\", None))\n\n        return LanguageModelOutput(\n            logits=out,\n            model_state=model_state,\n        )",
    "Forward pass, producing a sequence of logits."
  ],
  [
    "def __call__(\n        self,\n        embeddings: jax.Array,  # [B, T, D]\n        mask: jax.Array,  # [B, T]\n        memory: Optional[Memory],\n    ) -> TransformerOutput:\n        \n\n        fprop_dtype = embeddings.dtype\n        _, seq_len, model_size = embeddings.shape\n        padding_mask = mask.copy()\n        mask = mask[:, None, None, :]  # [B, H=1, T'=1, T]\n\n        # Compute causal mask for autoregressive sequence modelling.\n        causal_mask = jnp.tril(jnp.ones((1, 1, seq_len, seq_len))).astype(\n            fprop_dtype\n        )  # [B=1, H=1, T, T]\n        mask = mask * causal_mask  # [B, H=1, T, T]\n\n        h = embeddings\n        kv_memories = []\n\n        def block(\n            h,\n            mask,\n            padding_mask,\n            memory,\n            layer_index: Optional[int] = None,\n            widening_factor: Optional[int] = None,\n            name: Optional[str] = None,\n        ) -> DecoderOutput:\n            return DecoderLayer(\n                num_q_heads=self.num_q_heads,\n                num_kv_heads=self.num_kv_heads,\n                key_size=self.key_size,\n                widening_factor=widening_factor or self.widening_factor,\n                num_layers=self.num_layers,\n                mesh=self.mesh,\n                data_axis=self.data_axis,\n                model_axis=self.model_axis,\n                attn_output_multiplier=self.attn_output_multiplier,\n                shard_activations=self.shard_activations,\n                # MoE.\n                num_experts=self.num_experts,\n                num_selected_experts=self.num_selected_experts,\n                name=name,\n                layer_index=layer_index,\n            )(\n                h,\n                mask,\n                padding_mask,\n                memory,\n            )\n\n        for i in range(self.num_layers):\n            decoder_output = block(\n                h,\n                mask,\n                padding_mask,\n                memory.layers[i] if memory else None,\n                layer_index=i,\n                name=f\"decoder_layer_{i}\",\n            )\n            h, new_kv_memory = (\n                decoder_output.embeddings,\n                decoder_output.memory,\n            )\n            kv_memories.append(new_kv_memory)\n\n        return TransformerOutput(\n            embeddings=h,\n            memory=Memory(layers=kv_memories),\n        )",
    "Transforms input embedding sequences to output embedding sequences."
  ],
  [
    "class BaseAgent(BaseModel, ABC):\n    \"\"\"Abstract base class for managing agent state and execution.\n\n    Provides foundational functionality for state transitions, memory management,\n    and a step-based execution loop. Subclasses must implement the `step` method.\n    \"\"\"\n\n    # Core attributes\n    name: str = Field(..., description=\"Unique name of the agent\")\n    description: Optional[str] = Field(None, description=\"Optional agent description\")\n\n    # Prompts\n    system_prompt: Optional[str] = Field(\n        None, description=\"System-level instruction prompt\"\n    )\n    next_step_prompt: Optional[str] = Field(\n        None, description=\"Prompt for determining next action\"\n    )\n\n    # Dependencies\n    llm: LLM = Field(default_factory=LLM, description=\"Language model instance\")\n    memory: Memory = Field(default_factory=Memory, description=\"Agent's memory store\")\n    state: AgentState = Field(\n        default=AgentState.IDLE, description=\"Current agent state\"\n    )\n\n    # Execution control\n    max_steps: int = Field(default=10, description=\"Maximum steps before termination\")\n    current_step: int = Field(default=0, description=\"Current step in execution\")\n\n    duplicate_threshold: int = 2\n\n    class Config:\n        arbitrary_types_allowed = True\n        extra = \"allow\"  # Allow extra fields for flexibility in subclasses\n\n    @model_validator(mode=\"after\")\n    def initialize_agent(self) -> \"BaseAgent\":\n        \"\"\"Initialize agent with default settings if not provided.\"\"\"\n        if self.llm is None or not isinstance(self.llm, LLM):\n            self.llm = LLM(config_name=self.name.lower())\n        if not isinstance(self.memory, Memory):\n            self.memory = Memory()\n        return self\n\n    @asynccontextmanager\n    async def state_context(self, new_state: AgentState):\n        \"\"\"Context manager for safe agent state transitions.\n\n        Args:\n            new_state: The state to transition to during the context.\n\n        Yields:\n            None: Allows execution within the new state.\n\n        Raises:\n            ValueError: If the new_state is invalid.\n        \"\"\"\n        if not isinstance(new_state, AgentState):\n            raise ValueError(f\"Invalid state: {new_state}\")\n\n        previous_state = self.state\n        self.state = new_state\n        try:\n            yield\n        except Exception as e:\n            self.state = AgentState.ERROR  # Transition to ERROR on failure\n            raise e\n        finally:\n            self.state = previous_state  # Revert to previous state\n\n    def update_memory(\n        self,\n        role: ROLE_TYPE,  # type: ignore\n        content: str,\n        base64_image: Optional[str] = None,\n        **kwargs,\n    ) -> None:\n        \"\"\"Add a message to the agent's memory.\n\n        Args:\n            role: The role of the message sender (user, system, assistant, tool).\n            content: The message content.\n            base64_image: Optional base64 encoded image.\n            **kwargs: Additional arguments (e.g., tool_call_id for tool messages).\n\n        Raises:\n            ValueError: If the role is unsupported.\n        \"\"\"\n        message_map = {\n            \"user\": Message.user_message,\n            \"system\": Message.system_message,\n            \"assistant\": Message.assistant_message,\n            \"tool\": lambda content, **kw: Message.tool_message(content, **kw),\n        }\n\n        if role not in message_map:\n            raise ValueError(f\"Unsupported message role: {role}\")\n\n        # Create message with appropriate parameters based on role\n        kwargs = {\"base64_image\": base64_image, **(kwargs if role == \"tool\" else {})}\n        self.memory.add_message(message_map[role](content, **kwargs))\n\n    async def run(self, request: Optional[str] = None) -> str:\n        \"\"\"Execute the agent's main loop asynchronously.\n\n        Args:\n            request: Optional initial user request to process.\n\n        Returns:\n            A string summarizing the execution results.\n\n        Raises:\n            RuntimeError: If the agent is not in IDLE state at start.\n        \"\"\"\n        if self.state != AgentState.IDLE:\n            raise RuntimeError(f\"Cannot run agent from state: {self.state}\")\n\n        if request:\n            self.update_memory(\"user\", request)\n\n        results: List[str] = []\n        async with self.state_context(AgentState.RUNNING):\n            while (\n                self.current_step < self.max_steps and self.state != AgentState.FINISHED\n            ):\n                self.current_step += 1\n                logger.info(f\"Executing step {self.current_step}/{self.max_steps}\")\n                step_result = await self.step()\n\n                # Check for stuck state\n                if self.is_stuck():\n                    self.handle_stuck_state()\n\n                results.append(f\"Step {self.current_step}: {step_result}\")\n\n            if self.current_step >= self.max_steps:\n                self.current_step = 0\n                self.state = AgentState.IDLE\n                results.append(f\"Terminated: Reached max steps ({self.max_steps})\")\n        await SANDBOX_CLIENT.cleanup()\n        return \"\\n\".join(results) if results else \"No steps executed\"\n\n    @abstractmethod\n    async def step(self) -> str:\n        \"\"\"Execute a single step in the agent's workflow.\n\n        Must be implemented by subclasses to define specific behavior.\n        \"\"\"\n\n    def handle_stuck_state(self):\n        \"\"\"Handle stuck state by adding a prompt to change strategy\"\"\"\n        stuck_prompt = \"\\\n        Observed duplicate responses. Consider new strategies and avoid repeating ineffective paths already attempted.\"\n        self.next_step_prompt = f\"{stuck_prompt}\\n{self.next_step_prompt}\"\n        logger.warning(f\"Agent detected stuck state. Added prompt: {stuck_prompt}\")\n\n    def is_stuck(self) -> bool:\n        \"\"\"Check if the agent is stuck in a loop by detecting duplicate content\"\"\"\n        if len(self.memory.messages) < 2:\n            return False\n\n        last_message = self.memory.messages[-1]\n        if not last_message.content:\n            return False\n\n        # Count identical content occurrences\n        duplicate_count = sum(\n            1\n            for msg in reversed(self.memory.messages[:-1])\n            if msg.role == \"assistant\" and msg.content == last_message.content\n        )\n\n        return duplicate_count >= self.duplicate_threshold\n\n    @property\n    def messages(self) -> List[Message]:\n        \"\"\"Retrieve a list of messages from the agent's memory.\"\"\"\n        return self.memory.messages\n\n    @messages.setter\n    def messages(self, value: List[Message]):\n        \"\"\"Set the list of messages in the agent's memory.\"\"\"\n        self.memory.messages = value",
    "Abstract base class for managing agent state and execution.\n\nProvides foundational functionality for state transitions, memory management,\nand a step-based execution loop. Subclasses must implement the `step` method."
  ],
  [
    "def initialize_agent(self) -> \"BaseAgent\":\n        \n        if self.llm is None or not isinstance(self.llm, LLM):\n            self.llm = LLM(config_name=self.name.lower())\n        if not isinstance(self.memory, Memory):\n            self.memory = Memory()\n        return self",
    "Initialize agent with default settings if not provided."
  ],
  [
    "async def state_context(self, new_state: AgentState):\n        \"\"\"Context manager for safe agent state transitions.\n\n        Args:\n            new_state: The state to transition to during the context.\n\n        Yields:\n            None: Allows execution within the new state.\n\n        Raises:\n            ValueError: If the new_state is invalid.\n        \"\"\"\n        if not isinstance(new_state, AgentState):\n            raise ValueError(f\"Invalid state: {new_state}\")\n\n        previous_state = self.state\n        self.state = new_state\n        try:\n            yield\n        except Exception as e:\n            self.state = AgentState.ERROR  # Transition to ERROR on failure\n            raise e\n        finally:\n            self.state = previous_state  # Revert to previous state",
    "Context manager for safe agent state transitions.\n\nArgs:\n    new_state: The state to transition to during the context.\n\nYields:\n    None: Allows execution within the new state.\n\nRaises:\n    ValueError: If the new_state is invalid."
  ],
  [
    "def update_memory(\n        self,\n        role: ROLE_TYPE,  # type: ignore\n        content: str,\n        base64_image: Optional[str] = None,\n        **kwargs,\n    ) -> None:\n        \"\"\"Add a message to the agent's memory.\n\n        Args:\n            role: The role of the message sender (user, system, assistant, tool).\n            content: The message content.\n            base64_image: Optional base64 encoded image.\n            **kwargs: Additional arguments (e.g., tool_call_id for tool messages).\n\n        Raises:\n            ValueError: If the role is unsupported.\n        \"\"\"\n        message_map = {\n            \"user\": Message.user_message,\n            \"system\": Message.system_message,\n            \"assistant\": Message.assistant_message,\n            \"tool\": lambda content, **kw: Message.tool_message(content, **kw),\n        }\n\n        if role not in message_map:\n            raise ValueError(f\"Unsupported message role: {role}\")\n\n        # Create message with appropriate parameters based on role\n        kwargs = {\"base64_image\": base64_image, **(kwargs if role == \"tool\" else {})}\n        self.memory.add_message(message_map[role](content, **kwargs))",
    "Add a message to the agent's memory.\n\nArgs:\n    role: The role of the message sender (user, system, assistant, tool).\n    content: The message content.\n    base64_image: Optional base64 encoded image.\n    **kwargs: Additional arguments (e.g., tool_call_id for tool messages).\n\nRaises:\n    ValueError: If the role is unsupported."
  ],
  [
    "async def run(self, request: Optional[str] = None) -> str:\n        \"\"\"Execute the agent's main loop asynchronously.\n\n        Args:\n            request: Optional initial user request to process.\n\n        Returns:\n            A string summarizing the execution results.\n\n        Raises:\n            RuntimeError: If the agent is not in IDLE state at start.\n        \"\"\"\n        if self.state != AgentState.IDLE:\n            raise RuntimeError(f\"Cannot run agent from state: {self.state}\")\n\n        if request:\n            self.update_memory(\"user\", request)\n\n        results: List[str] = []\n        async with self.state_context(AgentState.RUNNING):\n            while (\n                self.current_step < self.max_steps and self.state != AgentState.FINISHED\n            ):\n                self.current_step += 1\n                logger.info(f\"Executing step {self.current_step}/{self.max_steps}\")\n                step_result = await self.step()\n\n                # Check for stuck state\n                if self.is_stuck():\n                    self.handle_stuck_state()\n\n                results.append(f\"Step {self.current_step}: {step_result}\")\n\n            if self.current_step >= self.max_steps:\n                self.current_step = 0\n                self.state = AgentState.IDLE\n                results.append(f\"Terminated: Reached max steps ({self.max_steps})\")\n        await SANDBOX_CLIENT.cleanup()\n        return \"\\n\".join(results) if results else \"No steps executed\"",
    "Execute the agent's main loop asynchronously.\n\nArgs:\n    request: Optional initial user request to process.\n\nReturns:\n    A string summarizing the execution results.\n\nRaises:\n    RuntimeError: If the agent is not in IDLE state at start."
  ],
  [
    "async def step(self) -> str:\n        \"\"\"Execute a single step in the agent's workflow.\n\n        Must be implemented by subclasses to define specific behavior.\n        \"\"\"",
    "Execute a single step in the agent's workflow.\n\nMust be implemented by subclasses to define specific behavior."
  ],
  [
    "def handle_stuck_state(self):\n        \n        stuck_prompt = \"\\\n        Observed duplicate responses. Consider new strategies and avoid repeating ineffective paths already attempted.\"\n        self.next_step_prompt = f\"{stuck_prompt}\\n{self.next_step_prompt}\"\n        logger.warning(f\"Agent detected stuck state. Added prompt: {stuck_prompt}\")",
    "Handle stuck state by adding a prompt to change strategy"
  ],
  [
    "def is_stuck(self) -> bool:\n        \n        if len(self.memory.messages) < 2:\n            return False\n\n        last_message = self.memory.messages[-1]\n        if not last_message.content:\n            return False\n\n        # Count identical content occurrences\n        duplicate_count = sum(\n            1\n            for msg in reversed(self.memory.messages[:-1])\n            if msg.role == \"assistant\" and msg.content == last_message.content\n        )\n\n        return duplicate_count >= self.duplicate_threshold",
    "Check if the agent is stuck in a loop by detecting duplicate content"
  ],
  [
    "def messages(self) -> List[Message]:\n        \n        return self.memory.messages",
    "Retrieve a list of messages from the agent's memory."
  ],
  [
    "def messages(self, value: List[Message]):\n        \n        self.memory.messages = value",
    "Set the list of messages in the agent's memory."
  ],
  [
    "class BrowserAgent(ToolCallAgent):\n    \"\"\"\n    A browser agent that uses the browser_use library to control a browser.\n\n    This agent can navigate web pages, interact with elements, fill forms,\n    extract content, and perform other browser-based actions to accomplish tasks.\n    \"\"\"\n\n    name: str = \"browser\"\n    description: str = \"A browser agent that can control a browser to accomplish tasks\"\n\n    system_prompt: str = SYSTEM_PROMPT\n    next_step_prompt: str = NEXT_STEP_PROMPT\n\n    max_observe: int = 10000\n    max_steps: int = 20\n\n    # Configure the available tools\n    available_tools: ToolCollection = Field(\n        default_factory=lambda: ToolCollection(BrowserUseTool(), Terminate())\n    )\n\n    # Use Auto for tool choice to allow both tool usage and free-form responses\n    tool_choices: ToolChoice = ToolChoice.AUTO\n    special_tool_names: list[str] = Field(default_factory=lambda: [Terminate().name])\n\n    browser_context_helper: Optional[BrowserContextHelper] = None\n\n    @model_validator(mode=\"after\")\n    def initialize_helper(self) -> \"BrowserAgent\":\n        self.browser_context_helper = BrowserContextHelper(self)\n        return self\n\n    async def think(self) -> bool:\n        \"\"\"Process current state and decide next actions using tools, with browser state info added\"\"\"\n        self.next_step_prompt = (\n            await self.browser_context_helper.format_next_step_prompt()\n        )\n        return await super().think()\n\n    async def cleanup(self):\n        \"\"\"Clean up browser agent resources by calling parent cleanup.\"\"\"\n        await self.browser_context_helper.cleanup_browser()",
    "A browser agent that uses the browser_use library to control a browser.\n\nThis agent can navigate web pages, interact with elements, fill forms,\nextract content, and perform other browser-based actions to accomplish tasks."
  ],
  [
    "async def format_next_step_prompt(self) -> str:\n        \n        browser_state = await self.get_browser_state()\n        url_info, tabs_info, content_above_info, content_below_info = \"\", \"\", \"\", \"\"\n        results_info = \"\"  # Or get from agent if needed elsewhere\n\n        if browser_state and not browser_state.get(\"error\"):\n            url_info = f\"\\n   URL: {browser_state.get('url', 'N/A')}\\n   Title: {browser_state.get('title', 'N/A')}\"\n            tabs = browser_state.get(\"tabs\", [])\n            if tabs:\n                tabs_info = f\"\\n   {len(tabs)} tab(s) available\"\n            pixels_above = browser_state.get(\"pixels_above\", 0)\n            pixels_below = browser_state.get(\"pixels_below\", 0)\n            if pixels_above > 0:\n                content_above_info = f\" ({pixels_above} pixels)\"\n            if pixels_below > 0:\n                content_below_info = f\" ({pixels_below} pixels)\"\n\n            if self._current_base64_image:\n                image_message = Message.user_message(\n                    content=\"Current browser screenshot:\",\n                    base64_image=self._current_base64_image,\n                )\n                self.agent.memory.add_message(image_message)\n                self._current_base64_image = None  # Consume the image after adding\n\n        return NEXT_STEP_PROMPT.format(\n            url_placeholder=url_info,\n            tabs_placeholder=tabs_info,\n            content_above_placeholder=content_above_info,\n            content_below_placeholder=content_below_info,\n            results_placeholder=results_info,\n        )",
    "Gets browser state and formats the browser prompt."
  ],
  [
    "async def think(self) -> bool:\n        \n        self.next_step_prompt = (\n            await self.browser_context_helper.format_next_step_prompt()\n        )\n        return await super().think()",
    "Process current state and decide next actions using tools, with browser state info added"
  ],
  [
    "async def cleanup(self):\n        \n        await self.browser_context_helper.cleanup_browser()",
    "Clean up browser agent resources by calling parent cleanup."
  ],
  [
    "class DataAnalysis(ToolCallAgent):\n    \"\"\"\n    A data analysis agent that uses planning to solve various data analysis tasks.\n\n    This agent extends ToolCallAgent with a comprehensive set of tools and capabilities,\n    including Data Analysis, Chart Visualization, Data Report.\n    \"\"\"\n\n    name: str = \"Data_Analysis\"\n    description: str = \"An analytical agent that utilizes python and data visualization tools to solve diverse data analysis tasks\"\n\n    system_prompt: str = SYSTEM_PROMPT.format(directory=config.workspace_root)\n    next_step_prompt: str = NEXT_STEP_PROMPT\n\n    max_observe: int = 15000\n    max_steps: int = 20\n\n    # Add general-purpose tools to the tool collection\n    available_tools: ToolCollection = Field(\n        default_factory=lambda: ToolCollection(\n            NormalPythonExecute(),\n            VisualizationPrepare(),\n            DataVisualization(),\n            Terminate(),\n        )\n    )",
    "A data analysis agent that uses planning to solve various data analysis tasks.\n\nThis agent extends ToolCallAgent with a comprehensive set of tools and capabilities,\nincluding Data Analysis, Chart Visualization, Data Report."
  ],
  [
    "class Manus(ToolCallAgent):\n    \n\n    name: str = \"Manus\"\n    description: str = \"A versatile agent that can solve various tasks using multiple tools including MCP-based tools\"\n\n    system_prompt: str = SYSTEM_PROMPT.format(directory=config.workspace_root)\n    next_step_prompt: str = NEXT_STEP_PROMPT\n\n    max_observe: int = 10000\n    max_steps: int = 20\n\n    # MCP clients for remote tool access\n    mcp_clients: MCPClients = Field(default_factory=MCPClients)\n\n    # Add general-purpose tools to the tool collection\n    available_tools: ToolCollection = Field(\n        default_factory=lambda: ToolCollection(\n            PythonExecute(),\n            BrowserUseTool(),\n            StrReplaceEditor(),\n            AskHuman(),\n            Terminate(),\n        )\n    )\n\n    special_tool_names: list[str] = Field(default_factory=lambda: [Terminate().name])\n    browser_context_helper: Optional[BrowserContextHelper] = None\n\n    # Track connected MCP servers\n    connected_servers: Dict[str, str] = Field(\n        default_factory=dict\n    )  # server_id -> url/command\n    _initialized: bool = False\n\n    @model_validator(mode=\"after\")\n    def initialize_helper(self) -> \"Manus\":\n        \"\"\"Initialize basic components synchronously.\"\"\"\n        self.browser_context_helper = BrowserContextHelper(self)\n        return self\n\n    @classmethod\n    async def create(cls, **kwargs) -> \"Manus\":\n        \"\"\"Factory method to create and properly initialize a Manus instance.\"\"\"\n        instance = cls(**kwargs)\n        await instance.initialize_mcp_servers()\n        instance._initialized = True\n        return instance\n\n    async def initialize_mcp_servers(self) -> None:\n        \"\"\"Initialize connections to configured MCP servers.\"\"\"\n        for server_id, server_config in config.mcp_config.servers.items():\n            try:\n                if server_config.type == \"sse\":\n                    if server_config.url:\n                        await self.connect_mcp_server(server_config.url, server_id)\n                        logger.info(\n                            f\"Connected to MCP server {server_id} at {server_config.url}\"\n                        )\n                elif server_config.type == \"stdio\":\n                    if server_config.command:\n                        await self.connect_mcp_server(\n                            server_config.command,\n                            server_id,\n                            use_stdio=True,\n                            stdio_args=server_config.args,\n                        )\n                        logger.info(\n                            f\"Connected to MCP server {server_id} using command {server_config.command}\"\n                        )\n            except Exception as e:\n                logger.error(f\"Failed to connect to MCP server {server_id}: {e}\")\n\n    async def connect_mcp_server(\n        self,\n        server_url: str,\n        server_id: str = \"\",\n        use_stdio: bool = False,\n        stdio_args: List[str] = None,\n    ) -> None:\n        \"\"\"Connect to an MCP server and add its tools.\"\"\"\n        if use_stdio:\n            await self.mcp_clients.connect_stdio(\n                server_url, stdio_args or [], server_id\n            )\n            self.connected_servers[server_id or server_url] = server_url\n        else:\n            await self.mcp_clients.connect_sse(server_url, server_id)\n            self.connected_servers[server_id or server_url] = server_url\n\n        # Update available tools with only the new tools from this server\n        new_tools = [\n            tool for tool in self.mcp_clients.tools if tool.server_id == server_id\n        ]\n        self.available_tools.add_tools(*new_tools)\n\n    async def disconnect_mcp_server(self, server_id: str = \"\") -> None:\n        \"\"\"Disconnect from an MCP server and remove its tools.\"\"\"\n        await self.mcp_clients.disconnect(server_id)\n        if server_id:\n            self.connected_servers.pop(server_id, None)\n        else:\n            self.connected_servers.clear()\n\n        # Rebuild available tools without the disconnected server's tools\n        base_tools = [\n            tool\n            for tool in self.available_tools.tools\n            if not isinstance(tool, MCPClientTool)\n        ]\n        self.available_tools = ToolCollection(*base_tools)\n        self.available_tools.add_tools(*self.mcp_clients.tools)\n\n    async def cleanup(self):\n        \"\"\"Clean up Manus agent resources.\"\"\"\n        if self.browser_context_helper:\n            await self.browser_context_helper.cleanup_browser()\n        # Disconnect from all MCP servers only if we were initialized\n        if self._initialized:\n            await self.disconnect_mcp_server()\n            self._initialized = False\n\n    async def think(self) -> bool:\n        \"\"\"Process current state and decide next actions with appropriate context.\"\"\"\n        if not self._initialized:\n            await self.initialize_mcp_servers()\n            self._initialized = True\n\n        original_prompt = self.next_step_prompt\n        recent_messages = self.memory.messages[-3:] if self.memory.messages else []\n        browser_in_use = any(\n            tc.function.name == BrowserUseTool().name\n            for msg in recent_messages\n            if msg.tool_calls\n            for tc in msg.tool_calls\n        )\n\n        if browser_in_use:\n            self.next_step_prompt = (\n                await self.browser_context_helper.format_next_step_prompt()\n            )\n\n        result = await super().think()\n\n        # Restore original prompt\n        self.next_step_prompt = original_prompt\n\n        return result",
    "A versatile general-purpose agent with support for both local and MCP tools."
  ],
  [
    "async def create(cls, **kwargs) -> \"Manus\":\n        \n        instance = cls(**kwargs)\n        await instance.initialize_mcp_servers()\n        instance._initialized = True\n        return instance",
    "Factory method to create and properly initialize a Manus instance."
  ],
  [
    "async def initialize_mcp_servers(self) -> None:\n        \n        for server_id, server_config in config.mcp_config.servers.items():\n            try:\n                if server_config.type == \"sse\":\n                    if server_config.url:\n                        await self.connect_mcp_server(server_config.url, server_id)\n                        logger.info(\n                            f\"Connected to MCP server {server_id} at {server_config.url}\"\n                        )\n                elif server_config.type == \"stdio\":\n                    if server_config.command:\n                        await self.connect_mcp_server(\n                            server_config.command,\n                            server_id,\n                            use_stdio=True,\n                            stdio_args=server_config.args,\n                        )\n                        logger.info(\n                            f\"Connected to MCP server {server_id} using command {server_config.command}\"\n                        )\n            except Exception as e:\n                logger.error(f\"Failed to connect to MCP server {server_id}: {e}\")",
    "Initialize connections to configured MCP servers."
  ],
  [
    "async def connect_mcp_server(\n        self,\n        server_url: str,\n        server_id: str = \"\",\n        use_stdio: bool = False,\n        stdio_args: List[str] = None,\n    ) -> None:\n        \n        if use_stdio:\n            await self.mcp_clients.connect_stdio(\n                server_url, stdio_args or [], server_id\n            )\n            self.connected_servers[server_id or server_url] = server_url\n        else:\n            await self.mcp_clients.connect_sse(server_url, server_id)\n            self.connected_servers[server_id or server_url] = server_url\n\n        # Update available tools with only the new tools from this server\n        new_tools = [\n            tool for tool in self.mcp_clients.tools if tool.server_id == server_id\n        ]\n        self.available_tools.add_tools(*new_tools)",
    "Connect to an MCP server and add its tools."
  ],
  [
    "async def disconnect_mcp_server(self, server_id: str = \"\") -> None:\n        \n        await self.mcp_clients.disconnect(server_id)\n        if server_id:\n            self.connected_servers.pop(server_id, None)\n        else:\n            self.connected_servers.clear()\n\n        # Rebuild available tools without the disconnected server's tools\n        base_tools = [\n            tool\n            for tool in self.available_tools.tools\n            if not isinstance(tool, MCPClientTool)\n        ]\n        self.available_tools = ToolCollection(*base_tools)\n        self.available_tools.add_tools(*self.mcp_clients.tools)",
    "Disconnect from an MCP server and remove its tools."
  ],
  [
    "async def think(self) -> bool:\n        \n        if not self._initialized:\n            await self.initialize_mcp_servers()\n            self._initialized = True\n\n        original_prompt = self.next_step_prompt\n        recent_messages = self.memory.messages[-3:] if self.memory.messages else []\n        browser_in_use = any(\n            tc.function.name == BrowserUseTool().name\n            for msg in recent_messages\n            if msg.tool_calls\n            for tc in msg.tool_calls\n        )\n\n        if browser_in_use:\n            self.next_step_prompt = (\n                await self.browser_context_helper.format_next_step_prompt()\n            )\n\n        result = await super().think()\n\n        # Restore original prompt\n        self.next_step_prompt = original_prompt\n\n        return result",
    "Process current state and decide next actions with appropriate context."
  ],
  [
    "class MCPAgent(ToolCallAgent):\n    \"\"\"Agent for interacting with MCP (Model Context Protocol) servers.\n\n    This agent connects to an MCP server using either SSE or stdio transport\n    and makes the server's tools available through the agent's tool interface.\n    \"\"\"\n\n    name: str = \"mcp_agent\"\n    description: str = \"An agent that connects to an MCP server and uses its tools.\"\n\n    system_prompt: str = SYSTEM_PROMPT\n    next_step_prompt: str = NEXT_STEP_PROMPT\n\n    # Initialize MCP tool collection\n    mcp_clients: MCPClients = Field(default_factory=MCPClients)\n    available_tools: MCPClients = None  # Will be set in initialize()\n\n    max_steps: int = 20\n    connection_type: str = \"stdio\"  # \"stdio\" or \"sse\"\n\n    # Track tool schemas to detect changes\n    tool_schemas: Dict[str, Dict[str, Any]] = Field(default_factory=dict)\n    _refresh_tools_interval: int = 5  # Refresh tools every N steps\n\n    # Special tool names that should trigger termination\n    special_tool_names: List[str] = Field(default_factory=lambda: [\"terminate\"])\n\n    async def initialize(\n        self,\n        connection_type: Optional[str] = None,\n        server_url: Optional[str] = None,\n        command: Optional[str] = None,\n        args: Optional[List[str]] = None,\n    ) -> None:\n        \"\"\"Initialize the MCP connection.\n\n        Args:\n            connection_type: Type of connection to use (\"stdio\" or \"sse\")\n            server_url: URL of the MCP server (for SSE connection)\n            command: Command to run (for stdio connection)\n            args: Arguments for the command (for stdio connection)\n        \"\"\"\n        if connection_type:\n            self.connection_type = connection_type\n\n        # Connect to the MCP server based on connection type\n        if self.connection_type == \"sse\":\n            if not server_url:\n                raise ValueError(\"Server URL is required for SSE connection\")\n            await self.mcp_clients.connect_sse(server_url=server_url)\n        elif self.connection_type == \"stdio\":\n            if not command:\n                raise ValueError(\"Command is required for stdio connection\")\n            await self.mcp_clients.connect_stdio(command=command, args=args or [])\n        else:\n            raise ValueError(f\"Unsupported connection type: {self.connection_type}\")\n\n        # Set available_tools to our MCP instance\n        self.available_tools = self.mcp_clients\n\n        # Store initial tool schemas\n        await self._refresh_tools()\n\n        # Add system message about available tools\n        tool_names = list(self.mcp_clients.tool_map.keys())\n        tools_info = \", \".join(tool_names)\n\n        # Add system prompt and available tools information\n        self.memory.add_message(\n            Message.system_message(\n                f\"{self.system_prompt}\\n\\nAvailable MCP tools: {tools_info}\"\n            )\n        )\n\n    async def _refresh_tools(self) -> Tuple[List[str], List[str]]:\n        \"\"\"Refresh the list of available tools from the MCP server.\n\n        Returns:\n            A tuple of (added_tools, removed_tools)\n        \"\"\"\n        if not self.mcp_clients.sessions:\n            return [], []\n\n        # Get current tool schemas directly from the server\n        response = await self.mcp_clients.list_tools()\n        current_tools = {tool.name: tool.inputSchema for tool in response.tools}\n\n        # Determine added, removed, and changed tools\n        current_names = set(current_tools.keys())\n        previous_names = set(self.tool_schemas.keys())\n\n        added_tools = list(current_names - previous_names)\n        removed_tools = list(previous_names - current_names)\n\n        # Check for schema changes in existing tools\n        changed_tools = []\n        for name in current_names.intersection(previous_names):\n            if current_tools[name] != self.tool_schemas.get(name):\n                changed_tools.append(name)\n\n        # Update stored schemas\n        self.tool_schemas = current_tools\n\n        # Log and notify about changes\n        if added_tools:\n            logger.info(f\"Added MCP tools: {added_tools}\")\n            self.memory.add_message(\n                Message.system_message(f\"New tools available: {', '.join(added_tools)}\")\n            )\n        if removed_tools:\n            logger.info(f\"Removed MCP tools: {removed_tools}\")\n            self.memory.add_message(\n                Message.system_message(\n                    f\"Tools no longer available: {', '.join(removed_tools)}\"\n                )\n            )\n        if changed_tools:\n            logger.info(f\"Changed MCP tools: {changed_tools}\")\n\n        return added_tools, removed_tools\n\n    async def think(self) -> bool:\n        \"\"\"Process current state and decide next action.\"\"\"\n        # Check MCP session and tools availability\n        if not self.mcp_clients.sessions or not self.mcp_clients.tool_map:\n            logger.info(\"MCP service is no longer available, ending interaction\")\n            self.state = AgentState.FINISHED\n            return False\n\n        # Refresh tools periodically\n        if self.current_step % self._refresh_tools_interval == 0:\n            await self._refresh_tools()\n            # All tools removed indicates shutdown\n            if not self.mcp_clients.tool_map:\n                logger.info(\"MCP service has shut down, ending interaction\")\n                self.state = AgentState.FINISHED\n                return False\n\n        # Use the parent class's think method\n        return await super().think()\n\n    async def _handle_special_tool(self, name: str, result: Any, **kwargs) -> None:\n        \"\"\"Handle special tool execution and state changes\"\"\"\n        # First process with parent handler\n        await super()._handle_special_tool(name, result, **kwargs)\n\n        # Handle multimedia responses\n        if isinstance(result, ToolResult) and result.base64_image:\n            self.memory.add_message(\n                Message.system_message(\n                    MULTIMEDIA_RESPONSE_PROMPT.format(tool_name=name)\n                )\n            )\n\n    def _should_finish_execution(self, name: str, **kwargs) -> bool:\n        \"\"\"Determine if tool execution should finish the agent\"\"\"\n        # Terminate if the tool name is 'terminate'\n        return name.lower() == \"terminate\"\n\n    async def cleanup(self) -> None:\n        \"\"\"Clean up MCP connection when done.\"\"\"\n        if self.mcp_clients.sessions:\n            await self.mcp_clients.disconnect()\n            logger.info(\"MCP connection closed\")\n\n    async def run(self, request: Optional[str] = None) -> str:\n        \"\"\"Run the agent with cleanup when done.\"\"\"\n        try:\n            result = await super().run(request)\n            return result\n        finally:\n            # Ensure cleanup happens even if there's an error\n            await self.cleanup()",
    "Agent for interacting with MCP (Model Context Protocol) servers.\n\nThis agent connects to an MCP server using either SSE or stdio transport\nand makes the server's tools available through the agent's tool interface."
  ],
  [
    "async def initialize(\n        self,\n        connection_type: Optional[str] = None,\n        server_url: Optional[str] = None,\n        command: Optional[str] = None,\n        args: Optional[List[str]] = None,\n    ) -> None:\n        \"\"\"Initialize the MCP connection.\n\n        Args:\n            connection_type: Type of connection to use (\"stdio\" or \"sse\")\n            server_url: URL of the MCP server (for SSE connection)\n            command: Command to run (for stdio connection)\n            args: Arguments for the command (for stdio connection)\n        \"\"\"\n        if connection_type:\n            self.connection_type = connection_type\n\n        # Connect to the MCP server based on connection type\n        if self.connection_type == \"sse\":\n            if not server_url:\n                raise ValueError(\"Server URL is required for SSE connection\")\n            await self.mcp_clients.connect_sse(server_url=server_url)\n        elif self.connection_type == \"stdio\":\n            if not command:\n                raise ValueError(\"Command is required for stdio connection\")\n            await self.mcp_clients.connect_stdio(command=command, args=args or [])\n        else:\n            raise ValueError(f\"Unsupported connection type: {self.connection_type}\")\n\n        # Set available_tools to our MCP instance\n        self.available_tools = self.mcp_clients\n\n        # Store initial tool schemas\n        await self._refresh_tools()\n\n        # Add system message about available tools\n        tool_names = list(self.mcp_clients.tool_map.keys())\n        tools_info = \", \".join(tool_names)\n\n        # Add system prompt and available tools information\n        self.memory.add_message(\n            Message.system_message(\n                f\"{self.system_prompt}\\n\\nAvailable MCP tools: {tools_info}\"\n            )\n        )",
    "Initialize the MCP connection.\n\nArgs:\n    connection_type: Type of connection to use (\"stdio\" or \"sse\")\n    server_url: URL of the MCP server (for SSE connection)\n    command: Command to run (for stdio connection)\n    args: Arguments for the command (for stdio connection)"
  ],
  [
    "async def _refresh_tools(self) -> Tuple[List[str], List[str]]:\n        \"\"\"Refresh the list of available tools from the MCP server.\n\n        Returns:\n            A tuple of (added_tools, removed_tools)\n        \"\"\"\n        if not self.mcp_clients.sessions:\n            return [], []\n\n        # Get current tool schemas directly from the server\n        response = await self.mcp_clients.list_tools()\n        current_tools = {tool.name: tool.inputSchema for tool in response.tools}\n\n        # Determine added, removed, and changed tools\n        current_names = set(current_tools.keys())\n        previous_names = set(self.tool_schemas.keys())\n\n        added_tools = list(current_names - previous_names)\n        removed_tools = list(previous_names - current_names)\n\n        # Check for schema changes in existing tools\n        changed_tools = []\n        for name in current_names.intersection(previous_names):\n            if current_tools[name] != self.tool_schemas.get(name):\n                changed_tools.append(name)\n\n        # Update stored schemas\n        self.tool_schemas = current_tools\n\n        # Log and notify about changes\n        if added_tools:\n            logger.info(f\"Added MCP tools: {added_tools}\")\n            self.memory.add_message(\n                Message.system_message(f\"New tools available: {', '.join(added_tools)}\")\n            )\n        if removed_tools:\n            logger.info(f\"Removed MCP tools: {removed_tools}\")\n            self.memory.add_message(\n                Message.system_message(\n                    f\"Tools no longer available: {', '.join(removed_tools)}\"\n                )\n            )\n        if changed_tools:\n            logger.info(f\"Changed MCP tools: {changed_tools}\")\n\n        return added_tools, removed_tools",
    "Refresh the list of available tools from the MCP server.\n\nReturns:\n    A tuple of (added_tools, removed_tools)"
  ],
  [
    "async def think(self) -> bool:\n        \n        # Check MCP session and tools availability\n        if not self.mcp_clients.sessions or not self.mcp_clients.tool_map:\n            logger.info(\"MCP service is no longer available, ending interaction\")\n            self.state = AgentState.FINISHED\n            return False\n\n        # Refresh tools periodically\n        if self.current_step % self._refresh_tools_interval == 0:\n            await self._refresh_tools()\n            # All tools removed indicates shutdown\n            if not self.mcp_clients.tool_map:\n                logger.info(\"MCP service has shut down, ending interaction\")\n                self.state = AgentState.FINISHED\n                return False\n\n        # Use the parent class's think method\n        return await super().think()",
    "Process current state and decide next action."
  ],
  [
    "async def _handle_special_tool(self, name: str, result: Any, **kwargs) -> None:\n        \n        # First process with parent handler\n        await super()._handle_special_tool(name, result, **kwargs)\n\n        # Handle multimedia responses\n        if isinstance(result, ToolResult) and result.base64_image:\n            self.memory.add_message(\n                Message.system_message(\n                    MULTIMEDIA_RESPONSE_PROMPT.format(tool_name=name)\n                )\n            )",
    "Handle special tool execution and state changes"
  ],
  [
    "def _should_finish_execution(self, name: str, **kwargs) -> bool:\n        \n        # Terminate if the tool name is 'terminate'\n        return name.lower() == \"terminate\"",
    "Determine if tool execution should finish the agent"
  ],
  [
    "async def cleanup(self) -> None:\n        \n        if self.mcp_clients.sessions:\n            await self.mcp_clients.disconnect()\n            logger.info(\"MCP connection closed\")",
    "Clean up MCP connection when done."
  ],
  [
    "async def run(self, request: Optional[str] = None) -> str:\n        \n        try:\n            result = await super().run(request)\n            return result\n        finally:\n            # Ensure cleanup happens even if there's an error\n            await self.cleanup()",
    "Run the agent with cleanup when done."
  ],
  [
    "async def think(self) -> bool:",
    "Process current state and decide next action"
  ],
  [
    "async def step(self) -> str:\n        \n        should_act = await self.think()\n        if not should_act:\n            return \"Thinking complete - no action needed\"\n        return await self.act()",
    "Execute a single step: think and act."
  ],
  [
    "class SWEAgent(ToolCallAgent):\n    \n\n    name: str = \"swe\"\n    description: str = \"an autonomous AI programmer that interacts directly with the computer to solve tasks.\"\n\n    system_prompt: str = SYSTEM_PROMPT\n    next_step_prompt: str = \"\"\n\n    available_tools: ToolCollection = ToolCollection(\n        Bash(), StrReplaceEditor(), Terminate()\n    )\n    special_tool_names: List[str] = Field(default_factory=lambda: [Terminate().name])\n\n    max_steps: int = 20",
    "An agent that implements the SWEAgent paradigm for executing code and natural conversations."
  ],
  [
    "class ToolCallAgent(ReActAgent):\n    \n\n    name: str = \"toolcall\"\n    description: str = \"an agent that can execute tool calls.\"\n\n    system_prompt: str = SYSTEM_PROMPT\n    next_step_prompt: str = NEXT_STEP_PROMPT\n\n    available_tools: ToolCollection = ToolCollection(\n        CreateChatCompletion(), Terminate()\n    )\n    tool_choices: TOOL_CHOICE_TYPE = ToolChoice.AUTO  # type: ignore\n    special_tool_names: List[str] = Field(default_factory=lambda: [Terminate().name])\n\n    tool_calls: List[ToolCall] = Field(default_factory=list)\n    _current_base64_image: Optional[str] = None\n\n    max_steps: int = 30\n    max_observe: Optional[Union[int, bool]] = None\n\n    async def think(self) -> bool:\n        \"\"\"Process current state and decide next actions using tools\"\"\"\n        if self.next_step_prompt:\n            user_msg = Message.user_message(self.next_step_prompt)\n            self.messages += [user_msg]\n\n        try:\n            # Get response with tool options\n            response = await self.llm.ask_tool(\n                messages=self.messages,\n                system_msgs=(\n                    [Message.system_message(self.system_prompt)]\n                    if self.system_prompt\n                    else None\n                ),\n                tools=self.available_tools.to_params(),\n                tool_choice=self.tool_choices,\n            )\n        except ValueError:\n            raise\n        except Exception as e:\n            # Check if this is a RetryError containing TokenLimitExceeded\n            if hasattr(e, \"__cause__\") and isinstance(e.__cause__, TokenLimitExceeded):\n                token_limit_error = e.__cause__\n                logger.error(\n                    f\" Token limit error (from RetryError): {token_limit_error}\"\n                )\n                self.memory.add_message(\n                    Message.assistant_message(\n                        f\"Maximum token limit reached, cannot continue execution: {str(token_limit_error)}\"\n                    )\n                )\n                self.state = AgentState.FINISHED\n                return False\n            raise\n\n        self.tool_calls = tool_calls = (\n            response.tool_calls if response and response.tool_calls else []\n        )\n        content = response.content if response and response.content else \"\"\n\n        # Log response info\n        logger.info(f\" {self.name}'s thoughts: {content}\")\n        logger.info(\n            f\" {self.name} selected {len(tool_calls) if tool_calls else 0} tools to use\"\n        )\n        if tool_calls:\n            logger.info(\n                f\" Tools being prepared: {[call.function.name for call in tool_calls]}\"\n            )\n            logger.info(f\" Tool arguments: {tool_calls[0].function.arguments}\")\n\n        try:\n            if response is None:\n                raise RuntimeError(\"No response received from the LLM\")\n\n            # Handle different tool_choices modes\n            if self.tool_choices == ToolChoice.NONE:\n                if tool_calls:\n                    logger.warning(\n                        f\" Hmm, {self.name} tried to use tools when they weren't available!\"\n                    )\n                if content:\n                    self.memory.add_message(Message.assistant_message(content))\n                    return True\n                return False\n\n            # Create and add assistant message\n            assistant_msg = (\n                Message.from_tool_calls(content=content, tool_calls=self.tool_calls)\n                if self.tool_calls\n                else Message.assistant_message(content)\n            )\n            self.memory.add_message(assistant_msg)\n\n            if self.tool_choices == ToolChoice.REQUIRED and not self.tool_calls:\n                return True  # Will be handled in act()\n\n            # For 'auto' mode, continue with content if no commands but content exists\n            if self.tool_choices == ToolChoice.AUTO and not self.tool_calls:\n                return bool(content)\n\n            return bool(self.tool_calls)\n        except Exception as e:\n            logger.error(f\" Oops! The {self.name}'s thinking process hit a snag: {e}\")\n            self.memory.add_message(\n                Message.assistant_message(\n                    f\"Error encountered while processing: {str(e)}\"\n                )\n            )\n            return False\n\n    async def act(self) -> str:\n        \"\"\"Execute tool calls and handle their results\"\"\"\n        if not self.tool_calls:\n            if self.tool_choices == ToolChoice.REQUIRED:\n                raise ValueError(TOOL_CALL_REQUIRED)\n\n            # Return last message content if no tool calls\n            return self.messages[-1].content or \"No content or commands to execute\"\n\n        results = []\n        for command in self.tool_calls:\n            # Reset base64_image for each tool call\n            self._current_base64_image = None\n\n            result = await self.execute_tool(command)\n\n            if self.max_observe:\n                result = result[: self.max_observe]\n\n            logger.info(\n                f\" Tool '{command.function.name}' completed its mission! Result: {result}\"\n            )\n\n            # Add tool response to memory\n            tool_msg = Message.tool_message(\n                content=result,\n                tool_call_id=command.id,\n                name=command.function.name,\n                base64_image=self._current_base64_image,\n            )\n            self.memory.add_message(tool_msg)\n            results.append(result)\n\n        return \"\\n\\n\".join(results)\n\n    async def execute_tool(self, command: ToolCall) -> str:\n        \"\"\"Execute a single tool call with robust error handling\"\"\"\n        if not command or not command.function or not command.function.name:\n            return \"Error: Invalid command format\"\n\n        name = command.function.name\n        if name not in self.available_tools.tool_map:\n            return f\"Error: Unknown tool '{name}'\"\n\n        try:\n            # Parse arguments\n            args = json.loads(command.function.arguments or \"{}\")\n\n            # Execute the tool\n            logger.info(f\" Activating tool: '{name}'...\")\n            result = await self.available_tools.execute(name=name, tool_input=args)\n\n            # Handle special tools\n            await self._handle_special_tool(name=name, result=result)\n\n            # Check if result is a ToolResult with base64_image\n            if hasattr(result, \"base64_image\") and result.base64_image:\n                # Store the base64_image for later use in tool_message\n                self._current_base64_image = result.base64_image\n\n            # Format result for display (standard case)\n            observation = (\n                f\"Observed output of cmd `{name}` executed:\\n{str(result)}\"\n                if result\n                else f\"Cmd `{name}` completed with no output\"\n            )\n\n            return observation\n        except json.JSONDecodeError:\n            error_msg = f\"Error parsing arguments for {name}: Invalid JSON format\"\n            logger.error(\n                f\" Oops! The arguments for '{name}' don't make sense - invalid JSON, arguments:{command.function.arguments}\"\n            )\n            return f\"Error: {error_msg}\"\n        except Exception as e:\n            error_msg = f\" Tool '{name}' encountered a problem: {str(e)}\"\n            logger.exception(error_msg)\n            return f\"Error: {error_msg}\"\n\n    async def _handle_special_tool(self, name: str, result: Any, **kwargs):\n        \"\"\"Handle special tool execution and state changes\"\"\"\n        if not self._is_special_tool(name):\n            return\n\n        if self._should_finish_execution(name=name, result=result, **kwargs):\n            # Set agent state to finished\n            logger.info(f\" Special tool '{name}' has completed the task!\")\n            self.state = AgentState.FINISHED\n\n    @staticmethod\n    def _should_finish_execution(**kwargs) -> bool:\n        \"\"\"Determine if tool execution should finish the agent\"\"\"\n        return True\n\n    def _is_special_tool(self, name: str) -> bool:\n        \"\"\"Check if tool name is in special tools list\"\"\"\n        return name.lower() in [n.lower() for n in self.special_tool_names]\n\n    async def cleanup(self):\n        \"\"\"Clean up resources used by the agent's tools.\"\"\"\n        logger.info(f\" Cleaning up resources for agent '{self.name}'...\")\n        for tool_name, tool_instance in self.available_tools.tool_map.items():\n            if hasattr(tool_instance, \"cleanup\") and asyncio.iscoroutinefunction(\n                tool_instance.cleanup\n            ):\n                try:\n                    logger.debug(f\" Cleaning up tool: {tool_name}\")\n                    await tool_instance.cleanup()\n                except Exception as e:\n                    logger.error(\n                        f\" Error cleaning up tool '{tool_name}': {e}\", exc_info=True\n                    )\n        logger.info(f\" Cleanup complete for agent '{self.name}'.\")\n\n    async def run(self, request: Optional[str] = None) -> str:\n        \"\"\"Run the agent with cleanup when done.\"\"\"\n        try:\n            return await super().run(request)\n        finally:\n            await self.cleanup()",
    "Base agent class for handling tool/function calls with enhanced abstraction"
  ],
  [
    "async def think(self) -> bool:\n        \n        if self.next_step_prompt:\n            user_msg = Message.user_message(self.next_step_prompt)\n            self.messages += [user_msg]\n\n        try:\n            # Get response with tool options\n            response = await self.llm.ask_tool(\n                messages=self.messages,\n                system_msgs=(\n                    [Message.system_message(self.system_prompt)]\n                    if self.system_prompt\n                    else None\n                ),\n                tools=self.available_tools.to_params(),\n                tool_choice=self.tool_choices,\n            )\n        except ValueError:\n            raise\n        except Exception as e:\n            # Check if this is a RetryError containing TokenLimitExceeded\n            if hasattr(e, \"__cause__\") and isinstance(e.__cause__, TokenLimitExceeded):\n                token_limit_error = e.__cause__\n                logger.error(\n                    f\" Token limit error (from RetryError): {token_limit_error}\"\n                )\n                self.memory.add_message(\n                    Message.assistant_message(\n                        f\"Maximum token limit reached, cannot continue execution: {str(token_limit_error)}\"\n                    )\n                )\n                self.state = AgentState.FINISHED\n                return False\n            raise\n\n        self.tool_calls = tool_calls = (\n            response.tool_calls if response and response.tool_calls else []\n        )\n        content = response.content if response and response.content else \"\"\n\n        # Log response info\n        logger.info(f\" {self.name}'s thoughts: {content}\")\n        logger.info(\n            f\" {self.name} selected {len(tool_calls) if tool_calls else 0} tools to use\"\n        )\n        if tool_calls:\n            logger.info(\n                f\" Tools being prepared: {[call.function.name for call in tool_calls]}\"\n            )\n            logger.info(f\" Tool arguments: {tool_calls[0].function.arguments}\")\n\n        try:\n            if response is None:\n                raise RuntimeError(\"No response received from the LLM\")\n\n            # Handle different tool_choices modes\n            if self.tool_choices == ToolChoice.NONE:\n                if tool_calls:\n                    logger.warning(\n                        f\" Hmm, {self.name} tried to use tools when they weren't available!\"\n                    )\n                if content:\n                    self.memory.add_message(Message.assistant_message(content))\n                    return True\n                return False\n\n            # Create and add assistant message\n            assistant_msg = (\n                Message.from_tool_calls(content=content, tool_calls=self.tool_calls)\n                if self.tool_calls\n                else Message.assistant_message(content)\n            )\n            self.memory.add_message(assistant_msg)\n\n            if self.tool_choices == ToolChoice.REQUIRED and not self.tool_calls:\n                return True  # Will be handled in act()\n\n            # For 'auto' mode, continue with content if no commands but content exists\n            if self.tool_choices == ToolChoice.AUTO and not self.tool_calls:\n                return bool(content)\n\n            return bool(self.tool_calls)\n        except Exception as e:\n            logger.error(f\" Oops! The {self.name}'s thinking process hit a snag: {e}\")\n            self.memory.add_message(\n                Message.assistant_message(\n                    f\"Error encountered while processing: {str(e)}\"\n                )\n            )\n            return False",
    "Process current state and decide next actions using tools"
  ],
  [
    "async def act(self) -> str:\n        \n        if not self.tool_calls:\n            if self.tool_choices == ToolChoice.REQUIRED:\n                raise ValueError(TOOL_CALL_REQUIRED)\n\n            # Return last message content if no tool calls\n            return self.messages[-1].content or \"No content or commands to execute\"\n\n        results = []\n        for command in self.tool_calls:\n            # Reset base64_image for each tool call\n            self._current_base64_image = None\n\n            result = await self.execute_tool(command)\n\n            if self.max_observe:\n                result = result[: self.max_observe]\n\n            logger.info(\n                f\" Tool '{command.function.name}' completed its mission! Result: {result}\"\n            )\n\n            # Add tool response to memory\n            tool_msg = Message.tool_message(\n                content=result,\n                tool_call_id=command.id,\n                name=command.function.name,\n                base64_image=self._current_base64_image,\n            )\n            self.memory.add_message(tool_msg)\n            results.append(result)\n\n        return \"\\n\\n\".join(results)",
    "Execute tool calls and handle their results"
  ],
  [
    "async def execute_tool(self, command: ToolCall) -> str:\n        \n        if not command or not command.function or not command.function.name:\n            return \"Error: Invalid command format\"\n\n        name = command.function.name\n        if name not in self.available_tools.tool_map:\n            return f\"Error: Unknown tool '{name}'\"\n\n        try:\n            # Parse arguments\n            args = json.loads(command.function.arguments or \"{}\")\n\n            # Execute the tool\n            logger.info(f\" Activating tool: '{name}'...\")\n            result = await self.available_tools.execute(name=name, tool_input=args)\n\n            # Handle special tools\n            await self._handle_special_tool(name=name, result=result)\n\n            # Check if result is a ToolResult with base64_image\n            if hasattr(result, \"base64_image\") and result.base64_image:\n                # Store the base64_image for later use in tool_message\n                self._current_base64_image = result.base64_image\n\n            # Format result for display (standard case)\n            observation = (\n                f\"Observed output of cmd `{name}` executed:\\n{str(result)}\"\n                if result\n                else f\"Cmd `{name}` completed with no output\"\n            )\n\n            return observation\n        except json.JSONDecodeError:\n            error_msg = f\"Error parsing arguments for {name}: Invalid JSON format\"\n            logger.error(\n                f\" Oops! The arguments for '{name}' don't make sense - invalid JSON, arguments:{command.function.arguments}\"\n            )\n            return f\"Error: {error_msg}\"\n        except Exception as e:\n            error_msg = f\" Tool '{name}' encountered a problem: {str(e)}\"\n            logger.exception(error_msg)\n            return f\"Error: {error_msg}\"",
    "Execute a single tool call with robust error handling"
  ],
  [
    "async def _handle_special_tool(self, name: str, result: Any, **kwargs):\n        \n        if not self._is_special_tool(name):\n            return\n\n        if self._should_finish_execution(name=name, result=result, **kwargs):\n            # Set agent state to finished\n            logger.info(f\" Special tool '{name}' has completed the task!\")\n            self.state = AgentState.FINISHED",
    "Handle special tool execution and state changes"
  ],
  [
    "def _should_finish_execution(**kwargs) -> bool:\n        \n        return True",
    "Determine if tool execution should finish the agent"
  ],
  [
    "def _is_special_tool(self, name: str) -> bool:\n        \n        return name.lower() in [n.lower() for n in self.special_tool_names]",
    "Check if tool name is in special tools list"
  ],
  [
    "async def cleanup(self):\n        \n        logger.info(f\" Cleaning up resources for agent '{self.name}'...\")\n        for tool_name, tool_instance in self.available_tools.tool_map.items():\n            if hasattr(tool_instance, \"cleanup\") and asyncio.iscoroutinefunction(\n                tool_instance.cleanup\n            ):\n                try:\n                    logger.debug(f\" Cleaning up tool: {tool_name}\")\n                    await tool_instance.cleanup()\n                except Exception as e:\n                    logger.error(\n                        f\" Error cleaning up tool '{tool_name}': {e}\", exc_info=True\n                    )\n        logger.info(f\" Cleanup complete for agent '{self.name}'.\")",
    "Clean up resources used by the agent's tools."
  ],
  [
    "async def run(self, request: Optional[str] = None) -> str:\n        \n        try:\n            return await super().run(request)\n        finally:\n            await self.cleanup()",
    "Run the agent with cleanup when done."
  ],
  [
    "class MCPServerConfig(BaseModel):\n    \n\n    type: str = Field(..., description=\"Server connection type (sse or stdio)\")\n    url: Optional[str] = Field(None, description=\"Server URL for SSE connections\")\n    command: Optional[str] = Field(None, description=\"Command for stdio connections\")\n    args: List[str] = Field(\n        default_factory=list, description=\"Arguments for stdio command\"\n    )",
    "Configuration for a single MCP server"
  ],
  [
    "class MCPSettings(BaseModel):\n    \n\n    server_reference: str = Field(\n        \"app.mcp.server\", description=\"Module reference for the MCP server\"\n    )\n    servers: Dict[str, MCPServerConfig] = Field(\n        default_factory=dict, description=\"MCP server configurations\"\n    )\n\n    @classmethod\n    def load_server_config(cls) -> Dict[str, MCPServerConfig]:\n        \"\"\"Load MCP server configuration from JSON file\"\"\"\n        config_path = PROJECT_ROOT / \"config\" / \"mcp.json\"\n\n        try:\n            config_file = config_path if config_path.exists() else None\n            if not config_file:\n                return {}\n\n            with config_file.open() as f:\n                data = json.load(f)\n                servers = {}\n\n                for server_id, server_config in data.get(\"mcpServers\", {}).items():\n                    servers[server_id] = MCPServerConfig(\n                        type=server_config[\"type\"],\n                        url=server_config.get(\"url\"),\n                        command=server_config.get(\"command\"),\n                        args=server_config.get(\"args\", []),\n                    )\n                return servers\n        except Exception as e:\n            raise ValueError(f\"Failed to load MCP server config: {e}\")",
    "Configuration for MCP (Model Context Protocol)"
  ],
  [
    "def load_server_config(cls) -> Dict[str, MCPServerConfig]:\n        \n        config_path = PROJECT_ROOT / \"config\" / \"mcp.json\"\n\n        try:\n            config_file = config_path if config_path.exists() else None\n            if not config_file:\n                return {}\n\n            with config_file.open() as f:\n                data = json.load(f)\n                servers = {}\n\n                for server_id, server_config in data.get(\"mcpServers\", {}).items():\n                    servers[server_id] = MCPServerConfig(\n                        type=server_config[\"type\"],\n                        url=server_config.get(\"url\"),\n                        command=server_config.get(\"command\"),\n                        args=server_config.get(\"args\", []),\n                    )\n                return servers\n        except Exception as e:\n            raise ValueError(f\"Failed to load MCP server config: {e}\")",
    "Load MCP server configuration from JSON file"
  ],
  [
    "def root_path(self) -> Path:\n        \n        return PROJECT_ROOT",
    "Get the root path of the application"
  ],
  [
    "class ToolError(Exception):\n    \n\n    def __init__(self, message):\n        self.message = message",
    "Raised when a tool encounters an error."
  ],
  [
    "class OpenManusError(Exception):",
    "Base exception for all OpenManus errors"
  ],
  [
    "class TokenLimitExceeded(OpenManusError):",
    "Exception raised when the token limit is exceeded"
  ],
  [
    "class BaseFlow(BaseModel, ABC):\n    \n\n    agents: Dict[str, BaseAgent]\n    tools: Optional[List] = None\n    primary_agent_key: Optional[str] = None\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def __init__(\n        self, agents: Union[BaseAgent, List[BaseAgent], Dict[str, BaseAgent]], **data\n    ):\n        # Handle different ways of providing agents\n        if isinstance(agents, BaseAgent):\n            agents_dict = {\"default\": agents}\n        elif isinstance(agents, list):\n            agents_dict = {f\"agent_{i}\": agent for i, agent in enumerate(agents)}\n        else:\n            agents_dict = agents\n\n        # If primary agent not specified, use first agent\n        primary_key = data.get(\"primary_agent_key\")\n        if not primary_key and agents_dict:\n            primary_key = next(iter(agents_dict))\n            data[\"primary_agent_key\"] = primary_key\n\n        # Set the agents dictionary\n        data[\"agents\"] = agents_dict\n\n        # Initialize using BaseModel's init\n        super().__init__(**data)\n\n    @property\n    def primary_agent(self) -> Optional[BaseAgent]:\n        \"\"\"Get the primary agent for the flow\"\"\"\n        return self.agents.get(self.primary_agent_key)\n\n    def get_agent(self, key: str) -> Optional[BaseAgent]:\n        \"\"\"Get a specific agent by key\"\"\"\n        return self.agents.get(key)\n\n    def add_agent(self, key: str, agent: BaseAgent) -> None:\n        \"\"\"Add a new agent to the flow\"\"\"\n        self.agents[key] = agent\n\n    @abstractmethod\n    async def execute(self, input_text: str) -> str:\n        \"\"\"Execute the flow with given input\"\"\"",
    "Base class for execution flows supporting multiple agents"
  ],
  [
    "def primary_agent(self) -> Optional[BaseAgent]:\n        \n        return self.agents.get(self.primary_agent_key)",
    "Get the primary agent for the flow"
  ],
  [
    "def get_agent(self, key: str) -> Optional[BaseAgent]:\n        \n        return self.agents.get(key)",
    "Get a specific agent by key"
  ],
  [
    "def add_agent(self, key: str, agent: BaseAgent) -> None:\n        \n        self.agents[key] = agent",
    "Add a new agent to the flow"
  ],
  [
    "async def execute(self, input_text: str) -> str:",
    "Execute the flow with given input"
  ],
  [
    "class FlowFactory:\n    \n\n    @staticmethod\n    def create_flow(\n        flow_type: FlowType,\n        agents: Union[BaseAgent, List[BaseAgent], Dict[str, BaseAgent]],\n        **kwargs,\n    ) -> BaseFlow:\n        flows = {\n            FlowType.PLANNING: PlanningFlow,\n        }\n\n        flow_class = flows.get(flow_type)\n        if not flow_class:\n            raise ValueError(f\"Unknown flow type: {flow_type}\")\n\n        return flow_class(agents, **kwargs)",
    "Factory for creating different types of flows with support for multiple agents"
  ],
  [
    "class PlanStepStatus(str, Enum):\n    \n\n    NOT_STARTED = \"not_started\"\n    IN_PROGRESS = \"in_progress\"\n    COMPLETED = \"completed\"\n    BLOCKED = \"blocked\"\n\n    @classmethod\n    def get_all_statuses(cls) -> list[str]:\n        \"\"\"Return a list of all possible step status values\"\"\"\n        return [status.value for status in cls]\n\n    @classmethod\n    def get_active_statuses(cls) -> list[str]:\n        \"\"\"Return a list of values representing active statuses (not started or in progress)\"\"\"\n        return [cls.NOT_STARTED.value, cls.IN_PROGRESS.value]\n\n    @classmethod\n    def get_status_marks(cls) -> Dict[str, str]:\n        \"\"\"Return a mapping of statuses to their marker symbols\"\"\"\n        return {\n            cls.COMPLETED.value: \"[]\",\n            cls.IN_PROGRESS.value: \"[]\",\n            cls.BLOCKED.value: \"[!]\",\n            cls.NOT_STARTED.value: \"[ ]\",\n        }",
    "Enum class defining possible statuses of a plan step"
  ],
  [
    "class PlanningFlow(BaseFlow):\n    \n\n    llm: LLM = Field(default_factory=lambda: LLM())\n    planning_tool: PlanningTool = Field(default_factory=PlanningTool)\n    executor_keys: List[str] = Field(default_factory=list)\n    active_plan_id: str = Field(default_factory=lambda: f\"plan_{int(time.time())}\")\n    current_step_index: Optional[int] = None\n\n    def __init__(\n        self, agents: Union[BaseAgent, List[BaseAgent], Dict[str, BaseAgent]], **data\n    ):\n        # Set executor keys before super().__init__\n        if \"executors\" in data:\n            data[\"executor_keys\"] = data.pop(\"executors\")\n\n        # Set plan ID if provided\n        if \"plan_id\" in data:\n            data[\"active_plan_id\"] = data.pop(\"plan_id\")\n\n        # Initialize the planning tool if not provided\n        if \"planning_tool\" not in data:\n            planning_tool = PlanningTool()\n            data[\"planning_tool\"] = planning_tool\n\n        # Call parent's init with the processed data\n        super().__init__(agents, **data)\n\n        # Set executor_keys to all agent keys if not specified\n        if not self.executor_keys:\n            self.executor_keys = list(self.agents.keys())\n\n    def get_executor(self, step_type: Optional[str] = None) -> BaseAgent:\n        \"\"\"\n        Get an appropriate executor agent for the current step.\n        Can be extended to select agents based on step type/requirements.\n        \"\"\"\n        # If step type is provided and matches an agent key, use that agent\n        if step_type and step_type in self.agents:\n            return self.agents[step_type]\n\n        # Otherwise use the first available executor or fall back to primary agent\n        for key in self.executor_keys:\n            if key in self.agents:\n                return self.agents[key]\n\n        # Fallback to primary agent\n        return self.primary_agent\n\n    async def execute(self, input_text: str) -> str:\n        \"\"\"Execute the planning flow with agents.\"\"\"\n        try:\n            if not self.primary_agent:\n                raise ValueError(\"No primary agent available\")\n\n            # Create initial plan if input provided\n            if input_text:\n                await self._create_initial_plan(input_text)\n\n                # Verify plan was created successfully\n                if self.active_plan_id not in self.planning_tool.plans:\n                    logger.error(\n                        f\"Plan creation failed. Plan ID {self.active_plan_id} not found in planning tool.\"\n                    )\n                    return f\"Failed to create plan for: {input_text}\"\n\n            result = \"\"\n            while True:\n                # Get current step to execute\n                self.current_step_index, step_info = await self._get_current_step_info()\n\n                # Exit if no more steps or plan completed\n                if self.current_step_index is None:\n                    result += await self._finalize_plan()\n                    break\n\n                # Execute current step with appropriate agent\n                step_type = step_info.get(\"type\") if step_info else None\n                executor = self.get_executor(step_type)\n                step_result = await self._execute_step(executor, step_info)\n                result += step_result + \"\\n\"\n\n                # Check if agent wants to terminate\n                if hasattr(executor, \"state\") and executor.state == AgentState.FINISHED:\n                    break\n\n            return result\n        except Exception as e:\n            logger.error(f\"Error in PlanningFlow: {str(e)}\")\n            return f\"Execution failed: {str(e)}\"\n\n    async def _create_initial_plan(self, request: str) -> None:\n        \"\"\"Create an initial plan based on the request using the flow's LLM and PlanningTool.\"\"\"\n        logger.info(f\"Creating initial plan with ID: {self.active_plan_id}\")\n\n        system_message_content = (\n            \"You are a planning assistant. Create a concise, actionable plan with clear steps. \"\n            \"Focus on key milestones rather than detailed sub-steps. \"\n            \"Optimize for clarity and efficiency.\"\n        )\n        agents_description = []\n        for key in self.executor_keys:\n            if key in self.agents:\n                agents_description.append(\n                    {\n                        \"name\": key.upper(),\n                        \"description\": self.agents[key].description,\n                    }\n                )\n        if len(agents_description) > 1:\n            # Add description of agents to select\n            system_message_content += (\n                f\"\\nNow we have {agents_description} agents. \"\n                f\"The infomation of them are below: {json.dumps(agents_description)}\\n\"\n                \"When creating steps in the planning tool, please specify the agent names using the format '[agent_name]'.\"\n            )\n\n        # Create a system message for plan creation\n        system_message = Message.system_message(system_message_content)\n\n        # Create a user message with the request\n        user_message = Message.user_message(\n            f\"Create a reasonable plan with clear steps to accomplish the task: {request}\"\n        )\n\n        # Call LLM with PlanningTool\n        response = await self.llm.ask_tool(\n            messages=[user_message],\n            system_msgs=[system_message],\n            tools=[self.planning_tool.to_param()],\n            tool_choice=ToolChoice.AUTO,\n        )\n\n        # Process tool calls if present\n        if response.tool_calls:\n            for tool_call in response.tool_calls:\n                if tool_call.function.name == \"planning\":\n                    # Parse the arguments\n                    args = tool_call.function.arguments\n                    if isinstance(args, str):\n                        try:\n                            args = json.loads(args)\n                        except json.JSONDecodeError:\n                            logger.error(f\"Failed to parse tool arguments: {args}\")\n                            continue\n\n                    # Ensure plan_id is set correctly and execute the tool\n                    args[\"plan_id\"] = self.active_plan_id\n\n                    # Execute the tool via ToolCollection instead of directly\n                    result = await self.planning_tool.execute(**args)\n\n                    logger.info(f\"Plan creation result: {str(result)}\")\n                    return\n\n        # If execution reached here, create a default plan\n        logger.warning(\"Creating default plan\")\n\n        # Create default plan using the ToolCollection\n        await self.planning_tool.execute(\n            **{\n                \"command\": \"create\",\n                \"plan_id\": self.active_plan_id,\n                \"title\": f\"Plan for: {request[:50]}{'...' if len(request) > 50 else ''}\",\n                \"steps\": [\"Analyze request\", \"Execute task\", \"Verify results\"],\n            }\n        )\n\n    async def _get_current_step_info(self) -> tuple[Optional[int], Optional[dict]]:\n        \"\"\"\n        Parse the current plan to identify the first non-completed step's index and info.\n        Returns (None, None) if no active step is found.\n        \"\"\"\n        if (\n            not self.active_plan_id\n            or self.active_plan_id not in self.planning_tool.plans\n        ):\n            logger.error(f\"Plan with ID {self.active_plan_id} not found\")\n            return None, None\n\n        try:\n            # Direct access to plan data from planning tool storage\n            plan_data = self.planning_tool.plans[self.active_plan_id]\n            steps = plan_data.get(\"steps\", [])\n            step_statuses = plan_data.get(\"step_statuses\", [])\n\n            # Find first non-completed step\n            for i, step in enumerate(steps):\n                if i >= len(step_statuses):\n                    status = PlanStepStatus.NOT_STARTED.value\n                else:\n                    status = step_statuses[i]\n\n                if status in PlanStepStatus.get_active_statuses():\n                    # Extract step type/category if available\n                    step_info = {\"text\": step}\n\n                    # Try to extract step type from the text (e.g., [SEARCH] or [CODE])\n                    import re\n\n                    type_match = re.search(r\"\\[([A-Z_]+)\\]\", step)\n                    if type_match:\n                        step_info[\"type\"] = type_match.group(1).lower()\n\n                    # Mark current step as in_progress\n                    try:\n                        await self.planning_tool.execute(\n                            command=\"mark_step\",\n                            plan_id=self.active_plan_id,\n                            step_index=i,\n                            step_status=PlanStepStatus.IN_PROGRESS.value,\n                        )\n                    except Exception as e:\n                        logger.warning(f\"Error marking step as in_progress: {e}\")\n                        # Update step status directly if needed\n                        if i < len(step_statuses):\n                            step_statuses[i] = PlanStepStatus.IN_PROGRESS.value\n                        else:\n                            while len(step_statuses) < i:\n                                step_statuses.append(PlanStepStatus.NOT_STARTED.value)\n                            step_statuses.append(PlanStepStatus.IN_PROGRESS.value)\n\n                        plan_data[\"step_statuses\"] = step_statuses\n\n                    return i, step_info\n\n            return None, None  # No active step found\n\n        except Exception as e:\n            logger.warning(f\"Error finding current step index: {e}\")\n            return None, None\n\n    async def _execute_step(self, executor: BaseAgent, step_info: dict) -> str:\n        \"\"\"Execute the current step with the specified agent using agent.run().\"\"\"\n        # Prepare context for the agent with current plan status\n        plan_status = await self._get_plan_text()\n        step_text = step_info.get(\"text\", f\"Step {self.current_step_index}\")\n\n        # Create a prompt for the agent to execute the current step\n        step_prompt = f\"\"\"\n        CURRENT PLAN STATUS:\n        {plan_status}\n\n        YOUR CURRENT TASK:\n        You are now working on step {self.current_step_index}: \"{step_text}\"\n\n        Please only execute this current step using the appropriate tools. When you're done, provide a summary of what you accomplished.\n        \"\"\"\n\n        # Use agent.run() to execute the step\n        try:\n            step_result = await executor.run(step_prompt)\n\n            # Mark the step as completed after successful execution\n            await self._mark_step_completed()\n\n            return step_result\n        except Exception as e:\n            logger.error(f\"Error executing step {self.current_step_index}: {e}\")\n            return f\"Error executing step {self.current_step_index}: {str(e)}\"\n\n    async def _mark_step_completed(self) -> None:\n        \"\"\"Mark the current step as completed.\"\"\"\n        if self.current_step_index is None:\n            return\n\n        try:\n            # Mark the step as completed\n            await self.planning_tool.execute(\n                command=\"mark_step\",\n                plan_id=self.active_plan_id,\n                step_index=self.current_step_index,\n                step_status=PlanStepStatus.COMPLETED.value,\n            )\n            logger.info(\n                f\"Marked step {self.current_step_index} as completed in plan {self.active_plan_id}\"\n            )\n        except Exception as e:\n            logger.warning(f\"Failed to update plan status: {e}\")\n            # Update step status directly in planning tool storage\n            if self.active_plan_id in self.planning_tool.plans:\n                plan_data = self.planning_tool.plans[self.active_plan_id]\n                step_statuses = plan_data.get(\"step_statuses\", [])\n\n                # Ensure the step_statuses list is long enough\n                while len(step_statuses) <= self.current_step_index:\n                    step_statuses.append(PlanStepStatus.NOT_STARTED.value)\n\n                # Update the status\n                step_statuses[self.current_step_index] = PlanStepStatus.COMPLETED.value\n                plan_data[\"step_statuses\"] = step_statuses\n\n    async def _get_plan_text(self) -> str:\n        \"\"\"Get the current plan as formatted text.\"\"\"\n        try:\n            result = await self.planning_tool.execute(\n                command=\"get\", plan_id=self.active_plan_id\n            )\n            return result.output if hasattr(result, \"output\") else str(result)\n        except Exception as e:\n            logger.error(f\"Error getting plan: {e}\")\n            return self._generate_plan_text_from_storage()\n\n    def _generate_plan_text_from_storage(self) -> str:\n        \"\"\"Generate plan text directly from storage if the planning tool fails.\"\"\"\n        try:\n            if self.active_plan_id not in self.planning_tool.plans:\n                return f\"Error: Plan with ID {self.active_plan_id} not found\"\n\n            plan_data = self.planning_tool.plans[self.active_plan_id]\n            title = plan_data.get(\"title\", \"Untitled Plan\")\n            steps = plan_data.get(\"steps\", [])\n            step_statuses = plan_data.get(\"step_statuses\", [])\n            step_notes = plan_data.get(\"step_notes\", [])\n\n            # Ensure step_statuses and step_notes match the number of steps\n            while len(step_statuses) < len(steps):\n                step_statuses.append(PlanStepStatus.NOT_STARTED.value)\n            while len(step_notes) < len(steps):\n                step_notes.append(\"\")\n\n            # Count steps by status\n            status_counts = {status: 0 for status in PlanStepStatus.get_all_statuses()}\n\n            for status in step_statuses:\n                if status in status_counts:\n                    status_counts[status] += 1\n\n            completed = status_counts[PlanStepStatus.COMPLETED.value]\n            total = len(steps)\n            progress = (completed / total) * 100 if total > 0 else 0\n\n            plan_text = f\"Plan: {title} (ID: {self.active_plan_id})\\n\"\n            plan_text += \"=\" * len(plan_text) + \"\\n\\n\"\n\n            plan_text += (\n                f\"Progress: {completed}/{total} steps completed ({progress:.1f}%)\\n\"\n            )\n            plan_text += f\"Status: {status_counts[PlanStepStatus.COMPLETED.value]} completed, {status_counts[PlanStepStatus.IN_PROGRESS.value]} in progress, \"\n            plan_text += f\"{status_counts[PlanStepStatus.BLOCKED.value]} blocked, {status_counts[PlanStepStatus.NOT_STARTED.value]} not started\\n\\n\"\n            plan_text += \"Steps:\\n\"\n\n            status_marks = PlanStepStatus.get_status_marks()\n\n            for i, (step, status, notes) in enumerate(\n                zip(steps, step_statuses, step_notes)\n            ):\n                # Use status marks to indicate step status\n                status_mark = status_marks.get(\n                    status, status_marks[PlanStepStatus.NOT_STARTED.value]\n                )\n\n                plan_text += f\"{i}. {status_mark} {step}\\n\"\n                if notes:\n                    plan_text += f\"   Notes: {notes}\\n\"\n\n            return plan_text\n        except Exception as e:\n            logger.error(f\"Error generating plan text from storage: {e}\")\n            return f\"Error: Unable to retrieve plan with ID {self.active_plan_id}\"\n\n    async def _finalize_plan(self) -> str:\n        \"\"\"Finalize the plan and provide a summary using the flow's LLM directly.\"\"\"\n        plan_text = await self._get_plan_text()\n\n        # Create a summary using the flow's LLM directly\n        try:\n            system_message = Message.system_message(\n                \"You are a planning assistant. Your task is to summarize the completed plan.\"\n            )\n\n            user_message = Message.user_message(\n                f\"The plan has been completed. Here is the final plan status:\\n\\n{plan_text}\\n\\nPlease provide a summary of what was accomplished and any final thoughts.\"\n            )\n\n            response = await self.llm.ask(\n                messages=[user_message], system_msgs=[system_message]\n            )\n\n            return f\"Plan completed:\\n\\n{response}\"\n        except Exception as e:\n            logger.error(f\"Error finalizing plan with LLM: {e}\")\n\n            # Fallback to using an agent for the summary\n            try:\n                agent = self.primary_agent\n                summary_prompt = f\"\"\"\n                The plan has been completed. Here is the final plan status:\n\n                {plan_text}\n\n                Please provide a summary of what was accomplished and any final thoughts.\n                \"\"\"\n                summary = await agent.run(summary_prompt)\n                return f\"Plan completed:\\n\\n{summary}\"\n            except Exception as e2:\n                logger.error(f\"Error finalizing plan with agent: {e2}\")\n                return \"Plan completed. Error generating summary.\"",
    "A flow that manages planning and execution of tasks using agents."
  ],
  [
    "def get_all_statuses(cls) -> list[str]:\n        \n        return [status.value for status in cls]",
    "Return a list of all possible step status values"
  ],
  [
    "def get_active_statuses(cls) -> list[str]:\n        \n        return [cls.NOT_STARTED.value, cls.IN_PROGRESS.value]",
    "Return a list of values representing active statuses (not started or in progress)"
  ],
  [
    "def get_status_marks(cls) -> Dict[str, str]:\n        \n        return {\n            cls.COMPLETED.value: \"[]\",\n            cls.IN_PROGRESS.value: \"[]\",\n            cls.BLOCKED.value: \"[!]\",\n            cls.NOT_STARTED.value: \"[ ]\",\n        }",
    "Return a mapping of statuses to their marker symbols"
  ],
  [
    "def get_executor(self, step_type: Optional[str] = None) -> BaseAgent:\n        \"\"\"\n        Get an appropriate executor agent for the current step.\n        Can be extended to select agents based on step type/requirements.\n        \"\"\"\n        # If step type is provided and matches an agent key, use that agent\n        if step_type and step_type in self.agents:\n            return self.agents[step_type]\n\n        # Otherwise use the first available executor or fall back to primary agent\n        for key in self.executor_keys:\n            if key in self.agents:\n                return self.agents[key]\n\n        # Fallback to primary agent\n        return self.primary_agent",
    "Get an appropriate executor agent for the current step.\nCan be extended to select agents based on step type/requirements."
  ],
  [
    "async def execute(self, input_text: str) -> str:\n        \n        try:\n            if not self.primary_agent:\n                raise ValueError(\"No primary agent available\")\n\n            # Create initial plan if input provided\n            if input_text:\n                await self._create_initial_plan(input_text)\n\n                # Verify plan was created successfully\n                if self.active_plan_id not in self.planning_tool.plans:\n                    logger.error(\n                        f\"Plan creation failed. Plan ID {self.active_plan_id} not found in planning tool.\"\n                    )\n                    return f\"Failed to create plan for: {input_text}\"\n\n            result = \"\"\n            while True:\n                # Get current step to execute\n                self.current_step_index, step_info = await self._get_current_step_info()\n\n                # Exit if no more steps or plan completed\n                if self.current_step_index is None:\n                    result += await self._finalize_plan()\n                    break\n\n                # Execute current step with appropriate agent\n                step_type = step_info.get(\"type\") if step_info else None\n                executor = self.get_executor(step_type)\n                step_result = await self._execute_step(executor, step_info)\n                result += step_result + \"\\n\"\n\n                # Check if agent wants to terminate\n                if hasattr(executor, \"state\") and executor.state == AgentState.FINISHED:\n                    break\n\n            return result\n        except Exception as e:\n            logger.error(f\"Error in PlanningFlow: {str(e)}\")\n            return f\"Execution failed: {str(e)}\"",
    "Execute the planning flow with agents."
  ],
  [
    "async def _create_initial_plan(self, request: str) -> None:\n        \n        logger.info(f\"Creating initial plan with ID: {self.active_plan_id}\")\n\n        system_message_content = (\n            \"You are a planning assistant. Create a concise, actionable plan with clear steps. \"\n            \"Focus on key milestones rather than detailed sub-steps. \"\n            \"Optimize for clarity and efficiency.\"\n        )\n        agents_description = []\n        for key in self.executor_keys:\n            if key in self.agents:\n                agents_description.append(\n                    {\n                        \"name\": key.upper(),\n                        \"description\": self.agents[key].description,\n                    }\n                )\n        if len(agents_description) > 1:\n            # Add description of agents to select\n            system_message_content += (\n                f\"\\nNow we have {agents_description} agents. \"\n                f\"The infomation of them are below: {json.dumps(agents_description)}\\n\"\n                \"When creating steps in the planning tool, please specify the agent names using the format '[agent_name]'.\"\n            )\n\n        # Create a system message for plan creation\n        system_message = Message.system_message(system_message_content)\n\n        # Create a user message with the request\n        user_message = Message.user_message(\n            f\"Create a reasonable plan with clear steps to accomplish the task: {request}\"\n        )\n\n        # Call LLM with PlanningTool\n        response = await self.llm.ask_tool(\n            messages=[user_message],\n            system_msgs=[system_message],\n            tools=[self.planning_tool.to_param()],\n            tool_choice=ToolChoice.AUTO,\n        )\n\n        # Process tool calls if present\n        if response.tool_calls:\n            for tool_call in response.tool_calls:\n                if tool_call.function.name == \"planning\":\n                    # Parse the arguments\n                    args = tool_call.function.arguments\n                    if isinstance(args, str):\n                        try:\n                            args = json.loads(args)\n                        except json.JSONDecodeError:\n                            logger.error(f\"Failed to parse tool arguments: {args}\")\n                            continue\n\n                    # Ensure plan_id is set correctly and execute the tool\n                    args[\"plan_id\"] = self.active_plan_id\n\n                    # Execute the tool via ToolCollection instead of directly\n                    result = await self.planning_tool.execute(**args)\n\n                    logger.info(f\"Plan creation result: {str(result)}\")\n                    return\n\n        # If execution reached here, create a default plan\n        logger.warning(\"Creating default plan\")\n\n        # Create default plan using the ToolCollection\n        await self.planning_tool.execute(\n            **{\n                \"command\": \"create\",\n                \"plan_id\": self.active_plan_id,\n                \"title\": f\"Plan for: {request[:50]}{'...' if len(request) > 50 else ''}\",\n                \"steps\": [\"Analyze request\", \"Execute task\", \"Verify results\"],\n            }\n        )",
    "Create an initial plan based on the request using the flow's LLM and PlanningTool."
  ],
  [
    "async def _get_current_step_info(self) -> tuple[Optional[int], Optional[dict]]:\n        \"\"\"\n        Parse the current plan to identify the first non-completed step's index and info.\n        Returns (None, None) if no active step is found.\n        \"\"\"\n        if (\n            not self.active_plan_id\n            or self.active_plan_id not in self.planning_tool.plans\n        ):\n            logger.error(f\"Plan with ID {self.active_plan_id} not found\")\n            return None, None\n\n        try:\n            # Direct access to plan data from planning tool storage\n            plan_data = self.planning_tool.plans[self.active_plan_id]\n            steps = plan_data.get(\"steps\", [])\n            step_statuses = plan_data.get(\"step_statuses\", [])\n\n            # Find first non-completed step\n            for i, step in enumerate(steps):\n                if i >= len(step_statuses):\n                    status = PlanStepStatus.NOT_STARTED.value\n                else:\n                    status = step_statuses[i]\n\n                if status in PlanStepStatus.get_active_statuses():\n                    # Extract step type/category if available\n                    step_info = {\"text\": step}\n\n                    # Try to extract step type from the text (e.g., [SEARCH] or [CODE])\n                    import re\n\n                    type_match = re.search(r\"\\[([A-Z_]+)\\]\", step)\n                    if type_match:\n                        step_info[\"type\"] = type_match.group(1).lower()\n\n                    # Mark current step as in_progress\n                    try:\n                        await self.planning_tool.execute(\n                            command=\"mark_step\",\n                            plan_id=self.active_plan_id,\n                            step_index=i,\n                            step_status=PlanStepStatus.IN_PROGRESS.value,\n                        )\n                    except Exception as e:\n                        logger.warning(f\"Error marking step as in_progress: {e}\")\n                        # Update step status directly if needed\n                        if i < len(step_statuses):\n                            step_statuses[i] = PlanStepStatus.IN_PROGRESS.value\n                        else:\n                            while len(step_statuses) < i:\n                                step_statuses.append(PlanStepStatus.NOT_STARTED.value)\n                            step_statuses.append(PlanStepStatus.IN_PROGRESS.value)\n\n                        plan_data[\"step_statuses\"] = step_statuses\n\n                    return i, step_info\n\n            return None, None  # No active step found\n\n        except Exception as e:\n            logger.warning(f\"Error finding current step index: {e}\")\n            return None, None",
    "Parse the current plan to identify the first non-completed step's index and info.\nReturns (None, None) if no active step is found."
  ],
  [
    "async def _execute_step(self, executor: BaseAgent, step_info: dict) -> str:\n        \n        # Prepare context for the agent with current plan status\n        plan_status = await self._get_plan_text()\n        step_text = step_info.get(\"text\", f\"Step {self.current_step_index}\")\n\n        # Create a prompt for the agent to execute the current step\n        step_prompt = f\"\"\"\n        CURRENT PLAN STATUS:\n        {plan_status}\n\n        YOUR CURRENT TASK:\n        You are now working on step {self.current_step_index}: \"{step_text}\"\n\n        Please only execute this current step using the appropriate tools. When you're done, provide a summary of what you accomplished.\n        \"\"\"\n\n        # Use agent.run() to execute the step\n        try:\n            step_result = await executor.run(step_prompt)\n\n            # Mark the step as completed after successful execution\n            await self._mark_step_completed()\n\n            return step_result\n        except Exception as e:\n            logger.error(f\"Error executing step {self.current_step_index}: {e}\")\n            return f\"Error executing step {self.current_step_index}: {str(e)}\"",
    "Execute the current step with the specified agent using agent.run()."
  ],
  [
    "async def _mark_step_completed(self) -> None:\n        \n        if self.current_step_index is None:\n            return\n\n        try:\n            # Mark the step as completed\n            await self.planning_tool.execute(\n                command=\"mark_step\",\n                plan_id=self.active_plan_id,\n                step_index=self.current_step_index,\n                step_status=PlanStepStatus.COMPLETED.value,\n            )\n            logger.info(\n                f\"Marked step {self.current_step_index} as completed in plan {self.active_plan_id}\"\n            )\n        except Exception as e:\n            logger.warning(f\"Failed to update plan status: {e}\")\n            # Update step status directly in planning tool storage\n            if self.active_plan_id in self.planning_tool.plans:\n                plan_data = self.planning_tool.plans[self.active_plan_id]\n                step_statuses = plan_data.get(\"step_statuses\", [])\n\n                # Ensure the step_statuses list is long enough\n                while len(step_statuses) <= self.current_step_index:\n                    step_statuses.append(PlanStepStatus.NOT_STARTED.value)\n\n                # Update the status\n                step_statuses[self.current_step_index] = PlanStepStatus.COMPLETED.value\n                plan_data[\"step_statuses\"] = step_statuses",
    "Mark the current step as completed."
  ],
  [
    "async def _get_plan_text(self) -> str:\n        \n        try:\n            result = await self.planning_tool.execute(\n                command=\"get\", plan_id=self.active_plan_id\n            )\n            return result.output if hasattr(result, \"output\") else str(result)\n        except Exception as e:\n            logger.error(f\"Error getting plan: {e}\")\n            return self._generate_plan_text_from_storage()",
    "Get the current plan as formatted text."
  ],
  [
    "def _generate_plan_text_from_storage(self) -> str:\n        \n        try:\n            if self.active_plan_id not in self.planning_tool.plans:\n                return f\"Error: Plan with ID {self.active_plan_id} not found\"\n\n            plan_data = self.planning_tool.plans[self.active_plan_id]\n            title = plan_data.get(\"title\", \"Untitled Plan\")\n            steps = plan_data.get(\"steps\", [])\n            step_statuses = plan_data.get(\"step_statuses\", [])\n            step_notes = plan_data.get(\"step_notes\", [])\n\n            # Ensure step_statuses and step_notes match the number of steps\n            while len(step_statuses) < len(steps):\n                step_statuses.append(PlanStepStatus.NOT_STARTED.value)\n            while len(step_notes) < len(steps):\n                step_notes.append(\"\")\n\n            # Count steps by status\n            status_counts = {status: 0 for status in PlanStepStatus.get_all_statuses()}\n\n            for status in step_statuses:\n                if status in status_counts:\n                    status_counts[status] += 1\n\n            completed = status_counts[PlanStepStatus.COMPLETED.value]\n            total = len(steps)\n            progress = (completed / total) * 100 if total > 0 else 0\n\n            plan_text = f\"Plan: {title} (ID: {self.active_plan_id})\\n\"\n            plan_text += \"=\" * len(plan_text) + \"\\n\\n\"\n\n            plan_text += (\n                f\"Progress: {completed}/{total} steps completed ({progress:.1f}%)\\n\"\n            )\n            plan_text += f\"Status: {status_counts[PlanStepStatus.COMPLETED.value]} completed, {status_counts[PlanStepStatus.IN_PROGRESS.value]} in progress, \"\n            plan_text += f\"{status_counts[PlanStepStatus.BLOCKED.value]} blocked, {status_counts[PlanStepStatus.NOT_STARTED.value]} not started\\n\\n\"\n            plan_text += \"Steps:\\n\"\n\n            status_marks = PlanStepStatus.get_status_marks()\n\n            for i, (step, status, notes) in enumerate(\n                zip(steps, step_statuses, step_notes)\n            ):\n                # Use status marks to indicate step status\n                status_mark = status_marks.get(\n                    status, status_marks[PlanStepStatus.NOT_STARTED.value]\n                )\n\n                plan_text += f\"{i}. {status_mark} {step}\\n\"\n                if notes:\n                    plan_text += f\"   Notes: {notes}\\n\"\n\n            return plan_text\n        except Exception as e:\n            logger.error(f\"Error generating plan text from storage: {e}\")\n            return f\"Error: Unable to retrieve plan with ID {self.active_plan_id}\"",
    "Generate plan text directly from storage if the planning tool fails."
  ],
  [
    "async def _finalize_plan(self) -> str:\n        \n        plan_text = await self._get_plan_text()\n\n        # Create a summary using the flow's LLM directly\n        try:\n            system_message = Message.system_message(\n                \"You are a planning assistant. Your task is to summarize the completed plan.\"\n            )\n\n            user_message = Message.user_message(\n                f\"The plan has been completed. Here is the final plan status:\\n\\n{plan_text}\\n\\nPlease provide a summary of what was accomplished and any final thoughts.\"\n            )\n\n            response = await self.llm.ask(\n                messages=[user_message], system_msgs=[system_message]\n            )\n\n            return f\"Plan completed:\\n\\n{response}\"\n        except Exception as e:\n            logger.error(f\"Error finalizing plan with LLM: {e}\")\n\n            # Fallback to using an agent for the summary\n            try:\n                agent = self.primary_agent\n                summary_prompt = f\"\"\"\n                The plan has been completed. Here is the final plan status:\n\n                {plan_text}\n\n                Please provide a summary of what was accomplished and any final thoughts.\n                \"\"\"\n                summary = await agent.run(summary_prompt)\n                return f\"Plan completed:\\n\\n{summary}\"\n            except Exception as e2:\n                logger.error(f\"Error finalizing plan with agent: {e2}\")\n                return \"Plan completed. Error generating summary.\"",
    "Finalize the plan and provide a summary using the flow's LLM directly."
  ],
  [
    "def count_text(self, text: str) -> int:\n        \n        return 0 if not text else len(self.tokenizer.encode(text))",
    "Calculate tokens for a text string"
  ],
  [
    "def count_image(self, image_item: dict) -> int:\n        \"\"\"\n        Calculate tokens for an image based on detail level and dimensions\n\n        For \"low\" detail: fixed 85 tokens\n        For \"high\" detail:\n        1. Scale to fit in 2048x2048 square\n        2. Scale shortest side to 768px\n        3. Count 512px tiles (170 tokens each)\n        4. Add 85 tokens\n        \"\"\"\n        detail = image_item.get(\"detail\", \"medium\")\n\n        # For low detail, always return fixed token count\n        if detail == \"low\":\n            return self.LOW_DETAIL_IMAGE_TOKENS\n\n        # For medium detail (default in OpenAI), use high detail calculation\n        # OpenAI doesn't specify a separate calculation for medium\n\n        # For high detail, calculate based on dimensions if available\n        if detail == \"high\" or detail == \"medium\":\n            # If dimensions are provided in the image_item\n            if \"dimensions\" in image_item:\n                width, height = image_item[\"dimensions\"]\n                return self._calculate_high_detail_tokens(width, height)\n\n        return (\n            self._calculate_high_detail_tokens(1024, 1024) if detail == \"high\" else 1024\n        )",
    "Calculate tokens for an image based on detail level and dimensions\n\nFor \"low\" detail: fixed 85 tokens\nFor \"high\" detail:\n1. Scale to fit in 2048x2048 square\n2. Scale shortest side to 768px\n3. Count 512px tiles (170 tokens each)\n4. Add 85 tokens"
  ],
  [
    "def _calculate_high_detail_tokens(self, width: int, height: int) -> int:\n        \n        # Step 1: Scale to fit in MAX_SIZE x MAX_SIZE square\n        if width > self.MAX_SIZE or height > self.MAX_SIZE:\n            scale = self.MAX_SIZE / max(width, height)\n            width = int(width * scale)\n            height = int(height * scale)\n\n        # Step 2: Scale so shortest side is HIGH_DETAIL_TARGET_SHORT_SIDE\n        scale = self.HIGH_DETAIL_TARGET_SHORT_SIDE / min(width, height)\n        scaled_width = int(width * scale)\n        scaled_height = int(height * scale)\n\n        # Step 3: Count number of 512px tiles\n        tiles_x = math.ceil(scaled_width / self.TILE_SIZE)\n        tiles_y = math.ceil(scaled_height / self.TILE_SIZE)\n        total_tiles = tiles_x * tiles_y\n\n        # Step 4: Calculate final token count\n        return (\n            total_tiles * self.HIGH_DETAIL_TILE_TOKENS\n        ) + self.LOW_DETAIL_IMAGE_TOKENS",
    "Calculate tokens for high detail images based on dimensions"
  ],
  [
    "def count_message_tokens(self, messages: List[dict]) -> int:\n        \n        total_tokens = self.FORMAT_TOKENS  # Base format tokens\n\n        for message in messages:\n            tokens = self.BASE_MESSAGE_TOKENS  # Base tokens per message\n\n            # Add role tokens\n            tokens += self.count_text(message.get(\"role\", \"\"))\n\n            # Add content tokens\n            if \"content\" in message:\n                tokens += self.count_content(message[\"content\"])\n\n            # Add tool calls tokens\n            if \"tool_calls\" in message:\n                tokens += self.count_tool_calls(message[\"tool_calls\"])\n\n            # Add name and tool_call_id tokens\n            tokens += self.count_text(message.get(\"name\", \"\"))\n            tokens += self.count_text(message.get(\"tool_call_id\", \"\"))\n\n            total_tokens += tokens\n\n        return total_tokens",
    "Calculate the total number of tokens in a message list"
  ],
  [
    "def count_tokens(self, text: str) -> int:\n        \n        if not text:\n            return 0\n        return len(self.tokenizer.encode(text))",
    "Calculate the number of tokens in a text"
  ],
  [
    "def check_token_limit(self, input_tokens: int) -> bool:\n        \n        if self.max_input_tokens is not None:\n            return (self.total_input_tokens + input_tokens) <= self.max_input_tokens\n        # If max_input_tokens is not set, always return True\n        return True",
    "Check if token limits are exceeded"
  ],
  [
    "def get_limit_error_message(self, input_tokens: int) -> str:\n        \n        if (\n            self.max_input_tokens is not None\n            and (self.total_input_tokens + input_tokens) > self.max_input_tokens\n        ):\n            return f\"Request may exceed input token limit (Current: {self.total_input_tokens}, Needed: {input_tokens}, Max: {self.max_input_tokens})\"\n\n        return \"Token limit exceeded\"",
    "Generate error message for token limit exceeded"
  ],
  [
    "def format_messages(\n        messages: List[Union[dict, Message]], supports_images: bool = False\n    ) -> List[dict]:\n        \"\"\"\n        Format messages for LLM by converting them to OpenAI message format.\n\n        Args:\n            messages: List of messages that can be either dict or Message objects\n            supports_images: Flag indicating if the target model supports image inputs\n\n        Returns:\n            List[dict]: List of formatted messages in OpenAI format\n\n        Raises:\n            ValueError: If messages are invalid or missing required fields\n            TypeError: If unsupported message types are provided\n\n        Examples:\n            >>> msgs = [\n            ...     Message.system_message(\"You are a helpful assistant\"),\n            ...     {\"role\": \"user\", \"content\": \"Hello\"},\n            ...     Message.user_message(\"How are you?\")\n            ... ]\n            >>> formatted = LLM.format_messages(msgs)\n        \"\"\"\n        formatted_messages = []\n\n        for message in messages:\n            # Convert Message objects to dictionaries\n            if isinstance(message, Message):\n                message = message.to_dict()\n\n            if isinstance(message, dict):\n                # If message is a dict, ensure it has required fields\n                if \"role\" not in message:\n                    raise ValueError(\"Message dict must contain 'role' field\")\n\n                # Process base64 images if present and model supports images\n                if supports_images and message.get(\"base64_image\"):\n                    # Initialize or convert content to appropriate format\n                    if not message.get(\"content\"):\n                        message[\"content\"] = []\n                    elif isinstance(message[\"content\"], str):\n                        message[\"content\"] = [\n                            {\"type\": \"text\", \"text\": message[\"content\"]}\n                        ]\n                    elif isinstance(message[\"content\"], list):\n                        # Convert string items to proper text objects\n                        message[\"content\"] = [\n                            (\n                                {\"type\": \"text\", \"text\": item}\n                                if isinstance(item, str)\n                                else item\n                            )\n                            for item in message[\"content\"]\n                        ]\n\n                    # Add the image to content\n                    message[\"content\"].append(\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/jpeg;base64,{message['base64_image']}\"\n                            },\n                        }\n                    )\n\n                    # Remove the base64_image field\n                    del message[\"base64_image\"]\n                # If model doesn't support images but message has base64_image, handle gracefully\n                elif not supports_images and message.get(\"base64_image\"):\n                    # Just remove the base64_image field and keep the text content\n                    del message[\"base64_image\"]\n\n                if \"content\" in message or \"tool_calls\" in message:\n                    formatted_messages.append(message)\n                # else: do not include the message\n            else:\n                raise TypeError(f\"Unsupported message type: {type(message)}\")\n\n        # Validate all messages have required fields\n        for msg in formatted_messages:\n            if msg[\"role\"] not in ROLE_VALUES:\n                raise ValueError(f\"Invalid role: {msg['role']}\")\n\n        return formatted_messages",
    "Format messages for LLM by converting them to OpenAI message format.\n\nArgs:\n    messages: List of messages that can be either dict or Message objects\n    supports_images: Flag indicating if the target model supports image inputs\n\nReturns:\n    List[dict]: List of formatted messages in OpenAI format\n\nRaises:\n    ValueError: If messages are invalid or missing required fields\n    TypeError: If unsupported message types are provided\n\nExamples:\n    >>> msgs = [\n    ...     Message.system_message(\"You are a helpful assistant\"),\n    ...     {\"role\": \"user\", \"content\": \"Hello\"},\n    ...     Message.user_message(\"How are you?\")\n    ... ]\n    >>> formatted = LLM.format_messages(msgs)"
  ],
  [
    "async def ask(\n        self,\n        messages: List[Union[dict, Message]],\n        system_msgs: Optional[List[Union[dict, Message]]] = None,\n        stream: bool = True,\n        temperature: Optional[float] = None,\n    ) -> str:\n        \"\"\"\n        Send a prompt to the LLM and get the response.\n\n        Args:\n            messages: List of conversation messages\n            system_msgs: Optional system messages to prepend\n            stream (bool): Whether to stream the response\n            temperature (float): Sampling temperature for the response\n\n        Returns:\n            str: The generated response\n\n        Raises:\n            TokenLimitExceeded: If token limits are exceeded\n            ValueError: If messages are invalid or response is empty\n            OpenAIError: If API call fails after retries\n            Exception: For unexpected errors\n        \"\"\"\n        try:\n            # Check if the model supports images\n            supports_images = self.model in MULTIMODAL_MODELS\n\n            # Format system and user messages with image support check\n            if system_msgs:\n                system_msgs = self.format_messages(system_msgs, supports_images)\n                messages = system_msgs + self.format_messages(messages, supports_images)\n            else:\n                messages = self.format_messages(messages, supports_images)\n\n            # Calculate input token count\n            input_tokens = self.count_message_tokens(messages)\n\n            # Check if token limits are exceeded\n            if not self.check_token_limit(input_tokens):\n                error_message = self.get_limit_error_message(input_tokens)\n                # Raise a special exception that won't be retried\n                raise TokenLimitExceeded(error_message)\n\n            params = {\n                \"model\": self.model,\n                \"messages\": messages,\n            }\n\n            if self.model in REASONING_MODELS:\n                params[\"max_completion_tokens\"] = self.max_tokens\n            else:\n                params[\"max_tokens\"] = self.max_tokens\n                params[\"temperature\"] = (\n                    temperature if temperature is not None else self.temperature\n                )\n\n            if not stream:\n                # Non-streaming request\n                response = await self.client.chat.completions.create(\n                    **params, stream=False\n                )\n\n                if not response.choices or not response.choices[0].message.content:\n                    raise ValueError(\"Empty or invalid response from LLM\")\n\n                # Update token counts\n                self.update_token_count(\n                    response.usage.prompt_tokens, response.usage.completion_tokens\n                )\n\n                return response.choices[0].message.content\n\n            # Streaming request, For streaming, update estimated token count before making the request\n            self.update_token_count(input_tokens)\n\n            response = await self.client.chat.completions.create(**params, stream=True)\n\n            collected_messages = []\n            completion_text = \"\"\n            async for chunk in response:\n                chunk_message = chunk.choices[0].delta.content or \"\"\n                collected_messages.append(chunk_message)\n                completion_text += chunk_message\n                print(chunk_message, end=\"\", flush=True)\n\n            print()  # Newline after streaming\n            full_response = \"\".join(collected_messages).strip()\n            if not full_response:\n                raise ValueError(\"Empty response from streaming LLM\")\n\n            # estimate completion tokens for streaming response\n            completion_tokens = self.count_tokens(completion_text)\n            logger.info(\n                f\"Estimated completion tokens for streaming response: {completion_tokens}\"\n            )\n            self.total_completion_tokens += completion_tokens\n\n            return full_response\n\n        except TokenLimitExceeded:\n            # Re-raise token limit errors without logging\n            raise\n        except ValueError:\n            logger.exception(f\"Validation error\")\n            raise\n        except OpenAIError as oe:\n            logger.exception(f\"OpenAI API error\")\n            if isinstance(oe, AuthenticationError):\n                logger.error(\"Authentication failed. Check API key.\")\n            elif isinstance(oe, RateLimitError):\n                logger.error(\"Rate limit exceeded. Consider increasing retry attempts.\")\n            elif isinstance(oe, APIError):\n                logger.error(f\"API error: {oe}\")\n            raise\n        except Exception:\n            logger.exception(f\"Unexpected error in ask\")\n            raise",
    "Send a prompt to the LLM and get the response.\n\nArgs:\n    messages: List of conversation messages\n    system_msgs: Optional system messages to prepend\n    stream (bool): Whether to stream the response\n    temperature (float): Sampling temperature for the response\n\nReturns:\n    str: The generated response\n\nRaises:\n    TokenLimitExceeded: If token limits are exceeded\n    ValueError: If messages are invalid or response is empty\n    OpenAIError: If API call fails after retries\n    Exception: For unexpected errors"
  ],
  [
    "async def ask_with_images(\n        self,\n        messages: List[Union[dict, Message]],\n        images: List[Union[str, dict]],\n        system_msgs: Optional[List[Union[dict, Message]]] = None,\n        stream: bool = False,\n        temperature: Optional[float] = None,\n    ) -> str:\n        \"\"\"\n        Send a prompt with images to the LLM and get the response.\n\n        Args:\n            messages: List of conversation messages\n            images: List of image URLs or image data dictionaries\n            system_msgs: Optional system messages to prepend\n            stream (bool): Whether to stream the response\n            temperature (float): Sampling temperature for the response\n\n        Returns:\n            str: The generated response\n\n        Raises:\n            TokenLimitExceeded: If token limits are exceeded\n            ValueError: If messages are invalid or response is empty\n            OpenAIError: If API call fails after retries\n            Exception: For unexpected errors\n        \"\"\"\n        try:\n            # For ask_with_images, we always set supports_images to True because\n            # this method should only be called with models that support images\n            if self.model not in MULTIMODAL_MODELS:\n                raise ValueError(\n                    f\"Model {self.model} does not support images. Use a model from {MULTIMODAL_MODELS}\"\n                )\n\n            # Format messages with image support\n            formatted_messages = self.format_messages(messages, supports_images=True)\n\n            # Ensure the last message is from the user to attach images\n            if not formatted_messages or formatted_messages[-1][\"role\"] != \"user\":\n                raise ValueError(\n                    \"The last message must be from the user to attach images\"\n                )\n\n            # Process the last user message to include images\n            last_message = formatted_messages[-1]\n\n            # Convert content to multimodal format if needed\n            content = last_message[\"content\"]\n            multimodal_content = (\n                [{\"type\": \"text\", \"text\": content}]\n                if isinstance(content, str)\n                else content\n                if isinstance(content, list)\n                else []\n            )\n\n            # Add images to content\n            for image in images:\n                if isinstance(image, str):\n                    multimodal_content.append(\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": image}}\n                    )\n                elif isinstance(image, dict) and \"url\" in image:\n                    multimodal_content.append({\"type\": \"image_url\", \"image_url\": image})\n                elif isinstance(image, dict) and \"image_url\" in image:\n                    multimodal_content.append(image)\n                else:\n                    raise ValueError(f\"Unsupported image format: {image}\")\n\n            # Update the message with multimodal content\n            last_message[\"content\"] = multimodal_content\n\n            # Add system messages if provided\n            if system_msgs:\n                all_messages = (\n                    self.format_messages(system_msgs, supports_images=True)\n                    + formatted_messages\n                )\n            else:\n                all_messages = formatted_messages\n\n            # Calculate tokens and check limits\n            input_tokens = self.count_message_tokens(all_messages)\n            if not self.check_token_limit(input_tokens):\n                raise TokenLimitExceeded(self.get_limit_error_message(input_tokens))\n\n            # Set up API parameters\n            params = {\n                \"model\": self.model,\n                \"messages\": all_messages,\n                \"stream\": stream,\n            }\n\n            # Add model-specific parameters\n            if self.model in REASONING_MODELS:\n                params[\"max_completion_tokens\"] = self.max_tokens\n            else:\n                params[\"max_tokens\"] = self.max_tokens\n                params[\"temperature\"] = (\n                    temperature if temperature is not None else self.temperature\n                )\n\n            # Handle non-streaming request\n            if not stream:\n                response = await self.client.chat.completions.create(**params)\n\n                if not response.choices or not response.choices[0].message.content:\n                    raise ValueError(\"Empty or invalid response from LLM\")\n\n                self.update_token_count(response.usage.prompt_tokens)\n                return response.choices[0].message.content\n\n            # Handle streaming request\n            self.update_token_count(input_tokens)\n            response = await self.client.chat.completions.create(**params)\n\n            collected_messages = []\n            async for chunk in response:\n                chunk_message = chunk.choices[0].delta.content or \"\"\n                collected_messages.append(chunk_message)\n                print(chunk_message, end=\"\", flush=True)\n\n            print()  # Newline after streaming\n            full_response = \"\".join(collected_messages).strip()\n\n            if not full_response:\n                raise ValueError(\"Empty response from streaming LLM\")\n\n            return full_response\n\n        except TokenLimitExceeded:\n            raise\n        except ValueError as ve:\n            logger.error(f\"Validation error in ask_with_images: {ve}\")\n            raise\n        except OpenAIError as oe:\n            logger.error(f\"OpenAI API error: {oe}\")\n            if isinstance(oe, AuthenticationError):\n                logger.error(\"Authentication failed. Check API key.\")\n            elif isinstance(oe, RateLimitError):\n                logger.error(\"Rate limit exceeded. Consider increasing retry attempts.\")\n            elif isinstance(oe, APIError):\n                logger.error(f\"API error: {oe}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error in ask_with_images: {e}\")\n            raise",
    "Send a prompt with images to the LLM and get the response.\n\nArgs:\n    messages: List of conversation messages\n    images: List of image URLs or image data dictionaries\n    system_msgs: Optional system messages to prepend\n    stream (bool): Whether to stream the response\n    temperature (float): Sampling temperature for the response\n\nReturns:\n    str: The generated response\n\nRaises:\n    TokenLimitExceeded: If token limits are exceeded\n    ValueError: If messages are invalid or response is empty\n    OpenAIError: If API call fails after retries\n    Exception: For unexpected errors"
  ],
  [
    "async def ask_tool(\n        self,\n        messages: List[Union[dict, Message]],\n        system_msgs: Optional[List[Union[dict, Message]]] = None,\n        timeout: int = 300,\n        tools: Optional[List[dict]] = None,\n        tool_choice: TOOL_CHOICE_TYPE = ToolChoice.AUTO,  # type: ignore\n        temperature: Optional[float] = None,\n        **kwargs,\n    ) -> ChatCompletionMessage | None:\n        \"\"\"\n        Ask LLM using functions/tools and return the response.\n\n        Args:\n            messages: List of conversation messages\n            system_msgs: Optional system messages to prepend\n            timeout: Request timeout in seconds\n            tools: List of tools to use\n            tool_choice: Tool choice strategy\n            temperature: Sampling temperature for the response\n            **kwargs: Additional completion arguments\n\n        Returns:\n            ChatCompletionMessage: The model's response\n\n        Raises:\n            TokenLimitExceeded: If token limits are exceeded\n            ValueError: If tools, tool_choice, or messages are invalid\n            OpenAIError: If API call fails after retries\n            Exception: For unexpected errors\n        \"\"\"\n        try:\n            # Validate tool_choice\n            if tool_choice not in TOOL_CHOICE_VALUES:\n                raise ValueError(f\"Invalid tool_choice: {tool_choice}\")\n\n            # Check if the model supports images\n            supports_images = self.model in MULTIMODAL_MODELS\n\n            # Format messages\n            if system_msgs:\n                system_msgs = self.format_messages(system_msgs, supports_images)\n                messages = system_msgs + self.format_messages(messages, supports_images)\n            else:\n                messages = self.format_messages(messages, supports_images)\n\n            # Calculate input token count\n            input_tokens = self.count_message_tokens(messages)\n\n            # If there are tools, calculate token count for tool descriptions\n            tools_tokens = 0\n            if tools:\n                for tool in tools:\n                    tools_tokens += self.count_tokens(str(tool))\n\n            input_tokens += tools_tokens\n\n            # Check if token limits are exceeded\n            if not self.check_token_limit(input_tokens):\n                error_message = self.get_limit_error_message(input_tokens)\n                # Raise a special exception that won't be retried\n                raise TokenLimitExceeded(error_message)\n\n            # Validate tools if provided\n            if tools:\n                for tool in tools:\n                    if not isinstance(tool, dict) or \"type\" not in tool:\n                        raise ValueError(\"Each tool must be a dict with 'type' field\")\n\n            # Set up the completion request\n            params = {\n                \"model\": self.model,\n                \"messages\": messages,\n                \"tools\": tools,\n                \"tool_choice\": tool_choice,\n                \"timeout\": timeout,\n                **kwargs,\n            }\n\n            if self.model in REASONING_MODELS:\n                params[\"max_completion_tokens\"] = self.max_tokens\n            else:\n                params[\"max_tokens\"] = self.max_tokens\n                params[\"temperature\"] = (\n                    temperature if temperature is not None else self.temperature\n                )\n\n            params[\"stream\"] = False  # Always use non-streaming for tool requests\n            response: ChatCompletion = await self.client.chat.completions.create(\n                **params\n            )\n\n            # Check if response is valid\n            if not response.choices or not response.choices[0].message:\n                print(response)\n                # raise ValueError(\"Invalid or empty response from LLM\")\n                return None\n\n            # Update token counts\n            self.update_token_count(\n                response.usage.prompt_tokens, response.usage.completion_tokens\n            )\n\n            return response.choices[0].message\n\n        except TokenLimitExceeded:\n            # Re-raise token limit errors without logging\n            raise\n        except ValueError as ve:\n            logger.error(f\"Validation error in ask_tool: {ve}\")\n            raise\n        except OpenAIError as oe:\n            logger.error(f\"OpenAI API error: {oe}\")\n            if isinstance(oe, AuthenticationError):\n                logger.error(\"Authentication failed. Check API key.\")\n            elif isinstance(oe, RateLimitError):\n                logger.error(\"Rate limit exceeded. Consider increasing retry attempts.\")\n            elif isinstance(oe, APIError):\n                logger.error(f\"API error: {oe}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error in ask_tool: {e}\")\n            raise",
    "Ask LLM using functions/tools and return the response.\n\nArgs:\n    messages: List of conversation messages\n    system_msgs: Optional system messages to prepend\n    timeout: Request timeout in seconds\n    tools: List of tools to use\n    tool_choice: Tool choice strategy\n    temperature: Sampling temperature for the response\n    **kwargs: Additional completion arguments\n\nReturns:\n    ChatCompletionMessage: The model's response\n\nRaises:\n    TokenLimitExceeded: If token limits are exceeded\n    ValueError: If tools, tool_choice, or messages are invalid\n    OpenAIError: If API call fails after retries\n    Exception: For unexpected errors"
  ],
  [
    "def define_log_level(print_level=\"INFO\", logfile_level=\"DEBUG\", name: str = None):\n    \n    global _print_level\n    _print_level = print_level\n\n    current_date = datetime.now()\n    formatted_date = current_date.strftime(\"%Y%m%d%H%M%S\")\n    log_name = (\n        f\"{name}_{formatted_date}\" if name else formatted_date\n    )  # name a log with prefix name\n\n    _logger.remove()\n    _logger.add(sys.stderr, level=print_level)\n    _logger.add(PROJECT_ROOT / f\"logs/{log_name}.log\", level=logfile_level)\n    return _logger",
    "Adjust the log level to above level"
  ],
  [
    "def load_and_compute_8bit_ppl(result_queue, load_in_4bit=False, load_in_8bit=False):\n    \n    from unsloth import FastLanguageModel\n    from unsloth.chat_templates import get_chat_template\n    from tests.utils.perplexity_eval import ppl_model\n\n    # Load model\n    merged_model, merged_tokenizer = FastLanguageModel.from_pretrained(\n        model_name=\"./unsloth_out/merged_llama_text_model\",\n        max_seq_length=2048,\n        load_in_4bit=load_in_4bit,\n        load_in_8bit=load_in_8bit,\n    )\n    # Set up tokenizer\n    merged_tokenizer = get_chat_template(\n        merged_tokenizer,\n        chat_template=\"llama-3.1\",\n    )\n\n    # Load dataset fresh in subprocess\n    dataset_ppl = load_dataset(\"allenai/openassistant-guanaco-reformatted\", split=\"eval\")\n\n    # Format the dataset\n    def formatting_prompts_func(examples):\n        convos = examples[\"messages\"]\n        texts = [merged_tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n        return {\"text\": texts}\n\n    dataset_ppl = dataset_ppl.map(formatting_prompts_func, batched=True)\n\n    # Compute perplexity using the passed dataset\n    ppl_value = ppl_model(merged_model, merged_tokenizer, dataset_ppl)\n\n\n    # IMPORTANT: Convert to Python float if it's a tensor\n    if torch.is_tensor(ppl_value):\n        ppl_value = ppl_value.cpu().item()  # Move to CPU and convert to Python scalar\n    elif hasattr(ppl_value, 'item'):\n        ppl_value = ppl_value.item()  # Convert numpy or other array types\n    else:\n        ppl_value = float(ppl_value)  # Ensure it's a float\n\n    # Return only the perplexity value\n    result_queue.put(ppl_value)\n\n    # Clean up\n    del merged_model\n    del merged_tokenizer\n    del dataset_ppl\n    torch.cuda.empty_cache()\n    gc.collect()",
    "Load model and compute perplexity in subprocess"
  ],
  [
    "def load_and_compute_8bit_ppl(result_queue, load_in_4bit=False, load_in_8bit=False):\n    \n    from unsloth import FastLanguageModel\n    from tests.utils.perplexity_eval import ppl_model\n\n    # Load model\n    merged_model, merged_tokenizer = FastLanguageModel.from_pretrained(\n        model_name=\"./unsloth_out/merged_mistral_text_model\",\n        max_seq_length=2048,\n        load_in_4bit=load_in_4bit,\n        load_in_8bit=load_in_8bit,\n    )\n    # Set up tokenizer\n    # merged_tokenizer = get_chat_template(\n    #     merged_tokenizer,\n    #     chat_template=\"llama-3.1\",\n    # )\n\n    # Load dataset fresh in subprocess\n    dataset_ppl = load_dataset(\"allenai/openassistant-guanaco-reformatted\", split=\"eval\")\n\n    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n    ### Instruction:\n    {}\n\n    ### Input:\n    {}\n\n    ### Response:\n    {}\"\"\"\n\n    EOS_TOKEN = merged_tokenizer.eos_token\n\n    def formatting_prompts_func(examples):\n        instructions = []\n        inputs = []\n        outputs = []\n        texts = []\n\n        for conversation in examples[\"messages\"]:\n            # Extract user message and assistant response\n            user_message = \"\"\n            assistant_message = \"\"\n\n            for turn in conversation:\n                if turn[\"role\"] == \"user\":\n                    user_message = turn[\"content\"]\n                elif turn[\"role\"] == \"assistant\":\n                    assistant_message = turn[\"content\"]\n\n            # Store intermediate format\n            instruction = \"Complete the statement\"\n            instructions.append(instruction)\n            inputs.append(user_message)\n            outputs.append(assistant_message)\n\n            # Create formatted text\n            text = alpaca_prompt.format(instruction, user_message, assistant_message) + EOS_TOKEN\n            texts.append(text)\n\n        return {\n            \"instruction\": instructions,\n            \"input\": inputs,\n            \"output\": outputs,\n            \"text\": texts\n        }\n\n\n\n    dataset_ppl = dataset_ppl.map(formatting_prompts_func, batched=True)\n\n    # Compute perplexity using the passed dataset\n    ppl_value = ppl_model(merged_model, merged_tokenizer, dataset_ppl)\n\n\n    # IMPORTANT: Convert to Python float if it's a tensor\n    if torch.is_tensor(ppl_value):\n        ppl_value = ppl_value.cpu().item()  # Move to CPU and convert to Python scalar\n    elif hasattr(ppl_value, 'item'):\n        ppl_value = ppl_value.item()  # Convert numpy or other array types\n    else:\n        ppl_value = float(ppl_value)  # Ensure it's a float\n\n    # Return only the perplexity value\n    result_queue.put(ppl_value)\n\n    # Clean up\n    del merged_model\n    del merged_tokenizer\n    del dataset_ppl\n    torch.cuda.empty_cache()\n    gc.collect()",
    "Load model and compute perplexity in subprocess"
  ],
  [
    "def load_and_compute_8bit_ppl(result_queue, load_in_4bit=False, load_in_8bit=False):\n    \n    from unsloth import FastLanguageModel\n    from unsloth.chat_templates import get_chat_template\n    from tests.utils.perplexity_eval import ppl_model\n\n    # Load model\n    merged_model, merged_tokenizer = FastLanguageModel.from_pretrained(\n        model_name=\"./unsloth_out/merged_phi4_text_model\",\n        max_seq_length=2048,\n        load_in_4bit=load_in_4bit,\n        load_in_8bit=load_in_8bit,\n    )\n    # Set up tokenizer\n    merged_tokenizer = get_chat_template(\n        merged_tokenizer,\n        chat_template=\"phi-4\",\n    )\n\n    # Load dataset fresh in subprocess\n    dataset_ppl = load_dataset(\"allenai/openassistant-guanaco-reformatted\", split=\"eval\")\n\n    # Format the dataset\n    def formatting_prompts_func(examples):\n        convos = examples[\"messages\"]\n        texts = [merged_tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n        return {\"text\": texts}\n\n    dataset_ppl = dataset_ppl.map(formatting_prompts_func, batched=True)\n\n    # Compute perplexity using the passed dataset\n    ppl_value = ppl_model(merged_model, merged_tokenizer, dataset_ppl)\n\n\n    # IMPORTANT: Convert to Python float if it's a tensor\n    if torch.is_tensor(ppl_value):\n        ppl_value = ppl_value.cpu().item()  # Move to CPU and convert to Python scalar\n    elif hasattr(ppl_value, 'item'):\n        ppl_value = ppl_value.item()  # Convert numpy or other array types\n    else:\n        ppl_value = float(ppl_value)  # Ensure it's a float\n\n    # Return only the perplexity value\n    result_queue.put(ppl_value)\n\n    # Clean up\n    del merged_model\n    del merged_tokenizer\n    del dataset_ppl\n    torch.cuda.empty_cache()\n    gc.collect()",
    "Load model and compute perplexity in subprocess"
  ],
  [
    "def load_and_compute_8bit_ppl(result_queue, load_in_4bit=False, load_in_8bit=False):\n    \n    from unsloth import FastLanguageModel\n    from unsloth.chat_templates import get_chat_template\n    from tests.utils.perplexity_eval import ppl_model\n\n    # Load model\n    merged_model, merged_tokenizer = FastLanguageModel.from_pretrained(\n        model_name=\"./unsloth_out/merged_llama_text_model\",\n        max_seq_length=2048,\n        load_in_4bit=load_in_4bit,\n        load_in_8bit=load_in_8bit,\n    )\n    # Set up tokenizer\n    merged_tokenizer = get_chat_template(\n        merged_tokenizer,\n        chat_template=\"llama-3.1\",\n    )\n\n    # Load dataset fresh in subprocess\n    dataset_ppl = load_dataset(\"allenai/openassistant-guanaco-reformatted\", split=\"eval\")\n\n    # Format the dataset\n    def formatting_prompts_func(examples):\n        convos = examples[\"messages\"]\n        texts = [merged_tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n        return {\"text\": texts}\n\n    dataset_ppl = dataset_ppl.map(formatting_prompts_func, batched=True)\n\n    # Compute perplexity using the passed dataset\n    ppl_value = ppl_model(merged_model, merged_tokenizer, dataset_ppl)\n\n\n    # IMPORTANT: Convert to Python float if it's a tensor\n    if torch.is_tensor(ppl_value):\n        ppl_value = ppl_value.cpu().item()  # Move to CPU and convert to Python scalar\n    elif hasattr(ppl_value, 'item'):\n        ppl_value = ppl_value.item()  # Convert numpy or other array types\n    else:\n        ppl_value = float(ppl_value)  # Ensure it's a float\n\n    # Return only the perplexity value\n    result_queue.put(ppl_value)\n\n    # Clean up\n    del merged_model\n    del merged_tokenizer\n    del dataset_ppl\n    torch.cuda.empty_cache()\n    gc.collect()",
    "Load model and compute perplexity in subprocess"
  ],
  [
    "def load_and_compute_8bit_ppl(result_queue, load_in_4bit=False, load_in_8bit=False):\n    \n    from unsloth import FastLanguageModel\n    from tests.utils.perplexity_eval import ppl_model\n\n    # Load model\n    merged_model, merged_tokenizer = FastLanguageModel.from_pretrained(\n        model_name=\"./unsloth_out/merged_qwen_text_model\",\n        max_seq_length=2048,\n        load_in_4bit=load_in_4bit,\n        load_in_8bit=load_in_8bit,\n    )\n    # Set up tokenizer\n    # merged_tokenizer = get_chat_template(\n    #     merged_tokenizer,\n    #     chat_template=\"llama-3.1\",\n    # )\n\n    # Load dataset fresh in subprocess\n    dataset_ppl = load_dataset(\"allenai/openassistant-guanaco-reformatted\", split=\"eval\")\n\n    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n    ### Instruction:\n    {}\n\n    ### Input:\n    {}\n\n    ### Response:\n    {}\"\"\"\n\n    def formatting_prompts_func(examples):\n        instructions = []\n        inputs = []\n        outputs = []\n        texts = []\n\n        for conversation in examples[\"messages\"]:\n            # Extract user message and assistant response\n            user_message = \"\"\n            assistant_message = \"\"\n\n            for turn in conversation:\n                if turn[\"role\"] == \"user\":\n                    user_message = turn[\"content\"]\n                elif turn[\"role\"] == \"assistant\":\n                    assistant_message = turn[\"content\"]\n\n            # Store intermediate format\n            instruction = \"Complete the statement\"\n            instructions.append(instruction)\n            inputs.append(user_message)\n            outputs.append(assistant_message)\n\n            # Create formatted text\n            text = alpaca_prompt.format(instruction, user_message, assistant_message)\n            texts.append(text)\n\n        return {\n            \"instruction\": instructions,\n            \"input\": inputs,\n            \"output\": outputs,\n            \"text\": texts\n        }\n\n\n\n    dataset_ppl = dataset_ppl.map(formatting_prompts_func, batched=True)\n\n    # Compute perplexity using the passed dataset\n    ppl_value = ppl_model(merged_model, merged_tokenizer, dataset_ppl)\n\n\n    # IMPORTANT: Convert to Python float if it's a tensor\n    if torch.is_tensor(ppl_value):\n        ppl_value = ppl_value.cpu().item()  # Move to CPU and convert to Python scalar\n    elif hasattr(ppl_value, 'item'):\n        ppl_value = ppl_value.item()  # Convert numpy or other array types\n    else:\n        ppl_value = float(ppl_value)  # Ensure it's a float\n\n    # Return only the perplexity value\n    result_queue.put(ppl_value)",
    "Load model and compute perplexity in subprocess"
  ],
  [
    "def prepare_limo_dataset(dataset):\n        \n        if dataset is None:\n            return None\n\n        system_prompt = \"\"\"You are a helpful reasoning assistant. When given a problem, think through it step by step and provide your answer in the following format:\n\n    <reasoning>\n    [Your detailed step-by-step reasoning and solution process]\n    </reasoning>\n    <answer>\n    [Your final numerical answer]\n    </answer>\"\"\"\n\n        def format_limo(example):\n            # Create the assistant response\n            assistant_response = f\"<reasoning>\\n{example['solution']}\\n</reasoning>\\n<answer>\\n{example['answer']}\\n</answer>\"\n\n            # Return a DICTIONARY with the conversation in a field\n            return {\n                \"prompt\": [  #  This is the key change - wrap in a dict\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": example[\"question\"]},\n                    {\"role\": \"assistant\", \"content\": assistant_response}\n                ]\n            }\n\n        return dataset.map(format_limo)",
    "Format LIMO dataset for SFT training"
  ],
  [
    "def get_max_prompt_length(dataset, tokenizer):\n        \n        print(\"Analyzing prompt lengths...\")\n\n        lengths = dataset.map(\n            lambda x: {\n                \"tokens\": tokenizer.apply_chat_template(\n                    x[\"prompt\"],\n                    add_generation_prompt=True,\n                    tokenize=True\n                )\n            },\n            batched=True,\n        ).map(lambda x: {\"length\": len(x[\"tokens\"])})[\"length\"]\n\n        max_length = max(lengths)\n        avg_length = sum(lengths) / len(lengths)\n        min_length = min(lengths)\n\n        print(f\"Prompt lengths - Min: {min_length}, Max: {max_length}, Avg: {avg_length:.1f}\")\n        return max_length, avg_length",
    "Calculate maximum and average prompt length in dataset"
  ],
  [
    "def extract_unsloth_answer(text, start_tag=\"<SOLUTION>\", end_tag=\"</SOLUTION>\"):\n        \n        pattern = re.escape(start_tag) + r\"(.*?)\" + re.escape(end_tag)\n        matches = re.findall(pattern, text, re.DOTALL)\n\n        if matches:\n            answer = matches[-1]  # Get the last match\n            answer = re.sub(r\"[%$,]\", \"\", answer).strip()\n            return answer\n        return \"\"",
    "Extract answer from Unsloth SOLUTION tags"
  ],
  [
    "def find_number(search_string):\n        \n        numbers = re.compile(\n            r\"-?[\\d,]*\\.?\\d+\",\n            re.MULTILINE | re.DOTALL | re.IGNORECASE,\n        ).findall(search_string)\n\n        if numbers:\n            return numbers[-1].replace(\",\", \"\").strip()\n        return \"\"",
    "Find the last number in a string"
  ],
  [
    "def remove_symbols(x: str) -> str:\n        \n        if not x:\n            return \"\"\n        return x.replace(\",\", \"\").replace(\"%\", \"\").replace(\"$\", \"\").strip()",
    "Remove commas, percent and dollar symbols"
  ],
  [
    "def check_format_compliance(text, format_type=\"unsloth\"):\n        \n        if format_type == \"unsloth\":\n            reasoning_start = \"<start_reasoning>\"\n            reasoning_end = \"<end_reasoning>\"\n            solution_start = \"<SOLUTION>\"\n            solution_end = \"</SOLUTION>\"\n\n            pattern = (\n                rf\"^[\\s]*{re.escape(reasoning_start)}.+?{re.escape(reasoning_end)}.*?\"\n                rf\"{re.escape(solution_start)}.+?{re.escape(solution_end)}[\\s]*$\"\n            )\n        else:\n            return False\n\n        return bool(re.match(pattern, text.strip(), re.DOTALL))",
    "Check if response follows expected format"
  ],
  [
    "def evaluate_answer_correctness(extracted_answer, ground_truth):\n        \n        if not extracted_answer or not ground_truth:\n            return False, False, 0.0\n\n        norm_extracted = normalize_answer(extracted_answer)\n        norm_ground_truth = normalize_answer(ground_truth)\n\n        if norm_extracted == norm_ground_truth:\n            return True, True, 1.0\n\n        try:\n            extracted_num = float(norm_extracted)\n            ground_truth_num = float(norm_ground_truth)\n\n            if ground_truth_num != 0:\n                relative_error = abs(extracted_num - ground_truth_num) / abs(ground_truth_num)\n\n                if relative_error < 0.01:\n                    return True, True, 0.9\n                elif relative_error < 0.05:\n                    return False, True, 0.7\n                elif relative_error < 0.10:\n                    return False, True, 0.5\n            else:\n                if extracted_num == 0:\n                    return True, True, 1.0\n                elif abs(extracted_num) < 0.01:\n                    return False, True, 0.7\n\n        except (ValueError, TypeError):\n            if norm_extracted.lower() == norm_ground_truth.lower():\n                return True, True, 1.0\n\n        return False, False, 0.0",
    "Evaluate answer correctness with multiple criteria"
  ],
  [
    "def match_format_exactly(completions, **kwargs):\n        \n        reasoning_start = \"<reasoning>\"\n        reasoning_end = \"</reasoning>\"\n        solution_start = \"<answer>\"\n        solution_end = \"</answer>\"\n\n        pattern = (\n            rf\"^[\\s]*{re.escape(reasoning_start)}.+?{re.escape(reasoning_end)}.*?\"\n            rf\"{re.escape(solution_start)}.+?{re.escape(solution_end)}[\\s]*$\"\n        )\n\n        responses = [completion[0][\"content\"] for completion in completions]\n        rewards = [3.0 if re.match(pattern, response, re.DOTALL) else 0.0 for response in responses]\n        return rewards",
    "Reward function for exact format matching"
  ],
  [
    "def match_format_approximately(completions, **kwargs):\n        \n        reasoning_start = \"<reasoning>\"\n        reasoning_end = \"</reasoning>\"\n        solution_start = \"<answerr>\"\n        solution_end = \"</answer>\"\n\n        scores = []\n        for completion in completions:\n            score = 0\n            response = completion[0][\"content\"]\n            score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n            score += 0.5 if response.count(reasoning_end) == 1 else -1.0\n            score += 0.5 if response.count(solution_start) == 1 else -1.0\n            score += 0.5 if response.count(solution_end) == 1 else -1.0\n            scores.append(score)\n        return scores",
    "Reward function for approximate format matching"
  ],
  [
    "def compare_model_results(all_results):\n        \n        print(f\"\\n{'='*80}\")\n        print(\"COMPREHENSIVE MODEL COMPARISON\")\n        print(f\"{'='*80}\")\n\n        # Main table\n        print(f\"{'Model':<15} {'Format %':<10} {'Exact %':<10} {'Plausible %':<12} {'Confidence':<12}\")\n        print(\"-\" * 80)\n\n        for result in all_results:\n            print(f\"{result['model_type']:<15} \"\n                  f\"{result['correct_format_pct']:<10.1f} \"\n                  f\"{result['exact_match_pct']:<10.1f} \"\n                  f\"{result['plausible_match_pct']:<12.1f} \"\n                  f\"{result['avg_confidence']:<12.3f}\")\n\n        # Improvement analysis\n        if len(all_results) > 1:\n            print(f\"\\n{'='*50}\")\n            print(\"IMPROVEMENT ANALYSIS\")\n            print(f\"{'='*50}\")\n\n            base_result = all_results[0]\n            for result in all_results[1:]:\n                print(f\"\\n{result['model_type']} vs {base_result['model_type']}:\")\n                format_improvement = result['correct_format_pct'] - base_result['correct_format_pct']\n                exact_improvement = result['exact_match_pct'] - base_result['exact_match_pct']\n                plausible_improvement = result['plausible_match_pct'] - base_result['plausible_match_pct']\n\n                print(f\"  Format compliance: {format_improvement:+.1f}%\")\n                print(f\"  Exact matches:     {exact_improvement:+.1f}%\")\n                print(f\"  Plausible matches: {plausible_improvement:+.1f}%\")\n\n        # Save comparison\n        comparison_data = {\n            \"summary\": all_results,\n            \"best_model\": max(all_results, key=lambda x: x['exact_match_pct']),\n        }\n\n        with open(\"model_comparison_comprehensive.json\", \"w\") as f:\n            json.dump(comparison_data, f, indent=4)\n\n        print(f\"\\nBest performing model: {comparison_data['best_model']['model_type']} \"\n              f\"({comparison_data['best_model']['exact_match_pct']:.1f}% exact matches)\")",
    "Generate comprehensive comparison of multiple model results"
  ],
  [
    "class ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    ptuning_checkpoint: str = field(\n        default=None, metadata={\"help\": \"Path to p-tuning v2 checkpoints\"}\n    )\n    config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    model_revision: str = field(\n        default=\"main\",\n        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n    )\n    use_auth_token: bool = field(\n        default=False,\n        metadata={\n            \"help\": (\n                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n                \"with private models).\"\n            )\n        },\n    )\n    resize_position_embeddings: Optional[bool] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Whether to automatically resize the position embeddings if `max_source_length` exceeds \"\n                \"the model's position embeddings.\"\n            )\n        },\n    )\n    quantization_bit: Optional[int] = field(\n        default=None\n    )\n    pre_seq_len: Optional[int] = field(\n        default=None\n    )\n    prefix_projection: bool = field(\n        default=False\n    )",
    "Arguments pertaining to which model/config/tokenizer we are going to fine-tune from."
  ],
  [
    "class DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n\n    lang: Optional[str] = field(default=None, metadata={\"help\": \"Language id for summarization.\"})\n\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    prompt_column: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n    )\n    response_column: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The name of the column in the datasets containing the summaries (for summarization).\"},\n    )\n    history_column: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The name of the column in the datasets containing the history of chat.\"},\n    )\n    train_file: Optional[str] = field(\n        default=None, metadata={\"help\": \"The input training data file (a jsonlines or csv file).\"}\n    )\n    validation_file: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"An optional input evaluation data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n            )\n        },\n    )\n    test_file: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"An optional input test data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    max_source_length: Optional[int] = field(\n        default=1024,\n        metadata={\n            \"help\": (\n                \"The maximum total input sequence length after tokenization. Sequences longer \"\n                \"than this will be truncated, sequences shorter will be padded.\"\n            )\n        },\n    )\n    max_target_length: Optional[int] = field(\n        default=128,\n        metadata={\n            \"help\": (\n                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n                \"than this will be truncated, sequences shorter will be padded.\"\n            )\n        },\n    )\n    val_max_target_length: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n                \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n                \"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used \"\n                \"during ``evaluate`` and ``predict``.\"\n            )\n        },\n    )\n    pad_to_max_length: bool = field(\n        default=False,\n        metadata={\n            \"help\": (\n                \"Whether to pad all samples to model maximum sentence length. \"\n                \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n                \"efficient on GPU but very bad for TPU.\"\n            )\n        },\n    )\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_predict_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    num_beams: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, \"\n                \"which is used during ``evaluate`` and ``predict``.\"\n            )\n        },\n    )\n    ignore_pad_token_for_loss: bool = field(\n        default=True,\n        metadata={\n            \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n        },\n    )\n    source_prefix: Optional[str] = field(\n        default=\"\", metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n    )\n\n    forced_bos_token: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The token to force as the first generated token after the decoder_start_token_id.\"\n                \"Useful for multilingual models like mBART where the first generated token\"\n                \"needs to be the target language token (Usually it is the target language token)\"\n            )\n        },\n    )\n\n    \n\n    def __post_init__(self):\n        if self.dataset_name is None and self.train_file is None and self.validation_file is None and self.test_file is None:\n            raise ValueError(\"Need either a dataset name or a training/validation/test file.\")\n        else:\n            if self.train_file is not None:\n                extension = self.train_file.split(\".\")[-1]\n                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n            if self.validation_file is not None:\n                extension = self.validation_file.split(\".\")[-1]\n                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n        if self.val_max_target_length is None:\n            self.val_max_target_length = self.max_target_length",
    "Arguments pertaining to what data we are going to input our model for training and eval."
  ],
  [
    "class Trainer:\n    \"\"\"\n    Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for  Transformers.\n\n    Args:\n        model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):\n            The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.\n\n            <Tip>\n\n            [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use\n            your own models defined as `torch.nn.Module` as long as they work the same way as the  Transformers\n            models.\n\n            </Tip>\n\n        args ([`TrainingArguments`], *optional*):\n            The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the\n            `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.\n        data_collator (`DataCollator`, *optional*):\n            The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will\n            default to [`default_data_collator`] if no `tokenizer` is provided, an instance of\n            [`DataCollatorWithPadding`] otherwise.\n        train_dataset (`torch.utils.data.Dataset` or `torch.utils.data.IterableDataset`, *optional*):\n            The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the\n            `model.forward()` method are automatically removed.\n\n            Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a\n            distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a\n            `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will\n            manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally\n            sets the seed of the RNGs used.\n        eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`]), *optional*):\n             The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the\n             `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each\n             dataset prepending the dictionary key to the metric name.\n        tokenizer ([`PreTrainedTokenizerBase`], *optional*):\n            The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs to the\n            maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an\n            interrupted training or reuse the fine-tuned model.\n        model_init (`Callable[[], PreTrainedModel]`, *optional*):\n            A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start\n            from a new instance of the model as given by this function.\n\n            The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to\n            be able to choose different architectures according to hyper parameters (such as layer count, sizes of\n            inner layers, dropout probabilities etc).\n        compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):\n            The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return\n            a dictionary string to metric values.\n        callbacks (List of [`TrainerCallback`], *optional*):\n            A list of callbacks to customize the training loop. Will add those to the list of default callbacks\n            detailed in [here](callback).\n\n            If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.\n        optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*): A tuple\n            containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your model\n            and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.\n        preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):\n            A function that preprocess the logits right before caching them at each evaluation step. Must take two\n            tensors, the logits and the labels, and return the logits once processed as desired. The modifications made\n            by this function will be reflected in the predictions received by `compute_metrics`.\n\n            Note that the labels (second parameter) will be `None` if the dataset does not have them.\n\n    Important attributes:\n\n        - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]\n          subclass.\n        - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the\n          original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,\n          the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner\n          model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.\n        - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from\n          data parallelism, this means some of the model layers are split on different GPUs).\n        - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set\n          to `False` if model parallel or deepspeed is used, or if the default\n          `TrainingArguments.place_model_on_device` is overridden to return `False` .\n        - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while\n          in `train`)\n\n    \"\"\"\n\n    from transformers.trainer_pt_utils import _get_learning_rate, log_metrics, metrics_format, save_metrics, save_state\n\n    def __init__(\n        self,\n        model: Union[PreTrainedModel, nn.Module] = None,\n        args: TrainingArguments = None,\n        data_collator: Optional[DataCollator] = None,\n        train_dataset: Optional[Dataset] = None,\n        eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n        tokenizer: Optional[PreTrainedTokenizerBase] = None,\n        model_init: Optional[Callable[[], PreTrainedModel]] = None,\n        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n        callbacks: Optional[List[TrainerCallback]] = None,\n        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),\n        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n        save_prefixencoder: bool = False,\n    ):\n        self.save_prefixencoder = save_prefixencoder\n        if args is None:\n            output_dir = \"tmp_trainer\"\n            logger.info(f\"No `TrainingArguments` passed, using `output_dir={output_dir}`.\")\n            args = TrainingArguments(output_dir=output_dir)\n        self.args = args\n        # Seed must be set before instantiating the model when using model\n        enable_full_determinism(self.args.seed) if self.args.full_determinism else set_seed(self.args.seed)\n        self.hp_name = None\n        self.deepspeed = None\n        self.is_in_train = False\n\n        # memory metrics - must set up as early as possible\n        self._memory_tracker = TrainerMemoryTracker(self.args.skip_memory_metrics)\n        self._memory_tracker.start()\n\n        # set the correct log level depending on the node\n        log_level = args.get_process_log_level()\n        logging.set_verbosity(log_level)\n\n        # force device and distributed setup init explicitly\n        args._setup_devices\n\n        if model is None:\n            if model_init is not None:\n                self.model_init = model_init\n                model = self.call_model_init()\n            else:\n                raise RuntimeError(\"`Trainer` requires either a `model` or `model_init` argument\")\n        else:\n            if model_init is not None:\n                warnings.warn(\n                    \"`Trainer` requires either a `model` or `model_init` argument, but not both. `model_init` will\"\n                    \" overwrite your model when calling the `train` method. This will become a fatal error in the next\"\n                    \" release.\",\n                    FutureWarning,\n                )\n            self.model_init = model_init\n\n        if model.__class__.__name__ in MODEL_MAPPING_NAMES:\n            raise ValueError(\n                f\"The model you have picked ({model.__class__.__name__}) cannot be used as is for training: it only \"\n                \"computes hidden states and does not accept any labels. You should choose a model with a head \"\n                \"suitable for your task like any of the `AutoModelForXxx` listed at \"\n                \"https://huggingface.co/docs/transformers/model_doc/auto.\"\n            )\n\n        if hasattr(model, \"is_parallelizable\") and model.is_parallelizable and model.model_parallel:\n            self.is_model_parallel = True\n        else:\n            self.is_model_parallel = False\n\n        # At this stage the model is already loaded\n        if getattr(model, \"is_loaded_in_8bit\", False):\n            if getattr(model, \"_is_int8_training_enabled\", False):\n                logger.info(\n                    \"The model is loaded in 8-bit precision. To train this model you need to add additional modules\"\n                    \" inside the model such as adapters using `peft` library and freeze the model weights. Please\"\n                    \" check \"\n                    \" the examples in https://github.com/huggingface/peft for more details.\"\n                )\n            else:\n                raise ValueError(\n                    \"The model you want to train is loaded in 8-bit precision.  if you want to fine-tune an 8-bit\"\n                    \" model, please make sure that you have installed `bitsandbytes>=0.37.0`. \"\n                )\n\n        # Setup Sharded DDP training\n        self.sharded_ddp = None\n        if len(args.sharded_ddp) > 0:\n            if args.deepspeed:\n                raise ValueError(\n                    \"Using --sharded_ddp xxx together with --deepspeed is not possible, deactivate one of those flags.\"\n                )\n            if len(args.fsdp) > 0:\n                raise ValueError(\n                    \"Using --sharded_ddp xxx together with --fsdp is not possible, deactivate one of those flags.\"\n                )\n\n            if args.local_rank == -1:\n                raise ValueError(\"Using sharded DDP only works in distributed training.\")\n            elif not is_fairscale_available():\n                raise ImportError(\"Sharded DDP training requires fairscale: `pip install fairscale`.\")\n            elif ShardedDDPOption.SIMPLE not in args.sharded_ddp and FullyShardedDDP is None:\n                raise ImportError(\n                    \"Sharded DDP in a mode other than simple training requires fairscale version >= 0.3, found \"\n                    f\"{fairscale.__version__}. Upgrade your fairscale library: `pip install --upgrade fairscale`.\"\n                )\n            elif ShardedDDPOption.SIMPLE in args.sharded_ddp:\n                self.sharded_ddp = ShardedDDPOption.SIMPLE\n            elif ShardedDDPOption.ZERO_DP_2 in args.sharded_ddp:\n                self.sharded_ddp = ShardedDDPOption.ZERO_DP_2\n            elif ShardedDDPOption.ZERO_DP_3 in args.sharded_ddp:\n                self.sharded_ddp = ShardedDDPOption.ZERO_DP_3\n\n        self.fsdp = None\n        if len(args.fsdp) > 0:\n            if args.deepspeed:\n                raise ValueError(\n                    \"Using --fsdp xxx together with --deepspeed is not possible, deactivate one of those flags.\"\n                )\n            if not args.fsdp_config[\"xla\"] and args.local_rank == -1:\n                raise ValueError(\"Using fsdp only works in distributed training.\")\n\n            # dep_version_check(\"torch>=1.12.0\")\n            # Would have to update setup.py with torch>=1.12.0\n            # which isn't ideally given that it will force people not using FSDP to also use torch>=1.12.0\n            # below is the current alternative.\n            if version.parse(version.parse(torch.__version__).base_version) < version.parse(\"1.12.0\"):\n                raise ValueError(\"FSDP requires PyTorch >= 1.12.0\")\n\n            from torch.distributed.fsdp.fully_sharded_data_parallel import BackwardPrefetch, ShardingStrategy\n\n            if FSDPOption.FULL_SHARD in args.fsdp:\n                self.fsdp = ShardingStrategy.FULL_SHARD\n            elif FSDPOption.SHARD_GRAD_OP in args.fsdp:\n                self.fsdp = ShardingStrategy.SHARD_GRAD_OP\n            elif FSDPOption.NO_SHARD in args.fsdp:\n                self.fsdp = ShardingStrategy.NO_SHARD\n\n            self.backward_prefetch = BackwardPrefetch.BACKWARD_PRE\n            if \"backward_prefetch\" in self.args.fsdp_config and \"backward_pos\" not in self.backward_prefetch:\n                self.backward_prefetch = BackwardPrefetch.BACKWARD_POST\n\n            self.forword_prefetch = False\n            if self.args.fsdp_config.get(\"forword_prefect\", False):\n                self.forword_prefetch = True\n\n            self.limit_all_gathers = False\n            if self.args.fsdp_config.get(\"limit_all_gathers\", False):\n                self.limit_all_gathers = True\n\n        # one place to sort out whether to place the model on device or not\n        # postpone switching model to cuda when:\n        # 1. MP - since we are trying to fit a much bigger than 1 gpu model\n        # 2. fp16-enabled DeepSpeed loads the model in half the size and it doesn't need .to() anyway,\n        #    and we only use deepspeed for training at the moment\n        # 3. full bf16 or fp16 eval - since the model needs to be cast to the right dtype first\n        # 4. Sharded DDP - same as MP\n        # 5. FSDP - same as MP\n        self.place_model_on_device = args.place_model_on_device\n        if (\n            self.is_model_parallel\n            or args.deepspeed\n            or ((args.fp16_full_eval or args.bf16_full_eval) and not args.do_train)\n            or (self.sharded_ddp in [ShardedDDPOption.ZERO_DP_2, ShardedDDPOption.ZERO_DP_3])\n            or (self.fsdp is not None)\n        ):\n            self.place_model_on_device = False\n\n        default_collator = default_data_collator if tokenizer is None else DataCollatorWithPadding(tokenizer)\n        self.data_collator = data_collator if data_collator is not None else default_collator\n        self.train_dataset = train_dataset\n        self.eval_dataset = eval_dataset\n        self.tokenizer = tokenizer\n\n        if self.place_model_on_device and not getattr(model, \"is_loaded_in_8bit\", False):\n            self._move_model_to_device(model, args.device)\n\n        # Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\n        if self.is_model_parallel:\n            self.args._n_gpu = 1\n\n        # later use `self.model is self.model_wrapped` to check if it's wrapped or not\n        self.model_wrapped = model\n        self.model = model\n\n        self.compute_metrics = compute_metrics\n        self.preprocess_logits_for_metrics = preprocess_logits_for_metrics\n        self.optimizer, self.lr_scheduler = optimizers\n        if model_init is not None and (self.optimizer is not None or self.lr_scheduler is not None):\n            raise RuntimeError(\n                \"Passing a `model_init` is incompatible with providing the `optimizers` argument. \"\n                \"You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.\"\n            )\n        if is_torch_tpu_available() and self.optimizer is not None:\n            for param in self.model.parameters():\n                model_device = param.device\n                break\n            for param_group in self.optimizer.param_groups:\n                if len(param_group[\"params\"]) > 0:\n                    optimizer_device = param_group[\"params\"][0].device\n                    break\n            if model_device != optimizer_device:\n                raise ValueError(\n                    \"The model and the optimizer parameters are not on the same device, which probably means you\"\n                    \" created an optimizer around your model **before** putting on the device and passing it to the\"\n                    \" `Trainer`. Make sure the lines `import torch_xla.core.xla_model as xm` and\"\n                    \" `model.to(xm.xla_device())` is performed before the optimizer creation in your script.\"\n                )\n        if ((self.sharded_ddp is not None) or args.deepspeed or (self.fsdp is not None)) and (\n            self.optimizer is not None or self.lr_scheduler is not None\n        ):\n            raise RuntimeError(\n                \"Passing `optimizers` is not allowed if Fairscale, Deepspeed or PyTorch FSDP is enabled.\"\n                \"You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.\"\n            )\n        default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\n        callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\n        self.callback_handler = CallbackHandler(\n            callbacks, self.model, self.tokenizer, self.optimizer, self.lr_scheduler\n        )\n        self.add_callback(PrinterCallback if self.args.disable_tqdm else DEFAULT_PROGRESS_CALLBACK)\n\n        # Will be set to True by `self._setup_loggers()` on first call to `self.log()`.\n        self._loggers_initialized = False\n\n        # Create clone of distant repo and output directory if needed\n        if self.args.push_to_hub:\n            self.init_git_repo(at_init=True)\n            # In case of pull, we need to make sure every process has the latest.\n            if is_torch_tpu_available():\n                xm.rendezvous(\"init git repo\")\n            elif args.local_rank != -1:\n                dist.barrier()\n\n        if self.args.should_save:\n            os.makedirs(self.args.output_dir, exist_ok=True)\n\n        if not callable(self.data_collator) and callable(getattr(self.data_collator, \"collate_batch\", None)):\n            raise ValueError(\"The `data_collator` should be a simple callable (function, class with `__call__`).\")\n\n        if args.max_steps > 0:\n            logger.info(\"max_steps is given, it will override any value given in num_train_epochs\")\n\n        if train_dataset is not None and not has_length(train_dataset) and args.max_steps <= 0:\n            raise ValueError(\"train_dataset does not implement __len__, max_steps has to be specified\")\n\n        if (\n            train_dataset is not None\n            and isinstance(train_dataset, torch.utils.data.IterableDataset)\n            and args.group_by_length\n        ):\n            raise ValueError(\"the `--group_by_length` option is only available for `Dataset`, not `IterableDataset\")\n\n        self._signature_columns = None\n\n        # Mixed precision setup\n        self.use_apex = False\n        self.use_cuda_amp = False\n        self.use_cpu_amp = False\n\n        # Mixed precision setup for SageMaker Model Parallel\n        if is_sagemaker_mp_enabled():\n            # BF16 + model parallelism in SageMaker: currently not supported, raise an error\n            if args.bf16:\n                raise ValueError(\"SageMaker Model Parallelism does not support BF16 yet. Please use FP16 instead \")\n\n            if IS_SAGEMAKER_MP_POST_1_10:\n                # When there's mismatch between SMP config and trainer argument, use SMP config as truth\n                if args.fp16 != smp.state.cfg.fp16:\n                    logger.warning(\n                        f\"FP16 provided in SM_HP_MP_PARAMETERS is {smp.state.cfg.fp16},\"\n                        f\"but FP16 provided in trainer argument is {args.fp16},\"\n                        f\"setting to {smp.state.cfg.fp16}\"\n                    )\n                    args.fp16 = smp.state.cfg.fp16\n            else:\n                # smp < 1.10 does not support fp16 in trainer.\n                if hasattr(smp.state.cfg, \"fp16\"):\n                    logger.warning(\n                        f\"FP16 provided in SM_HP_MP_PARAMETERS is {smp.state.cfg.fp16}, \"\n                        \"but SageMaker Model Parallelism < 1.10 does not support FP16 in trainer.\"\n                    )\n\n        if args.fp16 or args.bf16:\n            if args.half_precision_backend == \"auto\":\n                if args.device == torch.device(\"cpu\"):\n                    if args.fp16:\n                        raise ValueError(\"Tried to use `fp16` but it is not supported on cpu\")\n                    elif _is_native_cpu_amp_available:\n                        args.half_precision_backend = \"cpu_amp\"\n                    else:\n                        raise ValueError(\"Tried to use cpu amp but native cpu amp is not available\")\n                else:\n                    args.half_precision_backend = \"cuda_amp\"\n\n            logger.info(f\"Using {args.half_precision_backend} half precision backend\")\n\n        self.do_grad_scaling = False\n        if (args.fp16 or args.bf16) and not (args.deepspeed or is_sagemaker_mp_enabled() or is_torch_tpu_available()):\n            # deepspeed and SageMaker Model Parallel manage their own half precision\n            if args.half_precision_backend == \"cuda_amp\":\n                self.use_cuda_amp = True\n                self.amp_dtype = torch.float16 if args.fp16 else torch.bfloat16\n                #  bf16 does not need grad scaling\n                self.do_grad_scaling = self.amp_dtype == torch.float16\n                if self.do_grad_scaling:\n                    if self.sharded_ddp is not None:\n                        self.scaler = ShardedGradScaler()\n                    elif self.fsdp is not None:\n                        from torch.distributed.fsdp.sharded_grad_scaler import (\n                            ShardedGradScaler as FSDPShardedGradScaler,\n                        )\n\n                        self.scaler = FSDPShardedGradScaler()\n                    elif is_torch_tpu_available():\n                        from torch_xla.amp import GradScaler\n\n                        self.scaler = GradScaler()\n                    else:\n                        self.scaler = torch.cuda.amp.GradScaler()\n            elif args.half_precision_backend == \"cpu_amp\":\n                self.use_cpu_amp = True\n                self.amp_dtype = torch.bfloat16\n            else:\n                if not is_apex_available():\n                    raise ImportError(\n                        \"Using FP16 with APEX but APEX is not installed, please refer to\"\n                        \" https://www.github.com/nvidia/apex.\"\n                    )\n                self.use_apex = True\n\n        # FP16 + model parallelism in SageMaker: gradient clipping does not work for now so we raise a helpful error.\n        if (\n            is_sagemaker_mp_enabled()\n            and self.use_cuda_amp\n            and args.max_grad_norm is not None\n            and args.max_grad_norm > 0\n        ):\n            raise ValueError(\n                \"SageMaker Model Parallelism in mixed precision mode does not support gradient clipping yet. Pass \"\n                \"along 'max_grad_norm': 0 in your hyperparameters.\"\n            )\n\n        # Label smoothing\n        if self.args.label_smoothing_factor != 0:\n            self.label_smoother = LabelSmoother(epsilon=self.args.label_smoothing_factor)\n        else:\n            self.label_smoother = None\n\n        self.state = TrainerState(\n            is_local_process_zero=self.is_local_process_zero(),\n            is_world_process_zero=self.is_world_process_zero(),\n        )\n\n        self.control = TrainerControl()\n        # Internal variable to count flos in each process, will be accumulated in `self.state.total_flos` then\n        # returned to 0 every time flos need to be logged\n        self.current_flos = 0\n        self.hp_search_backend = None\n        self.use_tune_checkpoints = False\n        default_label_names = find_labels(self.model.__class__)\n        self.label_names = default_label_names if self.args.label_names is None else self.args.label_names\n        self.can_return_loss = can_return_loss(self.model.__class__)\n        self.control = self.callback_handler.on_init_end(self.args, self.state, self.control)\n\n        # Internal variables to keep track of the original batch size\n        self._train_batch_size = args.train_batch_size\n\n        # very last\n        self._memory_tracker.stop_and_update_metrics()\n\n        # torch.compile\n        if args.torch_compile and not is_torch_compile_available():\n            raise RuntimeError(\"Using torch.compile requires PyTorch 2.0 or higher.\")\n\n    def add_callback(self, callback):\n        \"\"\"\n        Add a callback to the current list of [`~transformer.TrainerCallback`].\n\n        Args:\n           callback (`type` or [`~transformer.TrainerCallback`]):\n               A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n               first case, will instantiate a member of that class.\n        \"\"\"\n        self.callback_handler.add_callback(callback)\n\n    def pop_callback(self, callback):\n        \"\"\"\n        Remove a callback from the current list of [`~transformer.TrainerCallback`] and returns it.\n\n        If the callback is not found, returns `None` (and no error is raised).\n\n        Args:\n           callback (`type` or [`~transformer.TrainerCallback`]):\n               A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n               first case, will pop the first member of that class found in the list of callbacks.\n\n        Returns:\n            [`~transformer.TrainerCallback`]: The callback removed, if found.\n        \"\"\"\n        return self.callback_handler.pop_callback(callback)\n\n    def remove_callback(self, callback):\n        \"\"\"\n        Remove a callback from the current list of [`~transformer.TrainerCallback`].\n\n        Args:\n           callback (`type` or [`~transformer.TrainerCallback`]):\n               A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n               first case, will remove the first member of that class found in the list of callbacks.\n        \"\"\"\n        self.callback_handler.remove_callback(callback)\n\n    def _move_model_to_device(self, model, device):\n        model = model.to(device)\n        # Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\n        if self.args.parallel_mode == ParallelMode.TPU and hasattr(model, \"tie_weights\"):\n            model.tie_weights()\n\n    def _set_signature_columns_if_needed(self):\n        if self._signature_columns is None:\n            # Inspect model forward signature to keep only the arguments it accepts.\n            signature = inspect.signature(self.model.forward)\n            self._signature_columns = list(signature.parameters.keys())\n            # Labels may be named label or label_ids, the default data collator handles that.\n            self._signature_columns += list(set([\"label\", \"label_ids\"] + self.label_names))\n\n    def _remove_unused_columns(self, dataset: \"datasets.Dataset\", description: Optional[str] = None):\n        if not self.args.remove_unused_columns:\n            return dataset\n        self._set_signature_columns_if_needed()\n        signature_columns = self._signature_columns\n\n        ignored_columns = list(set(dataset.column_names) - set(signature_columns))\n        if len(ignored_columns) > 0:\n            dset_description = \"\" if description is None else f\"in the {description} set\"\n            logger.info(\n                f\"The following columns {dset_description} don't have a corresponding argument in \"\n                f\"`{self.model.__class__.__name__}.forward` and have been ignored: {', '.join(ignored_columns)}.\"\n                f\" If {', '.join(ignored_columns)} are not expected by `{self.model.__class__.__name__}.forward`, \"\n                \" you can safely ignore this message.\"\n            )\n\n        columns = [k for k in signature_columns if k in dataset.column_names]\n\n        if version.parse(datasets.__version__) < version.parse(\"1.4.0\"):\n            dataset.set_format(\n                type=dataset.format[\"type\"], columns=columns, format_kwargs=dataset.format[\"format_kwargs\"]\n            )\n            return dataset\n        else:\n            return dataset.remove_columns(ignored_columns)\n\n    def _get_collator_with_removed_columns(\n        self, data_collator: Callable, description: Optional[str] = None\n    ) -> Callable:\n        \"\"\"Wrap the data collator in a callable removing unused columns.\"\"\"\n        if not self.args.remove_unused_columns:\n            return data_collator\n        self._set_signature_columns_if_needed()\n        signature_columns = self._signature_columns\n\n        remove_columns_collator = RemoveColumnsCollator(\n            data_collator=data_collator,\n            signature_columns=signature_columns,\n            logger=logger,\n            description=description,\n            model_name=self.model.__class__.__name__,\n        )\n        return remove_columns_collator\n\n    def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:\n        if self.train_dataset is None or not has_length(self.train_dataset):\n            return None\n\n        generator = None\n        if self.args.world_size <= 1:\n            generator = torch.Generator()\n            # for backwards compatibility, we generate a seed here (which is sampled from a generator seeded with\n            # `args.seed`) if data_seed isn't provided.\n            # Further on in this method, we default to `args.seed` instead.\n            if self.args.data_seed is None:\n                seed = int(torch.empty((), dtype=torch.int64).random_().item())\n            else:\n                seed = self.args.data_seed\n            generator.manual_seed(seed)\n\n        seed = self.args.data_seed if self.args.data_seed is not None else self.args.seed\n\n        # Build the sampler.\n        if self.args.group_by_length:\n            if is_datasets_available() and isinstance(self.train_dataset, datasets.Dataset):\n                lengths = (\n                    self.train_dataset[self.args.length_column_name]\n                    if self.args.length_column_name in self.train_dataset.column_names\n                    else None\n                )\n            else:\n                lengths = None\n            model_input_name = self.tokenizer.model_input_names[0] if self.tokenizer is not None else None\n            if self.args.world_size <= 1:\n                return LengthGroupedSampler(\n                    self.args.train_batch_size * self.args.gradient_accumulation_steps,\n                    dataset=self.train_dataset,\n                    lengths=lengths,\n                    model_input_name=model_input_name,\n                    generator=generator,\n                )\n            else:\n                return DistributedLengthGroupedSampler(\n                    self.args.train_batch_size * self.args.gradient_accumulation_steps,\n                    dataset=self.train_dataset,\n                    num_replicas=self.args.world_size,\n                    rank=self.args.process_index,\n                    lengths=lengths,\n                    model_input_name=model_input_name,\n                    seed=seed,\n                )\n\n        else:\n            if self.args.world_size <= 1:\n                return RandomSampler(self.train_dataset, generator=generator)\n            elif (\n                self.args.parallel_mode in [ParallelMode.TPU, ParallelMode.SAGEMAKER_MODEL_PARALLEL]\n                and not self.args.dataloader_drop_last\n            ):\n                # Use a loop for TPUs when drop_last is False to have all batches have the same size.\n                return DistributedSamplerWithLoop(\n                    self.train_dataset,\n                    batch_size=self.args.per_device_train_batch_size,\n                    num_replicas=self.args.world_size,\n                    rank=self.args.process_index,\n                    seed=seed,\n                )\n            else:\n                return DistributedSampler(\n                    self.train_dataset,\n                    num_replicas=self.args.world_size,\n                    rank=self.args.process_index,\n                    seed=seed,\n                )\n\n    def get_train_dataloader(self) -> DataLoader:\n        \"\"\"\n        Returns the training [`~torch.utils.data.DataLoader`].\n\n        Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed\n        training if necessary) otherwise.\n\n        Subclass and override this method if you want to inject some custom behavior.\n        \"\"\"\n        if self.train_dataset is None:\n            raise ValueError(\"Trainer: training requires a train_dataset.\")\n\n        train_dataset = self.train_dataset\n        data_collator = self.data_collator\n        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n            train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n        else:\n            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\")\n\n        if isinstance(train_dataset, torch.utils.data.IterableDataset):\n            if self.args.world_size > 1:\n                train_dataset = IterableDatasetShard(\n                    train_dataset,\n                    batch_size=self._train_batch_size,\n                    drop_last=self.args.dataloader_drop_last,\n                    num_processes=self.args.world_size,\n                    process_index=self.args.process_index,\n                )\n\n            return DataLoader(\n                train_dataset,\n                batch_size=self._train_batch_size,\n                collate_fn=data_collator,\n                num_workers=self.args.dataloader_num_workers,\n                pin_memory=self.args.dataloader_pin_memory,\n            )\n\n        train_sampler = self._get_train_sampler()\n\n        return DataLoader(\n            train_dataset,\n            batch_size=self._train_batch_size,\n            sampler=train_sampler,\n            collate_fn=data_collator,\n            drop_last=self.args.dataloader_drop_last,\n            num_workers=self.args.dataloader_num_workers,\n            pin_memory=self.args.dataloader_pin_memory,\n            worker_init_fn=seed_worker,\n        )\n\n    def _get_eval_sampler(self, eval_dataset: Dataset) -> Optional[torch.utils.data.Sampler]:\n        # Deprecated code\n        if self.args.use_legacy_prediction_loop:\n            if is_torch_tpu_available():\n                return SequentialDistributedSampler(\n                    eval_dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal()\n                )\n            elif is_sagemaker_mp_enabled():\n                return SequentialDistributedSampler(\n                    eval_dataset,\n                    num_replicas=smp.dp_size(),\n                    rank=smp.dp_rank(),\n                    batch_size=self.args.per_device_eval_batch_size,\n                )\n            elif self.args.local_rank != -1:\n                return SequentialDistributedSampler(eval_dataset)\n            else:\n                return SequentialSampler(eval_dataset)\n\n        if self.args.world_size <= 1:\n            return SequentialSampler(eval_dataset)\n        else:\n            return ShardSampler(\n                eval_dataset,\n                batch_size=self.args.per_device_eval_batch_size,\n                num_processes=self.args.world_size,\n                process_index=self.args.process_index,\n            )\n\n    def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -> DataLoader:\n        \"\"\"\n        Returns the evaluation [`~torch.utils.data.DataLoader`].\n\n        Subclass and override this method if you want to inject some custom behavior.\n\n        Args:\n            eval_dataset (`torch.utils.data.Dataset`, *optional*):\n                If provided, will override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns not accepted\n                by the `model.forward()` method are automatically removed. It must implement `__len__`.\n        \"\"\"\n        if eval_dataset is None and self.eval_dataset is None:\n            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n        data_collator = self.data_collator\n\n        if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):\n            eval_dataset = self._remove_unused_columns(eval_dataset, description=\"evaluation\")\n        else:\n            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"evaluation\")\n\n        if isinstance(eval_dataset, torch.utils.data.IterableDataset):\n            if self.args.world_size > 1:\n                eval_dataset = IterableDatasetShard(\n                    eval_dataset,\n                    batch_size=self.args.per_device_eval_batch_size,\n                    drop_last=self.args.dataloader_drop_last,\n                    num_processes=self.args.world_size,\n                    process_index=self.args.process_index,\n                )\n            return DataLoader(\n                eval_dataset,\n                batch_size=self.args.eval_batch_size,\n                collate_fn=data_collator,\n                num_workers=self.args.dataloader_num_workers,\n                pin_memory=self.args.dataloader_pin_memory,\n            )\n\n        eval_sampler = self._get_eval_sampler(eval_dataset)\n\n        return DataLoader(\n            eval_dataset,\n            sampler=eval_sampler,\n            batch_size=self.args.eval_batch_size,\n            collate_fn=data_collator,\n            drop_last=self.args.dataloader_drop_last,\n            num_workers=self.args.dataloader_num_workers,\n            pin_memory=self.args.dataloader_pin_memory,\n        )\n\n    def get_test_dataloader(self, test_dataset: Dataset) -> DataLoader:\n        \"\"\"\n        Returns the test [`~torch.utils.data.DataLoader`].\n\n        Subclass and override this method if you want to inject some custom behavior.\n\n        Args:\n            test_dataset (`torch.utils.data.Dataset`, *optional*):\n                The test dataset to use. If it is a [`~datasets.Dataset`], columns not accepted by the\n                `model.forward()` method are automatically removed. It must implement `__len__`.\n        \"\"\"\n        data_collator = self.data_collator\n\n        if is_datasets_available() and isinstance(test_dataset, datasets.Dataset):\n            test_dataset = self._remove_unused_columns(test_dataset, description=\"test\")\n        else:\n            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"test\")\n\n        if isinstance(test_dataset, torch.utils.data.IterableDataset):\n            if self.args.world_size > 1:\n                test_dataset = IterableDatasetShard(\n                    test_dataset,\n                    batch_size=self.args.eval_batch_size,\n                    drop_last=self.args.dataloader_drop_last,\n                    num_processes=self.args.world_size,\n                    process_index=self.args.process_index,\n                )\n            return DataLoader(\n                test_dataset,\n                batch_size=self.args.eval_batch_size,\n                collate_fn=data_collator,\n                num_workers=self.args.dataloader_num_workers,\n                pin_memory=self.args.dataloader_pin_memory,\n            )\n\n        test_sampler = self._get_eval_sampler(test_dataset)\n\n        # We use the same batch_size as for eval.\n        return DataLoader(\n            test_dataset,\n            sampler=test_sampler,\n            batch_size=self.args.eval_batch_size,\n            collate_fn=data_collator,\n            drop_last=self.args.dataloader_drop_last,\n            num_workers=self.args.dataloader_num_workers,\n            pin_memory=self.args.dataloader_pin_memory,\n        )\n\n    def create_optimizer_and_scheduler(self, num_training_steps: int):\n        \"\"\"\n        Setup the optimizer and the learning rate scheduler.\n\n        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through `optimizers`, or subclass and override this method (or `create_optimizer` and/or\n        `create_scheduler`) in a subclass.\n        \"\"\"\n        self.create_optimizer()\n        if IS_SAGEMAKER_MP_POST_1_10 and smp.state.cfg.fp16:\n            # If smp >= 1.10 and fp16 is enabled, we unwrap the optimizer\n            optimizer = self.optimizer.optimizer\n        else:\n            optimizer = self.optimizer\n        self.create_scheduler(num_training_steps=num_training_steps, optimizer=optimizer)\n\n    def create_optimizer(self):\n        \"\"\"\n        Setup the optimizer.\n\n        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through `optimizers`, or subclass and override this method in a subclass.\n        \"\"\"\n        opt_model = self.model_wrapped if is_sagemaker_mp_enabled() else self.model\n\n        if self.optimizer is None:\n            decay_parameters = get_parameter_names(opt_model, ALL_LAYERNORM_LAYERS)\n            decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n            optimizer_grouped_parameters = [\n                {\n                    \"params\": [\n                        p for n, p in opt_model.named_parameters() if (n in decay_parameters and p.requires_grad)\n                    ],\n                    \"weight_decay\": self.args.weight_decay,\n                },\n                {\n                    \"params\": [\n                        p for n, p in opt_model.named_parameters() if (n not in decay_parameters and p.requires_grad)\n                    ],\n                    \"weight_decay\": 0.0,\n                },\n            ]\n\n            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args)\n\n            if self.sharded_ddp == ShardedDDPOption.SIMPLE:\n                self.optimizer = OSS(\n                    params=optimizer_grouped_parameters,\n                    optim=optimizer_cls,\n                    **optimizer_kwargs,\n                )\n            else:\n                self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)\n                if optimizer_cls.__name__ == \"Adam8bit\":\n                    import bitsandbytes\n\n                    manager = bitsandbytes.optim.GlobalOptimManager.get_instance()\n\n                    skipped = 0\n                    for module in opt_model.modules():\n                        if isinstance(module, nn.Embedding):\n                            skipped += sum({p.data_ptr(): p.numel() for p in module.parameters()}.values())\n                            print(f\"skipped {module}: {skipped/2**20}M params\")\n                            manager.register_module_override(module, \"weight\", {\"optim_bits\": 32})\n                            logger.debug(f\"bitsandbytes: will optimize {module} in fp32\")\n                    print(f\"skipped: {skipped/2**20}M params\")\n\n        if is_sagemaker_mp_enabled():\n            self.optimizer = smp.DistributedOptimizer(self.optimizer)\n\n        return self.optimizer\n\n    @staticmethod\n    def get_optimizer_cls_and_kwargs(args: TrainingArguments) -> Tuple[Any, Any]:\n        \"\"\"\n        Returns the optimizer class and optimizer parameters based on the training arguments.\n\n        Args:\n            args (`transformers.training_args.TrainingArguments`):\n                The training arguments for the training session.\n\n        \"\"\"\n\n        # parse args.optim_args\n        optim_args = {}\n        if args.optim_args:\n            for mapping in args.optim_args.replace(\" \", \"\").split(\",\"):\n                key, value = mapping.split(\"=\")\n                optim_args[key] = value\n\n        optimizer_kwargs = {\"lr\": args.learning_rate}\n\n        adam_kwargs = {\n            \"betas\": (args.adam_beta1, args.adam_beta2),\n            \"eps\": args.adam_epsilon,\n        }\n        if args.optim == OptimizerNames.ADAFACTOR:\n            optimizer_cls = Adafactor\n            optimizer_kwargs.update({\"scale_parameter\": False, \"relative_step\": False})\n        elif args.optim == OptimizerNames.ADAMW_HF:\n            from transformers.optimization import AdamW\n\n            optimizer_cls = AdamW\n            optimizer_kwargs.update(adam_kwargs)\n        elif args.optim in [OptimizerNames.ADAMW_TORCH, OptimizerNames.ADAMW_TORCH_FUSED]:\n            from torch.optim import AdamW\n\n            optimizer_cls = AdamW\n            optimizer_kwargs.update(adam_kwargs)\n            if args.optim == OptimizerNames.ADAMW_TORCH_FUSED:\n                optimizer_kwargs.update({\"fused\": True})\n        elif args.optim == OptimizerNames.ADAMW_TORCH_XLA:\n            try:\n                from torch_xla.amp.syncfree import AdamW\n\n                optimizer_cls = AdamW\n                optimizer_kwargs.update(adam_kwargs)\n            except ImportError:\n                raise ValueError(\"Trainer failed to import syncfree AdamW from torch_xla.\")\n        elif args.optim == OptimizerNames.ADAMW_APEX_FUSED:\n            try:\n                from apex.optimizers import FusedAdam\n\n                optimizer_cls = FusedAdam\n                optimizer_kwargs.update(adam_kwargs)\n            except ImportError:\n                raise ValueError(\"Trainer tried to instantiate apex FusedAdam but apex is not installed!\")\n        elif args.optim == OptimizerNames.ADAMW_BNB:\n            try:\n                from bitsandbytes.optim import Adam8bit\n\n                optimizer_cls = Adam8bit\n                optimizer_kwargs.update(adam_kwargs)\n            except ImportError:\n                raise ValueError(\"Trainer tried to instantiate bnb Adam8bit but bnb is not installed!\")\n        elif args.optim == OptimizerNames.ADAMW_ANYPRECISION:\n            try:\n                from torchdistx.optimizers import AnyPrecisionAdamW\n\n                optimizer_cls = AnyPrecisionAdamW\n                optimizer_kwargs.update(adam_kwargs)\n\n                # TODO Change dtypes back to M=FP32, Var = BF16, Kahan = False once they can be cast together in torchdistx.\n                optimizer_kwargs.update(\n                    {\n                        \"use_kahan_summation\": strtobool(optim_args.get(\"use_kahan_summation\", \"False\")),\n                        \"momentum_dtype\": getattr(torch, optim_args.get(\"momentum_dtype\", \"float32\")),\n                        \"variance_dtype\": getattr(torch, optim_args.get(\"variance_dtype\", \"float32\")),\n                        \"compensation_buffer_dtype\": getattr(\n                            torch, optim_args.get(\"compensation_buffer_dtype\", \"bfloat16\")\n                        ),\n                    }\n                )\n            except ImportError:\n                raise ValueError(\"Please install https://github.com/pytorch/torchdistx\")\n        elif args.optim == OptimizerNames.SGD:\n            optimizer_cls = torch.optim.SGD\n        elif args.optim == OptimizerNames.ADAGRAD:\n            optimizer_cls = torch.optim.Adagrad\n        else:\n            raise ValueError(f\"Trainer cannot instantiate unsupported optimizer: {args.optim}\")\n        return optimizer_cls, optimizer_kwargs\n\n    def create_scheduler(self, num_training_steps: int, optimizer: torch.optim.Optimizer = None):\n        \"\"\"\n        Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or\n        passed as an argument.\n\n        Args:\n            num_training_steps (int): The number of training steps to do.\n        \"\"\"\n        if self.lr_scheduler is None:\n            self.lr_scheduler = get_scheduler(\n                self.args.lr_scheduler_type,\n                optimizer=self.optimizer if optimizer is None else optimizer,\n                num_warmup_steps=self.args.get_warmup_steps(num_training_steps),\n                num_training_steps=num_training_steps,\n            )\n        return self.lr_scheduler\n\n    def num_examples(self, dataloader: DataLoader) -> int:\n        \"\"\"\n        Helper to get number of samples in a [`~torch.utils.data.DataLoader`] by accessing its dataset. When\n        dataloader.dataset does not exist or has no length, estimates as best it can\n        \"\"\"\n        try:\n            dataset = dataloader.dataset\n            # Special case for IterableDatasetShard, we need to dig deeper\n            if isinstance(dataset, IterableDatasetShard):\n                return len(dataloader.dataset.dataset)\n            return len(dataloader.dataset)\n        except (NameError, AttributeError, TypeError):  # no dataset or length, estimate by length of dataloader\n            return len(dataloader) * self.args.per_device_train_batch_size\n\n    def _hp_search_setup(self, trial: Union[\"optuna.Trial\", Dict[str, Any]]):\n        \"\"\"HP search setup code\"\"\"\n        self._trial = trial\n\n        if self.hp_search_backend is None or trial is None:\n            return\n        if self.hp_search_backend == HPSearchBackend.OPTUNA:\n            params = self.hp_space(trial)\n        elif self.hp_search_backend == HPSearchBackend.RAY:\n            params = trial\n            params.pop(\"wandb\", None)\n        elif self.hp_search_backend == HPSearchBackend.SIGOPT:\n            params = {k: int(v) if isinstance(v, str) else v for k, v in trial.assignments.items()}\n        elif self.hp_search_backend == HPSearchBackend.WANDB:\n            params = trial\n\n        for key, value in params.items():\n            if not hasattr(self.args, key):\n                logger.warning(\n                    f\"Trying to set {key} in the hyperparameter search but there is no corresponding field in\"\n                    \" `TrainingArguments`.\"\n                )\n                continue\n            old_attr = getattr(self.args, key, None)\n            # Casting value to the proper type\n            if old_attr is not None:\n                value = type(old_attr)(value)\n            setattr(self.args, key, value)\n        if self.hp_search_backend == HPSearchBackend.OPTUNA:\n            logger.info(f\"Trial: {trial.params}\")\n        if self.hp_search_backend == HPSearchBackend.SIGOPT:\n            logger.info(f\"SigOpt Assignments: {trial.assignments}\")\n        if self.hp_search_backend == HPSearchBackend.WANDB:\n            logger.info(f\"W&B Sweep parameters: {trial}\")\n        if self.args.deepspeed:\n            # Rebuild the deepspeed config to reflect the updated training parameters\n            from transformers.deepspeed import HfTrainerDeepSpeedConfig\n\n            self.args.hf_deepspeed_config = HfTrainerDeepSpeedConfig(self.args.deepspeed)\n            self.args.hf_deepspeed_config.trainer_config_process(self.args)\n\n    def _report_to_hp_search(self, trial: Union[\"optuna.Trial\", Dict[str, Any]], step: int, metrics: Dict[str, float]):\n        if self.hp_search_backend is None or trial is None:\n            return\n        self.objective = self.compute_objective(metrics.copy())\n        if self.hp_search_backend == HPSearchBackend.OPTUNA:\n            import optuna\n\n            trial.report(self.objective, step)\n            if trial.should_prune():\n                self.callback_handler.on_train_end(self.args, self.state, self.control)\n                raise optuna.TrialPruned()\n        elif self.hp_search_backend == HPSearchBackend.RAY:\n            from ray import tune\n\n            if self.control.should_save:\n                self._tune_save_checkpoint()\n            tune.report(objective=self.objective, **metrics)\n\n    def _tune_save_checkpoint(self):\n        from ray import tune\n\n        if not self.use_tune_checkpoints:\n            return\n        with tune.checkpoint_dir(step=self.state.global_step) as checkpoint_dir:\n            output_dir = os.path.join(checkpoint_dir, f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\")\n            self.save_model(output_dir, _internal_call=True)\n            if self.args.should_save:\n                self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))\n                torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))\n                torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, SCHEDULER_NAME))\n\n    def call_model_init(self, trial=None):\n        model_init_argcount = number_of_arguments(self.model_init)\n        if model_init_argcount == 0:\n            model = self.model_init()\n        elif model_init_argcount == 1:\n            model = self.model_init(trial)\n        else:\n            raise RuntimeError(\"model_init should have 0 or 1 argument.\")\n\n        if model is None:\n            raise RuntimeError(\"model_init should not return None.\")\n\n        return model\n\n    def torch_jit_model_eval(self, model, dataloader, training=False):\n        if not training:\n            if dataloader is None:\n                logger.warning(\"failed to use PyTorch jit mode due to current dataloader is none.\")\n                return model\n            example_batch = next(iter(dataloader))\n            example_batch = self._prepare_inputs(example_batch)\n            try:\n                jit_model = model.eval()\n                with ContextManagers([self.autocast_smart_context_manager(cache_enabled=False), torch.no_grad()]):\n                    if version.parse(version.parse(torch.__version__).base_version) >= version.parse(\"1.14.0\"):\n                        if isinstance(example_batch, dict):\n                            jit_model = torch.jit.trace(jit_model, example_kwarg_inputs=example_batch, strict=False)\n                        else:\n                            jit_model = torch.jit.trace(\n                                jit_model,\n                                example_kwarg_inputs={key: example_batch[key] for key in example_batch},\n                                strict=False,\n                            )\n                    else:\n                        jit_inputs = []\n                        for key in example_batch:\n                            example_tensor = torch.ones_like(example_batch[key])\n                            jit_inputs.append(example_tensor)\n                        jit_inputs = tuple(jit_inputs)\n                        jit_model = torch.jit.trace(jit_model, jit_inputs, strict=False)\n                jit_model = torch.jit.freeze(jit_model)\n                with torch.no_grad():\n                    jit_model(**example_batch)\n                    jit_model(**example_batch)\n                model = jit_model\n                self.use_cpu_amp = False\n                self.use_cuda_amp = False\n            except (RuntimeError, TypeError, ValueError, NameError, IndexError) as e:\n                logger.warning(f\"failed to use PyTorch jit mode due to: {e}.\")\n\n        return model\n\n    def ipex_optimize_model(self, model, training=False, dtype=torch.float32):\n        if not is_ipex_available():\n            raise ImportError(\n                \"Using IPEX but IPEX is not installed or IPEX's version does not match current PyTorch, please refer\"\n                \" to https://github.com/intel/intel-extension-for-pytorch.\"\n            )\n\n        import intel_extension_for_pytorch as ipex\n\n        if not training:\n            model.eval()\n            dtype = torch.bfloat16 if not self.is_in_train and self.args.bf16_full_eval else dtype\n            # conv_bn_folding is disabled as it fails in symbolic tracing, resulting in ipex warnings\n            model = ipex.optimize(model, dtype=dtype, level=\"O1\", conv_bn_folding=False, inplace=not self.is_in_train)\n        else:\n            if not model.training:\n                model.train()\n            model, self.optimizer = ipex.optimize(\n                model, dtype=dtype, optimizer=self.optimizer, inplace=True, level=\"O1\"\n            )\n\n        return model\n\n    def _wrap_model(self, model, training=True, dataloader=None):\n        if self.args.torch_compile:\n            model = torch.compile(model, backend=self.args.torch_compile_backend, mode=self.args.torch_compile_mode)\n\n        if self.args.use_ipex:\n            dtype = torch.bfloat16 if self.use_cpu_amp else torch.float32\n            model = self.ipex_optimize_model(model, training, dtype=dtype)\n\n        if is_sagemaker_mp_enabled():\n            # Wrapping the base model twice in a DistributedModel will raise an error.\n            if isinstance(self.model_wrapped, smp.model.DistributedModel):\n                return self.model_wrapped\n            return smp.DistributedModel(model, backward_passes_per_step=self.args.gradient_accumulation_steps)\n\n        # already initialized its own DDP and AMP\n        if self.deepspeed:\n            return self.deepspeed\n\n        # train/eval could be run multiple-times - if already wrapped, don't re-wrap it again\n        if unwrap_model(model) is not model:\n            return model\n\n        # Mixed precision training with apex (torch < 1.6)\n        if self.use_apex and training:\n            model, self.optimizer = amp.initialize(model, self.optimizer, opt_level=self.args.fp16_opt_level)\n\n        # Multi-gpu training (should be after apex fp16 initialization)\n        if self.args.n_gpu > 1:\n            model = nn.DataParallel(model)\n\n        if self.args.jit_mode_eval:\n            start_time = time.time()\n            model = self.torch_jit_model_eval(model, dataloader, training)\n            self.jit_compilation_time = round(time.time() - start_time, 4)\n\n        # Note: in torch.distributed mode, there's no point in wrapping the model\n        # inside a DistributedDataParallel as we'll be under `no_grad` anyways.\n        if not training:\n            return model\n\n        # Distributed training (should be after apex fp16 initialization)\n        if self.sharded_ddp is not None:\n            # Sharded DDP!\n            if self.sharded_ddp == ShardedDDPOption.SIMPLE:\n                model = ShardedDDP(model, self.optimizer)\n            else:\n                mixed_precision = self.args.fp16 or self.args.bf16\n                cpu_offload = ShardedDDPOption.OFFLOAD in self.args.sharded_ddp\n                zero_3 = self.sharded_ddp == ShardedDDPOption.ZERO_DP_3\n                # XXX: Breaking the self.model convention but I see no way around it for now.\n                if ShardedDDPOption.AUTO_WRAP in self.args.sharded_ddp:\n                    model = auto_wrap(model)\n                self.model = model = FullyShardedDDP(\n                    model,\n                    mixed_precision=mixed_precision,\n                    reshard_after_forward=zero_3,\n                    cpu_offload=cpu_offload,\n                ).to(self.args.device)\n        # Distributed training using PyTorch FSDP\n        elif self.fsdp is not None:\n            if not self.args.fsdp_config[\"xla\"]:\n                # PyTorch FSDP!\n                from torch.distributed.fsdp.fully_sharded_data_parallel import CPUOffload, MixedPrecision\n                from torch.distributed.fsdp.fully_sharded_data_parallel import FullyShardedDataParallel as FSDP\n                from torch.distributed.fsdp.wrap import size_based_auto_wrap_policy, transformer_auto_wrap_policy\n\n                if FSDPOption.OFFLOAD in self.args.fsdp:\n                    cpu_offload = CPUOffload(offload_params=True)\n                else:\n                    cpu_offload = CPUOffload(offload_params=False)\n\n                auto_wrap_policy = None\n\n                if FSDPOption.AUTO_WRAP in self.args.fsdp:\n                    if self.args.fsdp_config[\"fsdp_min_num_params\"] > 0:\n                        auto_wrap_policy = functools.partial(\n                            size_based_auto_wrap_policy, min_num_params=self.args.fsdp_config[\"fsdp_min_num_params\"]\n                        )\n                    elif self.args.fsdp_config.get(\"fsdp_transformer_layer_cls_to_wrap\", None) is not None:\n                        transformer_cls_to_wrap = set()\n                        for layer_class in self.args.fsdp_config[\"fsdp_transformer_layer_cls_to_wrap\"]:\n                            transformer_cls = get_module_class_from_name(model, layer_class)\n                            if transformer_cls is None:\n                                raise Exception(\"Could not find the transformer layer class to wrap in the model.\")\n                            else:\n                                transformer_cls_to_wrap.add(transformer_cls)\n                        auto_wrap_policy = functools.partial(\n                            transformer_auto_wrap_policy,\n                            # Transformer layer class to wrap\n                            transformer_layer_cls=transformer_cls_to_wrap,\n                        )\n                mixed_precision_policy = None\n                dtype = None\n                if self.args.fp16:\n                    dtype = torch.float16\n                elif self.args.bf16:\n                    dtype = torch.bfloat16\n                if dtype is not None:\n                    mixed_precision_policy = MixedPrecision(param_dtype=dtype, reduce_dtype=dtype, buffer_dtype=dtype)\n                if type(model) != FSDP:\n                    # XXX: Breaking the self.model convention but I see no way around it for now.\n                    self.model = model = FSDP(\n                        model,\n                        sharding_strategy=self.fsdp,\n                        cpu_offload=cpu_offload,\n                        auto_wrap_policy=auto_wrap_policy,\n                        mixed_precision=mixed_precision_policy,\n                        device_id=self.args.device,\n                        backward_prefetch=self.backward_prefetch,\n                        forward_prefetch=self.forword_prefetch,\n                        limit_all_gathers=self.limit_all_gathers,\n                    )\n            else:\n                try:\n                    from torch_xla.distributed.fsdp import XlaFullyShardedDataParallel as FSDP\n                    from torch_xla.distributed.fsdp import checkpoint_module\n                    from torch_xla.distributed.fsdp.wrap import (\n                        size_based_auto_wrap_policy,\n                        transformer_auto_wrap_policy,\n                    )\n                except ImportError:\n                    raise ImportError(\"Missing XLA FSDP related module; please make sure to use torch-xla >= 2.0.\")\n                auto_wrap_policy = None\n                auto_wrapper_callable = None\n                if self.args.fsdp_config[\"fsdp_min_num_params\"] > 0:\n                    auto_wrap_policy = functools.partial(\n                        size_based_auto_wrap_policy, min_num_params=self.args.fsdp_config[\"fsdp_min_num_params\"]\n                    )\n                elif self.args.fsdp_config.get(\"fsdp_transformer_layer_cls_to_wrap\", None) is not None:\n                    transformer_cls_to_wrap = set()\n                    for layer_class in self.args.fsdp_config[\"fsdp_transformer_layer_cls_to_wrap\"]:\n                        transformer_cls = get_module_class_from_name(model, layer_class)\n                        if transformer_cls is None:\n                            raise Exception(\"Could not find the transformer layer class to wrap in the model.\")\n                        else:\n                            transformer_cls_to_wrap.add(transformer_cls)\n                    auto_wrap_policy = functools.partial(\n                        transformer_auto_wrap_policy,\n                        # Transformer layer class to wrap\n                        transformer_layer_cls=transformer_cls_to_wrap,\n                    )\n                fsdp_kwargs = self.args.xla_fsdp_config\n                if self.args.fsdp_config[\"xla_fsdp_grad_ckpt\"]:\n                    # Apply gradient checkpointing to auto-wrapped sub-modules if specified\n                    def auto_wrapper_callable(m, *args, **kwargs):\n                        return FSDP(checkpoint_module(m), *args, **kwargs)\n\n                # Wrap the base model with an outer FSDP wrapper\n                self.model = model = FSDP(\n                    model,\n                    auto_wrap_policy=auto_wrap_policy,\n                    auto_wrapper_callable=auto_wrapper_callable,\n                    **fsdp_kwargs,\n                )\n\n                # Patch `xm.optimizer_step` should not reduce gradients in this case,\n                # as FSDP does not need gradient reduction over sharded parameters.\n                def patched_optimizer_step(optimizer, barrier=False, optimizer_args={}):\n                    loss = optimizer.step(**optimizer_args)\n                    if barrier:\n                        xm.mark_step()\n                    return loss\n\n                xm.optimizer_step = patched_optimizer_step\n        elif is_sagemaker_dp_enabled():\n            model = nn.parallel.DistributedDataParallel(\n                model, device_ids=[int(os.getenv(\"SMDATAPARALLEL_LOCAL_RANK\"))]\n            )\n        elif self.args.local_rank != -1:\n            kwargs = {}\n            if self.args.ddp_find_unused_parameters is not None:\n                kwargs[\"find_unused_parameters\"] = self.args.ddp_find_unused_parameters\n            elif isinstance(model, PreTrainedModel):\n                # find_unused_parameters breaks checkpointing as per\n                # https://github.com/huggingface/transformers/pull/4659#issuecomment-643356021\n                kwargs[\"find_unused_parameters\"] = not model.is_gradient_checkpointing\n            else:\n                kwargs[\"find_unused_parameters\"] = True\n\n            if self.args.ddp_bucket_cap_mb is not None:\n                kwargs[\"bucket_cap_mb\"] = self.args.ddp_bucket_cap_mb\n            if is_torch_neuroncore_available():\n                return model\n            model = nn.parallel.DistributedDataParallel(\n                model,\n                device_ids=[self.args.local_rank] if self.args._n_gpu != 0 else None,\n                output_device=self.args.local_rank if self.args._n_gpu != 0 else None,\n                **kwargs,\n            )\n\n        return model\n\n    def train(\n        self,\n        resume_from_checkpoint: Optional[Union[str, bool]] = None,\n        trial: Union[\"optuna.Trial\", Dict[str, Any]] = None,\n        ignore_keys_for_eval: Optional[List[str]] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Main training entry point.\n\n        Args:\n            resume_from_checkpoint (`str` or `bool`, *optional*):\n                If a `str`, local path to a saved checkpoint as saved by a previous instance of [`Trainer`]. If a\n                `bool` and equals `True`, load the last checkpoint in *args.output_dir* as saved by a previous instance\n                of [`Trainer`]. If present, training will resume from the model/optimizer/scheduler states loaded here.\n            trial (`optuna.Trial` or `Dict[str, Any]`, *optional*):\n                The trial run or the hyperparameter dictionary for hyperparameter search.\n            ignore_keys_for_eval (`List[str]`, *optional*)\n                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n                gathering predictions for evaluation during the training.\n            kwargs:\n                Additional keyword arguments used to hide deprecated arguments\n        \"\"\"\n        if resume_from_checkpoint is False:\n            resume_from_checkpoint = None\n\n        # memory metrics - must set up as early as possible\n        self._memory_tracker.start()\n\n        args = self.args\n\n        self.is_in_train = True\n\n        # do_train is not a reliable argument, as it might not be set and .train() still called, so\n        # the following is a workaround:\n        if (args.fp16_full_eval or args.bf16_full_eval) and not args.do_train:\n            self._move_model_to_device(self.model, args.device)\n\n        if \"model_path\" in kwargs:\n            resume_from_checkpoint = kwargs.pop(\"model_path\")\n            warnings.warn(\n                \"`model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` \"\n                \"instead.\",\n                FutureWarning,\n            )\n        if len(kwargs) > 0:\n            raise TypeError(f\"train() received got unexpected keyword arguments: {', '.join(list(kwargs.keys()))}.\")\n        # This might change the seed so needs to run first.\n        self._hp_search_setup(trial)\n        self._train_batch_size = self.args.train_batch_size\n\n        # Model re-init\n        model_reloaded = False\n        if self.model_init is not None:\n            # Seed must be set before instantiating the model when using model_init.\n            enable_full_determinism(self.args.seed) if self.args.full_determinism else set_seed(self.args.seed)\n            self.model = self.call_model_init(trial)\n            model_reloaded = True\n            # Reinitializes optimizer and scheduler\n            self.optimizer, self.lr_scheduler = None, None\n\n        # Load potential model checkpoint\n        if isinstance(resume_from_checkpoint, bool) and resume_from_checkpoint:\n            resume_from_checkpoint = get_last_checkpoint(args.output_dir)\n            if resume_from_checkpoint is None:\n                raise ValueError(f\"No valid checkpoint found in output directory ({args.output_dir})\")\n\n        if resume_from_checkpoint is not None and not is_sagemaker_mp_enabled() and args.deepspeed is None:\n            self._load_from_checkpoint(resume_from_checkpoint)\n\n        # If model was re-initialized, put it on the right device and update self.model_wrapped\n        if model_reloaded:\n            if self.place_model_on_device:\n                self._move_model_to_device(self.model, args.device)\n            self.model_wrapped = self.model\n\n        inner_training_loop = find_executable_batch_size(\n            self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size\n        )\n        return inner_training_loop(\n            args=args,\n            resume_from_checkpoint=resume_from_checkpoint,\n            trial=trial,\n            ignore_keys_for_eval=ignore_keys_for_eval,\n        )\n\n    def _inner_training_loop(\n        self, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None\n    ):\n        self._train_batch_size = batch_size\n        # Data loader and number of training steps\n        train_dataloader = self.get_train_dataloader()\n\n        # Setting up training control variables:\n        # number of training epochs: num_train_epochs\n        # number of training steps per epoch: num_update_steps_per_epoch\n        # total number of training steps to execute: max_steps\n        total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size\n\n        len_dataloader = None\n        if has_length(train_dataloader):\n            len_dataloader = len(train_dataloader)\n            num_update_steps_per_epoch = len_dataloader // args.gradient_accumulation_steps\n            num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n            num_examples = self.num_examples(train_dataloader)\n            if args.max_steps > 0:\n                max_steps = args.max_steps\n                num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n                    args.max_steps % num_update_steps_per_epoch > 0\n                )\n                # May be slightly incorrect if the last batch in the training dataloader has a smaller size but it's\n                # the best we can do.\n                num_train_samples = args.max_steps * total_train_batch_size\n            else:\n                max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n                num_train_epochs = math.ceil(args.num_train_epochs)\n                num_train_samples = self.num_examples(train_dataloader) * args.num_train_epochs\n        elif args.max_steps > 0:  # Rely on max_steps when dataloader does not have a working size\n            max_steps = args.max_steps\n            # Setting a very large number of epochs so we go as many times as necessary over the iterator.\n            num_train_epochs = sys.maxsize\n            num_update_steps_per_epoch = max_steps\n            num_examples = total_train_batch_size * args.max_steps\n            num_train_samples = args.max_steps * total_train_batch_size\n        else:\n            raise ValueError(\n                \"args.max_steps must be set to a positive value if dataloader does not have a length, was\"\n                f\" {args.max_steps}\"\n            )\n\n        if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:\n            if self.args.n_gpu > 1:\n                # nn.DataParallel(model) replicates the model, creating new variables and module\n                # references registered here no longer work on other gpus, breaking the module\n                raise ValueError(\n                    \"Currently --debug underflow_overflow is not supported under DP. Please use DDP\"\n                    \" (torch.distributed.launch).\"\n                )\n            else:\n                debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n\n        delay_optimizer_creation = (\n            self.sharded_ddp is not None\n            and self.sharded_ddp != ShardedDDPOption.SIMPLE\n            or is_sagemaker_mp_enabled()\n            or self.fsdp is not None\n        )\n        if args.deepspeed:\n            deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(\n                self, num_training_steps=max_steps, resume_from_checkpoint=resume_from_checkpoint\n            )\n            self.model = deepspeed_engine.module\n            self.model_wrapped = deepspeed_engine\n            self.deepspeed = deepspeed_engine\n            self.optimizer = optimizer\n            self.lr_scheduler = lr_scheduler\n        elif not delay_optimizer_creation:\n            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n\n        self.state = TrainerState()\n        self.state.is_hyper_param_search = trial is not None\n\n        # Activate gradient checkpointing if needed\n        if args.gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n\n        model = self._wrap_model(self.model_wrapped)\n\n        if is_sagemaker_mp_enabled() and resume_from_checkpoint is not None:\n            self._load_from_checkpoint(resume_from_checkpoint, model)\n\n        # for the rest of this function `model` is the outside model, whether it was wrapped or not\n        if model is not self.model:\n            self.model_wrapped = model\n\n        if delay_optimizer_creation:\n            self.create_optimizer_and_scheduler(num_training_steps=max_steps)\n\n        # Check if saved optimizer or scheduler states exist\n        self._load_optimizer_and_scheduler(resume_from_checkpoint)\n\n        # important: at this point:\n        # self.model         is the Transformers Model\n        # self.model_wrapped is DDP(Transformers Model), Deepspeed(Transformers Model), etc.\n\n        # Train!\n        logger.info(\"***** Running training *****\")\n        logger.info(f\"  Num examples = {num_examples}\")\n        logger.info(f\"  Num Epochs = {num_train_epochs}\")\n        logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n        logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_train_batch_size}\")\n        logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n        logger.info(f\"  Total optimization steps = {max_steps}\")\n        logger.info(\n            f\"  Number of trainable parameters = {sum(p.numel() for p in model.parameters() if p.requires_grad)}\"\n        )\n\n        self.state.epoch = 0\n        start_time = time.time()\n        epochs_trained = 0\n        steps_trained_in_current_epoch = 0\n        steps_trained_progress_bar = None\n\n        # Check if continuing training from a checkpoint\n        if resume_from_checkpoint is not None and os.path.isfile(\n            os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME)\n        ):\n            self.state = TrainerState.load_from_json(os.path.join(resume_from_checkpoint, TRAINER_STATE_NAME))\n            epochs_trained = self.state.global_step // num_update_steps_per_epoch\n            if not args.ignore_data_skip:\n                steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n                steps_trained_in_current_epoch *= args.gradient_accumulation_steps\n            else:\n                steps_trained_in_current_epoch = 0\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(f\"  Continuing training from epoch {epochs_trained}\")\n            logger.info(f\"  Continuing training from global step {self.state.global_step}\")\n            if not args.ignore_data_skip:\n                if skip_first_batches is None:\n                    logger.info(\n                        f\"  Will skip the first {epochs_trained} epochs then the first\"\n                        f\" {steps_trained_in_current_epoch} batches in the first epoch. If this takes a lot of time,\"\n                        \" you can install the latest version of Accelerate with `pip install -U accelerate`.You can\"\n                        \" also add the `--ignore_data_skip` flag to your launch command, but you will resume the\"\n                        \" training on data already seen by your model.\"\n                    )\n                else:\n                    logger.info(\n                        f\"  Will skip the first {epochs_trained} epochs then the first\"\n                        f\" {steps_trained_in_current_epoch} batches in the first epoch.\"\n                    )\n                if self.is_local_process_zero() and not args.disable_tqdm and skip_first_batches is None:\n                    steps_trained_progress_bar = tqdm(total=steps_trained_in_current_epoch)\n                    steps_trained_progress_bar.set_description(\"Skipping the first batches\")\n\n        # Update the references\n        self.callback_handler.model = self.model\n        self.callback_handler.optimizer = self.optimizer\n        self.callback_handler.lr_scheduler = self.lr_scheduler\n        self.callback_handler.train_dataloader = train_dataloader\n        if self.hp_name is not None and self._trial is not None:\n            # use self._trial because the SigOpt/Optuna hpo only call `_hp_search_setup(trial)` instead of passing trial\n            # parameter to Train when using DDP.\n            self.state.trial_name = self.hp_name(self._trial)\n        if trial is not None:\n            assignments = trial.assignments if self.hp_search_backend == HPSearchBackend.SIGOPT else trial\n            self.state.trial_params = hp_params(assignments)\n        else:\n            self.state.trial_params = None\n        # This should be the same if the state has been saved but in case the training arguments changed, it's safer\n        # to set this after the load.\n        self.state.max_steps = max_steps\n        self.state.num_train_epochs = num_train_epochs\n        self.state.is_local_process_zero = self.is_local_process_zero()\n        self.state.is_world_process_zero = self.is_world_process_zero()\n\n        # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n        tr_loss = torch.tensor(0.0).to(args.device)\n        # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\n        self._total_loss_scalar = 0.0\n        self._globalstep_last_logged = self.state.global_step\n        model.zero_grad()\n\n        self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n\n        # Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\n        if not args.ignore_data_skip:\n            for epoch in range(epochs_trained):\n                is_random_sampler = hasattr(train_dataloader, \"sampler\") and isinstance(\n                    train_dataloader.sampler, RandomSampler\n                )\n                if is_torch_less_than_1_11 or not is_random_sampler:\n                    # We just need to begin an iteration to create the randomization of the sampler.\n                    # That was before PyTorch 1.11 however...\n                    for _ in train_dataloader:\n                        break\n                else:\n                    # Otherwise we need to call the whooooole sampler cause there is some random operation added\n                    # AT THE VERY END!\n                    _ = list(train_dataloader.sampler)\n\n        total_batched_samples = 0\n        for epoch in range(epochs_trained, num_train_epochs):\n            if isinstance(train_dataloader, DataLoader) and isinstance(train_dataloader.sampler, DistributedSampler):\n                train_dataloader.sampler.set_epoch(epoch)\n            elif hasattr(train_dataloader, \"dataset\") and isinstance(train_dataloader.dataset, IterableDatasetShard):\n                train_dataloader.dataset.set_epoch(epoch)\n\n            if is_torch_tpu_available():\n                parallel_loader = pl.ParallelLoader(train_dataloader, [args.device]).per_device_loader(args.device)\n                epoch_iterator = parallel_loader\n            else:\n                epoch_iterator = train_dataloader\n\n            # Reset the past mems state at the beginning of each epoch if necessary.\n            if args.past_index >= 0:\n                self._past = None\n\n            steps_in_epoch = (\n                len(epoch_iterator)\n                if len_dataloader is not None\n                else args.max_steps * args.gradient_accumulation_steps\n            )\n            self.control = self.callback_handler.on_epoch_begin(args, self.state, self.control)\n\n            if epoch == epochs_trained and resume_from_checkpoint is not None and steps_trained_in_current_epoch == 0:\n                self._load_rng_state(resume_from_checkpoint)\n\n            rng_to_sync = False\n            steps_skipped = 0\n            if skip_first_batches is not None and steps_trained_in_current_epoch > 0:\n                epoch_iterator = skip_first_batches(epoch_iterator, steps_trained_in_current_epoch)\n                steps_skipped = steps_trained_in_current_epoch\n                steps_trained_in_current_epoch = 0\n                rng_to_sync = True\n\n            step = -1\n            for step, inputs in enumerate(epoch_iterator):\n                total_batched_samples += 1\n                if rng_to_sync:\n                    self._load_rng_state(resume_from_checkpoint)\n                    rng_to_sync = False\n\n                # Skip past any already trained steps if resuming training\n                if steps_trained_in_current_epoch > 0:\n                    steps_trained_in_current_epoch -= 1\n                    if steps_trained_progress_bar is not None:\n                        steps_trained_progress_bar.update(1)\n                    if steps_trained_in_current_epoch == 0:\n                        self._load_rng_state(resume_from_checkpoint)\n                    continue\n                elif steps_trained_progress_bar is not None:\n                    steps_trained_progress_bar.close()\n                    steps_trained_progress_bar = None\n\n                if step % args.gradient_accumulation_steps == 0:\n                    self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n\n                if (\n                    (total_batched_samples % args.gradient_accumulation_steps != 0)\n                    and args.local_rank != -1\n                    and args._no_sync_in_gradient_accumulation\n                ):\n                    # Avoid unnecessary DDP synchronization since there will be no backward pass on this example.\n                    with model.no_sync():\n                        tr_loss_step = self.training_step(model, inputs)\n                else:\n                    tr_loss_step = self.training_step(model, inputs)\n\n                if (\n                    args.logging_nan_inf_filter\n                    and not is_torch_tpu_available()\n                    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n                ):\n                    # if loss is nan or inf simply add the average of previous logged losses\n                    tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n                else:\n                    tr_loss += tr_loss_step\n\n                self.current_flos += float(self.floating_point_ops(inputs))\n\n                # Optimizer step for deepspeed must be called on every step regardless of the value of gradient_accumulation_steps\n                if self.deepspeed:\n                    self.deepspeed.step()\n\n                if total_batched_samples % args.gradient_accumulation_steps == 0 or (\n                    # last step in epoch but step is always smaller than gradient_accumulation_steps\n                    steps_in_epoch <= args.gradient_accumulation_steps\n                    and (step + 1) == steps_in_epoch\n                ):\n                    # Gradient clipping\n                    if args.max_grad_norm is not None and args.max_grad_norm > 0 and not self.deepspeed:\n                        # deepspeed does its own clipping\n\n                        if self.do_grad_scaling:\n                            # Reduce gradients first for XLA\n                            if is_torch_tpu_available():\n                                gradients = xm._fetch_gradients(self.optimizer)\n                                xm.all_reduce(\"sum\", gradients, scale=1.0 / xm.xrt_world_size())\n                            # AMP: gradients need unscaling\n                            self.scaler.unscale_(self.optimizer)\n\n                        if is_sagemaker_mp_enabled() and args.fp16:\n                            self.optimizer.clip_master_grads(args.max_grad_norm)\n                        elif hasattr(self.optimizer, \"clip_grad_norm\"):\n                            # Some optimizers (like the sharded optimizer) have a specific way to do gradient clipping\n                            self.optimizer.clip_grad_norm(args.max_grad_norm)\n                        elif hasattr(model, \"clip_grad_norm_\"):\n                            # Some models (like FullyShardedDDP) have a specific way to do gradient clipping\n                            model.clip_grad_norm_(args.max_grad_norm)\n                        else:\n                            # Revert to normal clipping otherwise, handling Apex or full precision\n                            nn.utils.clip_grad_norm_(\n                                amp.master_params(self.optimizer) if self.use_apex else model.parameters(),\n                                args.max_grad_norm,\n                            )\n\n                    # Optimizer step\n                    optimizer_was_run = True\n                    if self.deepspeed:\n                        pass  # called outside the loop\n                    elif is_torch_tpu_available():\n                        if self.do_grad_scaling:\n                            self.scaler.step(self.optimizer)\n                            self.scaler.update()\n                        else:\n                            xm.optimizer_step(self.optimizer)\n                    elif self.do_grad_scaling:\n                        scale_before = self.scaler.get_scale()\n                        self.scaler.step(self.optimizer)\n                        self.scaler.update()\n                        scale_after = self.scaler.get_scale()\n                        optimizer_was_run = scale_before <= scale_after\n                    else:\n                        self.optimizer.step()\n\n                    if optimizer_was_run and not self.deepspeed:\n                        self.lr_scheduler.step()\n\n                    model.zero_grad()\n                    self.state.global_step += 1\n                    self.state.epoch = epoch + (step + 1 + steps_skipped) / steps_in_epoch\n                    self.control = self.callback_handler.on_step_end(args, self.state, self.control)\n\n                    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n                else:\n                    self.control = self.callback_handler.on_substep_end(args, self.state, self.control)\n\n                if self.control.should_epoch_stop or self.control.should_training_stop:\n                    break\n            if step < 0:\n                logger.warning(\n                    \"There seems to be not a single sample in your epoch_iterator, stopping training at step\"\n                    f\" {self.state.global_step}! This is expected if you're using an IterableDataset and set\"\n                    f\" num_steps ({max_steps}) higher than the number of available samples.\"\n                )\n                self.control.should_training_stop = True\n\n            self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\n            self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n\n            if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n                if is_torch_tpu_available():\n                    # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n                    xm.master_print(met.metrics_report())\n                else:\n                    logger.warning(\n                        \"You enabled PyTorch/XLA debug metrics but you don't have a TPU \"\n                        \"configured. Check your training configuration if this is unexpected.\"\n                    )\n            if self.control.should_training_stop:\n                break\n\n        if args.past_index and hasattr(self, \"_past\"):\n            # Clean the state at the end of training\n            delattr(self, \"_past\")\n\n        logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n        if args.load_best_model_at_end and self.state.best_model_checkpoint is not None:\n            # Wait for everyone to get here so we are sur the model has been saved by process 0.\n            if is_torch_tpu_available():\n                xm.rendezvous(\"load_best_model_at_end\")\n            elif args.local_rank != -1:\n                dist.barrier()\n            elif is_sagemaker_mp_enabled():\n                smp.barrier()\n\n            self._load_best_model()\n\n        # add remaining tr_loss\n        self._total_loss_scalar += tr_loss.item()\n        train_loss = self._total_loss_scalar / self.state.global_step\n\n        metrics = speed_metrics(\"train\", start_time, num_samples=num_train_samples, num_steps=self.state.max_steps)\n        self.store_flos()\n        metrics[\"total_flos\"] = self.state.total_flos\n        metrics[\"train_loss\"] = train_loss\n\n        self.is_in_train = False\n\n        self._memory_tracker.stop_and_update_metrics(metrics)\n\n        self.log(metrics)\n\n        run_dir = self._get_output_dir(trial)\n        checkpoints_sorted = self._sorted_checkpoints(use_mtime=False, output_dir=run_dir)\n\n        # Delete the last checkpoint when save_total_limit=1 if it's different from the best checkpoint and process allowed to save.\n        if self.args.should_save and self.state.best_model_checkpoint is not None and self.args.save_total_limit == 1:\n            for checkpoint in checkpoints_sorted:\n                if checkpoint != self.state.best_model_checkpoint:\n                    logger.info(f\"Deleting older checkpoint [{checkpoint}] due to args.save_total_limit\")\n                    shutil.rmtree(checkpoint)\n\n        self.control = self.callback_handler.on_train_end(args, self.state, self.control)\n\n        return TrainOutput(self.state.global_step, train_loss, metrics)\n\n    def _get_output_dir(self, trial):\n        if self.hp_search_backend is not None and trial is not None:\n            if self.hp_search_backend == HPSearchBackend.OPTUNA:\n                run_id = trial.number\n            elif self.hp_search_backend == HPSearchBackend.RAY:\n                from ray import tune\n\n                run_id = tune.get_trial_id()\n            elif self.hp_search_backend == HPSearchBackend.SIGOPT:\n                run_id = trial.id\n            elif self.hp_search_backend == HPSearchBackend.WANDB:\n                import wandb\n\n                run_id = wandb.run.id\n            run_name = self.hp_name(trial) if self.hp_name is not None else f\"run-{run_id}\"\n            run_dir = os.path.join(self.args.output_dir, run_name)\n        else:\n            run_dir = self.args.output_dir\n        return run_dir\n\n    def _load_from_checkpoint(self, resume_from_checkpoint, model=None):\n        if model is None:\n            model = self.model\n\n        if not os.path.isfile(os.path.join(resume_from_checkpoint, WEIGHTS_NAME)) and not os.path.isfile(\n            os.path.join(resume_from_checkpoint, WEIGHTS_INDEX_NAME)\n        ):\n            raise ValueError(f\"Can't find a valid checkpoint at {resume_from_checkpoint}\")\n\n        logger.info(f\"Loading model from {resume_from_checkpoint}.\")\n\n        if os.path.isfile(os.path.join(resume_from_checkpoint, CONFIG_NAME)):\n            config = PretrainedConfig.from_json_file(os.path.join(resume_from_checkpoint, CONFIG_NAME))\n            checkpoint_version = config.transformers_version\n            if checkpoint_version is not None and checkpoint_version != __version__:\n                logger.warning(\n                    f\"You are resuming training from a checkpoint trained with {checkpoint_version} of \"\n                    f\"Transformers but your current version is {__version__}. This is not recommended and could \"\n                    \"yield to errors or unwanted behaviors.\"\n                )\n\n        if os.path.isfile(os.path.join(resume_from_checkpoint, WEIGHTS_NAME)):\n            # If the model is on the GPU, it still works!\n            if is_sagemaker_mp_enabled():\n                if os.path.isfile(os.path.join(resume_from_checkpoint, \"user_content.pt\")):\n                    # If the 'user_content.pt' file exists, load with the new smp api.\n                    # Checkpoint must have been saved with the new smp api.\n                    smp.resume_from_checkpoint(\n                        path=resume_from_checkpoint, tag=WEIGHTS_NAME, partial=False, load_optimizer=False\n                    )\n                else:\n                    # If the 'user_content.pt' file does NOT exist, load with the old smp api.\n                    # Checkpoint must have been saved with the old smp api.\n                    if hasattr(self.args, \"fp16\") and self.args.fp16 is True:\n                        logger.warning(\n                            \"Enabling FP16 and loading from smp < 1.10 checkpoint together is not suppported.\"\n                        )\n                    state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location=\"cpu\")\n                    # Required for smp to not auto-translate state_dict from hf to smp (is already smp).\n                    state_dict[\"_smp_is_partial\"] = False\n                    load_result = model.load_state_dict(state_dict, strict=True)\n                    # release memory\n                    del state_dict\n            else:\n                # We load the model state dict on the CPU to avoid an OOM error.\n                state_dict = torch.load(os.path.join(resume_from_checkpoint, WEIGHTS_NAME), map_location=\"cpu\")\n                # workaround for FSDP bug https://github.com/pytorch/pytorch/issues/82963\n                # which takes *args instead of **kwargs\n                load_result = model.load_state_dict(state_dict, False)\n                # release memory\n                del state_dict\n                self._issue_warnings_after_load(load_result)\n        else:\n            # We load the sharded checkpoint\n            load_result = load_sharded_checkpoint(model, resume_from_checkpoint, strict=is_sagemaker_mp_enabled())\n            if not is_sagemaker_mp_enabled():\n                self._issue_warnings_after_load(load_result)\n\n    def _load_best_model(self):\n        logger.info(f\"Loading best model from {self.state.best_model_checkpoint} (score: {self.state.best_metric}).\")\n        best_model_path = os.path.join(self.state.best_model_checkpoint, WEIGHTS_NAME)\n        model = self.model_wrapped if is_sagemaker_mp_enabled() else self.model\n        if os.path.exists(best_model_path):\n            if self.deepspeed:\n                if self.model_wrapped is not None:\n                    # this removes the pre-hooks from the previous engine\n                    self.model_wrapped.destroy()\n                    self.model_wrapped = None\n\n                # temp hack until Deepspeed fixes the problem with resume from an existing engine that did some stepping\n                deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(\n                    self,\n                    num_training_steps=self.args.max_steps,\n                    resume_from_checkpoint=self.state.best_model_checkpoint,\n                )\n                self.model = deepspeed_engine.module\n                self.model_wrapped = deepspeed_engine\n                self.deepspeed = deepspeed_engine\n                self.optimizer = optimizer\n                self.lr_scheduler = lr_scheduler\n            else:\n                if is_sagemaker_mp_enabled():\n                    if os.path.isfile(os.path.join(self.state.best_model_checkpoint, \"user_content.pt\")):\n                        # If the 'user_content.pt' file exists, load with the new smp api.\n                        # Checkpoint must have been saved with the new smp api.\n                        smp.resume_from_checkpoint(\n                            path=self.state.best_model_checkpoint,\n                            tag=WEIGHTS_NAME,\n                            partial=False,\n                            load_optimizer=False,\n                        )\n                    else:\n                        # If the 'user_content.pt' file does NOT exist, load with the old smp api.\n                        # Checkpoint must have been saved with the old smp api.\n                        state_dict = torch.load(best_model_path, map_location=\"cpu\")\n                        state_dict[\"_smp_is_partial\"] = False\n                        load_result = model.load_state_dict(state_dict, strict=True)\n                else:\n                    # We load the model state dict on the CPU to avoid an OOM error.\n                    state_dict = torch.load(best_model_path, map_location=\"cpu\")\n                    # If the model is on the GPU, it still works!\n                    # workaround for FSDP bug https://github.com/pytorch/pytorch/issues/82963\n                    # which takes *args instead of **kwargs\n                    load_result = model.load_state_dict(state_dict, False)\n                if not is_sagemaker_mp_enabled():\n                    self._issue_warnings_after_load(load_result)\n        elif os.path.exists(os.path.join(self.state.best_model_checkpoint, WEIGHTS_INDEX_NAME)):\n            load_result = load_sharded_checkpoint(\n                model, self.state.best_model_checkpoint, strict=is_sagemaker_mp_enabled()\n            )\n            if not is_sagemaker_mp_enabled():\n                self._issue_warnings_after_load(load_result)\n        else:\n            logger.warning(\n                f\"Could not locate the best model at {best_model_path}, if you are running a distributed training \"\n                \"on multiple nodes, you should activate `--save_on_each_node`.\"\n            )\n\n    def _issue_warnings_after_load(self, load_result):\n        if len(load_result.missing_keys) != 0:\n            if self.model._keys_to_ignore_on_save is not None and set(load_result.missing_keys) == set(\n                self.model._keys_to_ignore_on_save\n            ):\n                self.model.tie_weights()\n            else:\n                logger.warning(f\"There were missing keys in the checkpoint model loaded: {load_result.missing_keys}.\")\n        if len(load_result.unexpected_keys) != 0:\n            logger.warning(\n                f\"There were unexpected keys in the checkpoint model loaded: {load_result.unexpected_keys}.\"\n            )\n\n    def _maybe_log_save_evaluate(self, tr_loss, model, trial, epoch, ignore_keys_for_eval):\n        if self.control.should_log:\n            if is_torch_tpu_available():\n                xm.mark_step()\n\n            logs: Dict[str, float] = {}\n\n            # all_gather + mean() to get average loss over all processes\n            tr_loss_scalar = self._nested_gather(tr_loss).mean().item()\n\n            # reset tr_loss to zero\n            tr_loss -= tr_loss\n\n            logs[\"loss\"] = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)\n            logs[\"learning_rate\"] = self._get_learning_rate()\n\n            self._total_loss_scalar += tr_loss_scalar\n            self._globalstep_last_logged = self.state.global_step\n            self.store_flos()\n\n            self.log(logs)\n\n        metrics = None\n        if self.control.should_evaluate:\n            if isinstance(self.eval_dataset, dict):\n                for eval_dataset_name, eval_dataset in self.eval_dataset.items():\n                    metrics = self.evaluate(\n                        eval_dataset=eval_dataset,\n                        ignore_keys=ignore_keys_for_eval,\n                        metric_key_prefix=f\"eval_{eval_dataset_name}\",\n                    )\n            else:\n                metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n            self._report_to_hp_search(trial, self.state.global_step, metrics)\n\n        if self.control.should_save:\n            self._save_checkpoint(model, trial, metrics=metrics)\n            self.control = self.callback_handler.on_save(self.args, self.state, self.control)\n\n    def _load_rng_state(self, checkpoint):\n        # Load RNG states from `checkpoint`\n        if checkpoint is None:\n            return\n\n        if self.args.world_size > 1:\n            process_index = self.args.process_index\n            rng_file = os.path.join(checkpoint, f\"rng_state_{process_index}.pth\")\n            if not os.path.isfile(rng_file):\n                logger.info(\n                    f\"Didn't find an RNG file for process {process_index}, if you are resuming a training that \"\n                    \"wasn't launched in a distributed fashion, reproducibility is not guaranteed.\"\n                )\n                return\n        else:\n            rng_file = os.path.join(checkpoint, \"rng_state.pth\")\n            if not os.path.isfile(rng_file):\n                logger.info(\n                    \"Didn't find an RNG file, if you are resuming a training that was launched in a distributed \"\n                    \"fashion, reproducibility is not guaranteed.\"\n                )\n                return\n\n        checkpoint_rng_state = torch.load(rng_file)\n        random.setstate(checkpoint_rng_state[\"python\"])\n        np.random.set_state(checkpoint_rng_state[\"numpy\"])\n        torch.random.set_rng_state(checkpoint_rng_state[\"cpu\"])\n        if torch.cuda.is_available():\n            if self.args.local_rank != -1:\n                torch.cuda.random.set_rng_state(checkpoint_rng_state[\"cuda\"])\n            else:\n                try:\n                    torch.cuda.random.set_rng_state_all(checkpoint_rng_state[\"cuda\"])\n                except Exception as e:\n                    logger.info(\n                        f\"Didn't manage to set back the RNG states of the GPU because of the following error:\\n {e}\"\n                        \"\\nThis won't yield the same results as if the training had not been interrupted.\"\n                    )\n        if is_torch_tpu_available():\n            xm.set_rng_state(checkpoint_rng_state[\"xla\"])\n\n    def _save_checkpoint(self, model, trial, metrics=None):\n        # In all cases, including ddp/dp/deepspeed, self.model is always a reference to the model we\n        # want to save except FullyShardedDDP.\n        # assert unwrap_model(model) is self.model, \"internal model should be a reference to self.model\"\n\n        # Save model checkpoint\n        checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\"\n\n        if self.hp_search_backend is None and trial is None:\n            self.store_flos()\n\n        run_dir = self._get_output_dir(trial=trial)\n        output_dir = os.path.join(run_dir, checkpoint_folder)\n        self.save_model(output_dir, _internal_call=True)\n        if self.deepspeed:\n            # under zero3 model file itself doesn't get saved since it's bogus! Unless deepspeed\n            # config `stage3_gather_16bit_weights_on_model_save` is True\n            self.deepspeed.save_checkpoint(output_dir)\n\n        # Save optimizer and scheduler\n        if self.sharded_ddp == ShardedDDPOption.SIMPLE:\n            self.optimizer.consolidate_state_dict()\n\n        if is_torch_tpu_available():\n            xm.rendezvous(\"saving_optimizer_states\")\n            xm.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))\n            with warnings.catch_warnings(record=True) as caught_warnings:\n                xm.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, SCHEDULER_NAME))\n                reissue_pt_warnings(caught_warnings)\n        elif is_sagemaker_mp_enabled():\n            opt_state_dict = self.optimizer.local_state_dict(gather_if_shard=False)\n            smp.barrier()\n            if smp.rdp_rank() == 0 or smp.state.cfg.shard_optimizer_state:\n                smp.save(\n                    opt_state_dict,\n                    os.path.join(output_dir, OPTIMIZER_NAME),\n                    partial=True,\n                    v3=smp.state.cfg.shard_optimizer_state,\n                )\n            if self.args.should_save:\n                with warnings.catch_warnings(record=True) as caught_warnings:\n                    torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, SCHEDULER_NAME))\n                reissue_pt_warnings(caught_warnings)\n                if self.do_grad_scaling:\n                    torch.save(self.scaler.state_dict(), os.path.join(output_dir, SCALER_NAME))\n        elif self.args.should_save and not self.deepspeed:\n            # deepspeed.save_checkpoint above saves model/optim/sched\n            torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))\n            with warnings.catch_warnings(record=True) as caught_warnings:\n                torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, SCHEDULER_NAME))\n            reissue_pt_warnings(caught_warnings)\n            if self.do_grad_scaling:\n                torch.save(self.scaler.state_dict(), os.path.join(output_dir, SCALER_NAME))\n\n        # Determine the new best metric / best model checkpoint\n        if metrics is not None and self.args.metric_for_best_model is not None:\n            metric_to_check = self.args.metric_for_best_model\n            if not metric_to_check.startswith(\"eval_\"):\n                metric_to_check = f\"eval_{metric_to_check}\"\n            metric_value = metrics[metric_to_check]\n\n            operator = np.greater if self.args.greater_is_better else np.less\n            if (\n                self.state.best_metric is None\n                or self.state.best_model_checkpoint is None\n                or operator(metric_value, self.state.best_metric)\n            ):\n                self.state.best_metric = metric_value\n                self.state.best_model_checkpoint = output_dir\n\n        # Save the Trainer state\n        if self.args.should_save:\n            self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))\n\n        # Save RNG state in non-distributed training\n        rng_states = {\n            \"python\": random.getstate(),\n            \"numpy\": np.random.get_state(),\n            \"cpu\": torch.random.get_rng_state(),\n        }\n        if torch.cuda.is_available():\n            if self.args.local_rank == -1:\n                # In non distributed, we save the global CUDA RNG state (will take care of DataParallel)\n                rng_states[\"cuda\"] = torch.cuda.random.get_rng_state_all()\n            else:\n                rng_states[\"cuda\"] = torch.cuda.random.get_rng_state()\n\n        if is_torch_tpu_available():\n            rng_states[\"xla\"] = xm.get_rng_state()\n\n        # A process can arrive here before the process 0 has a chance to save the model, in which case output_dir may\n        # not yet exist.\n        os.makedirs(output_dir, exist_ok=True)\n\n        if self.args.world_size <= 1:\n            torch.save(rng_states, os.path.join(output_dir, \"rng_state.pth\"))\n        else:\n            torch.save(rng_states, os.path.join(output_dir, f\"rng_state_{self.args.process_index}.pth\"))\n\n        if self.args.push_to_hub:\n            self._push_from_checkpoint(output_dir)\n\n        # Maybe delete some older checkpoints.\n        if self.args.should_save:\n            self._rotate_checkpoints(use_mtime=True, output_dir=run_dir)\n\n    def _load_optimizer_and_scheduler(self, checkpoint):\n        \"\"\"If optimizer and scheduler states exist, load them.\"\"\"\n        if checkpoint is None:\n            return\n\n        if self.deepspeed:\n            # deepspeed loads optimizer/lr_scheduler together with the model in deepspeed_init\n            return\n\n        checkpoint_file_exists = (\n            glob.glob(os.path.join(checkpoint, OPTIMIZER_NAME) + \"_*\")\n            if is_sagemaker_mp_enabled()\n            else os.path.isfile(os.path.join(checkpoint, OPTIMIZER_NAME))\n        )\n        if checkpoint_file_exists and os.path.isfile(os.path.join(checkpoint, SCHEDULER_NAME)):\n            # Load in optimizer and scheduler states\n            if is_torch_tpu_available():\n                # On TPU we have to take some extra precautions to properly load the states on the right device.\n                optimizer_state = torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=\"cpu\")\n                with warnings.catch_warnings(record=True) as caught_warnings:\n                    lr_scheduler_state = torch.load(os.path.join(checkpoint, SCHEDULER_NAME), map_location=\"cpu\")\n                reissue_pt_warnings(caught_warnings)\n\n                xm.send_cpu_data_to_device(optimizer_state, self.args.device)\n                xm.send_cpu_data_to_device(lr_scheduler_state, self.args.device)\n\n                self.optimizer.load_state_dict(optimizer_state)\n                self.lr_scheduler.load_state_dict(lr_scheduler_state)\n            else:\n                map_location = \"cpu\" if is_sagemaker_mp_enabled() else self.args.device\n                if is_sagemaker_mp_enabled():\n                    if os.path.isfile(os.path.join(checkpoint, \"user_content.pt\")):\n                        # Optimizer checkpoint was saved with smp >= 1.10\n                        def opt_load_hook(mod, opt):\n                            opt.load_state_dict(smp.load(os.path.join(checkpoint, OPTIMIZER_NAME), partial=True))\n\n                    else:\n                        # Optimizer checkpoint was saved with smp < 1.10\n                        def opt_load_hook(mod, opt):\n                            if IS_SAGEMAKER_MP_POST_1_10:\n                                opt.load_state_dict(\n                                    smp.load(os.path.join(checkpoint, OPTIMIZER_NAME), partial=True, back_compat=True)\n                                )\n                            else:\n                                opt.load_state_dict(smp.load(os.path.join(checkpoint, OPTIMIZER_NAME), partial=True))\n\n                    self.model_wrapped.register_post_step_hook(opt_load_hook)\n                else:\n                    self.optimizer.load_state_dict(\n                        torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n                    )\n                with warnings.catch_warnings(record=True) as caught_warnings:\n                    self.lr_scheduler.load_state_dict(torch.load(os.path.join(checkpoint, SCHEDULER_NAME)))\n                reissue_pt_warnings(caught_warnings)\n                if self.do_grad_scaling and os.path.isfile(os.path.join(checkpoint, SCALER_NAME)):\n                    self.scaler.load_state_dict(torch.load(os.path.join(checkpoint, SCALER_NAME)))\n\n    def hyperparameter_search(\n        self,\n        hp_space: Optional[Callable[[\"optuna.Trial\"], Dict[str, float]]] = None,\n        compute_objective: Optional[Callable[[Dict[str, float]], float]] = None,\n        n_trials: int = 20,\n        direction: str = \"minimize\",\n        backend: Optional[Union[\"str\", HPSearchBackend]] = None,\n        hp_name: Optional[Callable[[\"optuna.Trial\"], str]] = None,\n        **kwargs,\n    ) -> BestRun:\n        \"\"\"\n        Launch an hyperparameter search using `optuna` or `Ray Tune` or `SigOpt`. The optimized quantity is determined\n        by `compute_objective`, which defaults to a function returning the evaluation loss when no metric is provided,\n        the sum of all metrics otherwise.\n\n        <Tip warning={true}>\n\n        To use this method, you need to have provided a `model_init` when initializing your [`Trainer`]: we need to\n        reinitialize the model at each new run. This is incompatible with the `optimizers` argument, so you need to\n        subclass [`Trainer`] and override the method [`~Trainer.create_optimizer_and_scheduler`] for custom\n        optimizer/scheduler.\n\n        </Tip>\n\n        Args:\n            hp_space (`Callable[[\"optuna.Trial\"], Dict[str, float]]`, *optional*):\n                A function that defines the hyperparameter search space. Will default to\n                [`~trainer_utils.default_hp_space_optuna`] or [`~trainer_utils.default_hp_space_ray`] or\n                [`~trainer_utils.default_hp_space_sigopt`] depending on your backend.\n            compute_objective (`Callable[[Dict[str, float]], float]`, *optional*):\n                A function computing the objective to minimize or maximize from the metrics returned by the `evaluate`\n                method. Will default to [`~trainer_utils.default_compute_objective`].\n            n_trials (`int`, *optional*, defaults to 100):\n                The number of trial runs to test.\n            direction (`str`, *optional*, defaults to `\"minimize\"`):\n                Whether to optimize greater or lower objects. Can be `\"minimize\"` or `\"maximize\"`, you should pick\n                `\"minimize\"` when optimizing the validation loss, `\"maximize\"` when optimizing one or several metrics.\n            backend (`str` or [`~training_utils.HPSearchBackend`], *optional*):\n                The backend to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt, depending\n                on which one is installed. If all are installed, will default to optuna.\n            hp_name (`Callable[[\"optuna.Trial\"], str]]`, *optional*):\n                A function that defines the trial/run name. Will default to None.\n            kwargs (`Dict[str, Any]`, *optional*):\n                Additional keyword arguments passed along to `optuna.create_study` or `ray.tune.run`. For more\n                information see:\n\n                - the documentation of\n                  [optuna.create_study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html)\n                - the documentation of [tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run)\n                - the documentation of [sigopt](https://app.sigopt.com/docs/endpoints/experiments/create)\n\n        Returns:\n            [`trainer_utils.BestRun`]: All the information about the best run. Experiment summary can be found in\n            `run_summary` attribute for Ray backend.\n        \"\"\"\n        if backend is None:\n            backend = default_hp_search_backend()\n            if backend is None:\n                raise RuntimeError(\n                    \"At least one of optuna or ray should be installed. \"\n                    \"To install optuna run `pip install optuna`. \"\n                    \"To install ray run `pip install ray[tune]`. \"\n                    \"To install sigopt run `pip install sigopt`.\"\n                )\n        backend = HPSearchBackend(backend)\n        if backend == HPSearchBackend.OPTUNA and not is_optuna_available():\n            raise RuntimeError(\"You picked the optuna backend, but it is not installed. Use `pip install optuna`.\")\n        if backend == HPSearchBackend.RAY and not is_ray_tune_available():\n            raise RuntimeError(\n                \"You picked the Ray Tune backend, but it is not installed. Use `pip install 'ray[tune]'`.\"\n            )\n        if backend == HPSearchBackend.SIGOPT and not is_sigopt_available():\n            raise RuntimeError(\"You picked the sigopt backend, but it is not installed. Use `pip install sigopt`.\")\n        if backend == HPSearchBackend.WANDB and not is_wandb_available():\n            raise RuntimeError(\"You picked the wandb backend, but it is not installed. Use `pip install wandb`.\")\n        self.hp_search_backend = backend\n        if self.model_init is None:\n            raise RuntimeError(\n                \"To use hyperparameter search, you need to pass your model through a model_init function.\"\n            )\n\n        self.hp_space = default_hp_space[backend] if hp_space is None else hp_space\n        self.hp_name = hp_name\n        self.compute_objective = default_compute_objective if compute_objective is None else compute_objective\n\n        backend_dict = {\n            HPSearchBackend.OPTUNA: run_hp_search_optuna,\n            HPSearchBackend.RAY: run_hp_search_ray,\n            HPSearchBackend.SIGOPT: run_hp_search_sigopt,\n            HPSearchBackend.WANDB: run_hp_search_wandb,\n        }\n        best_run = backend_dict[backend](self, n_trials, direction, **kwargs)\n\n        self.hp_search_backend = None\n        return best_run\n\n    def log(self, logs: Dict[str, float]) -> None:\n        \"\"\"\n        Log `logs` on the various objects watching training.\n\n        Subclass and override this method to inject custom behavior.\n\n        Args:\n            logs (`Dict[str, float]`):\n                The values to log.\n        \"\"\"\n        if self.state.epoch is not None:\n            logs[\"epoch\"] = round(self.state.epoch, 2)\n\n        output = {**logs, **{\"step\": self.state.global_step}}\n        self.state.log_history.append(output)\n        self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n\n    def _prepare_input(self, data: Union[torch.Tensor, Any]) -> Union[torch.Tensor, Any]:\n        \"\"\"\n        Prepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.\n        \"\"\"\n        if isinstance(data, Mapping):\n            return type(data)({k: self._prepare_input(v) for k, v in data.items()})\n        elif isinstance(data, (tuple, list)):\n            return type(data)(self._prepare_input(v) for v in data)\n        elif isinstance(data, torch.Tensor):\n            kwargs = {\"device\": self.args.device}\n            if self.deepspeed and (torch.is_floating_point(data) or torch.is_complex(data)):\n                # NLP models inputs are int/uint and those get adjusted to the right dtype of the\n                # embedding. Other models such as wav2vec2's inputs are already float and thus\n                # may need special handling to match the dtypes of the model\n                kwargs.update({\"dtype\": self.args.hf_deepspeed_config.dtype()})\n            return data.to(**kwargs)\n        return data\n\n    def _prepare_inputs(self, inputs: Dict[str, Union[torch.Tensor, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:\n        \"\"\"\n        Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and\n        handling potential state.\n        \"\"\"\n        inputs = self._prepare_input(inputs)\n        if len(inputs) == 0:\n            raise ValueError(\n                \"The batch received was empty, your model won't be able to train on it. Double-check that your \"\n                f\"training dataset contains keys expected by the model: {','.join(self._signature_columns)}.\"\n            )\n        if self.args.past_index >= 0 and self._past is not None:\n            inputs[\"mems\"] = self._past\n\n        return inputs\n\n    def compute_loss_context_manager(self):\n        \"\"\"\n        A helper wrapper to group together context managers.\n        \"\"\"\n        return self.autocast_smart_context_manager()\n\n    def autocast_smart_context_manager(self, cache_enabled: Optional[bool] = True):\n        \"\"\"\n        A helper wrapper that creates an appropriate context manager for `autocast` while feeding it the desired\n        arguments, depending on the situation.\n        \"\"\"\n        if self.use_cuda_amp or self.use_cpu_amp:\n            if is_torch_greater_or_equal_than_1_10:\n                ctx_manager = (\n                    torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n                    if self.use_cpu_amp\n                    else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n                )\n            else:\n                ctx_manager = torch.cuda.amp.autocast()\n        else:\n            ctx_manager = contextlib.nullcontext() if sys.version_info >= (3, 7) else contextlib.suppress()\n\n        return ctx_manager\n\n    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        \"\"\"\n        Perform a training step on a batch of inputs.\n\n        Subclass and override to inject custom behavior.\n\n        Args:\n            model (`nn.Module`):\n                The model to train.\n            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n\n                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n                argument `labels`. Check your model's documentation for all accepted arguments.\n\n        Return:\n            `torch.Tensor`: The tensor with training loss on this batch.\n        \"\"\"\n        model.train()\n        inputs = self._prepare_inputs(inputs)\n\n        if is_sagemaker_mp_enabled():\n            loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n            return loss_mb.reduce_mean().detach().to(self.args.device)\n\n        with self.compute_loss_context_manager():\n            loss = self.compute_loss(model, inputs)\n\n        if self.args.n_gpu > 1:\n            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n\n        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:\n            # deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\n            loss = loss / self.args.gradient_accumulation_steps\n\n        if self.do_grad_scaling:\n            self.scaler.scale(loss).backward()\n        elif self.use_apex:\n            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                scaled_loss.backward()\n        elif self.deepspeed:\n            # loss gets scaled under gradient_accumulation_steps in deepspeed\n            loss = self.deepspeed.backward(loss)\n        else:\n            loss.backward()\n\n        return loss.detach()\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        \"\"\"\n        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n\n        Subclass and override for custom behavior.\n        \"\"\"\n        if self.label_smoother is not None and \"labels\" in inputs:\n            labels = inputs.pop(\"labels\")\n        else:\n            labels = None\n        outputs = model(**inputs)\n        # Save past state if it exists\n        # TODO: this needs to be fixed and made cleaner later.\n        if self.args.past_index >= 0:\n            self._past = outputs[self.args.past_index]\n\n        if labels is not None:\n            if unwrap_model(model)._get_name() in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n                loss = self.label_smoother(outputs, labels, shift_labels=True)\n            else:\n                loss = self.label_smoother(outputs, labels)\n        else:\n            if isinstance(outputs, dict) and \"loss\" not in outputs:\n                raise ValueError(\n                    \"The model did not return a loss from the inputs, only the following keys: \"\n                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n                )\n            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n\n        return (loss, outputs) if return_outputs else loss\n\n    def is_local_process_zero(self) -> bool:\n        \"\"\"\n        Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several\n        machines) main process.\n        \"\"\"\n        return self.args.local_process_index == 0\n\n    def is_world_process_zero(self) -> bool:\n        \"\"\"\n        Whether or not this process is the global main process (when training in a distributed fashion on several\n        machines, this is only going to be `True` for one process).\n        \"\"\"\n        # Special case for SageMaker ModelParallel since there process_index is dp_process_index, not the global\n        # process index.\n        if is_sagemaker_mp_enabled():\n            return smp.rank() == 0\n        else:\n            return self.args.process_index == 0\n\n    def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False):\n        \"\"\"\n        Will save the model, so you can reload it using `from_pretrained()`.\n\n        Will only save from the main process.\n        \"\"\"\n\n        if output_dir is None:\n            output_dir = self.args.output_dir\n\n        if is_torch_tpu_available():\n            self._save_tpu(output_dir)\n        elif is_sagemaker_mp_enabled():\n            # Calling the state_dict needs to be done on the wrapped model and on all processes.\n            os.makedirs(output_dir, exist_ok=True)\n            state_dict = self.model_wrapped.state_dict()\n            if self.args.should_save:\n                self._save(output_dir, state_dict=state_dict)\n            if IS_SAGEMAKER_MP_POST_1_10:\n                # 'user_content.pt' indicates model state_dict saved with smp >= 1.10\n                Path(os.path.join(output_dir, \"user_content.pt\")).touch()\n        elif (\n            ShardedDDPOption.ZERO_DP_2 in self.args.sharded_ddp\n            or ShardedDDPOption.ZERO_DP_3 in self.args.sharded_ddp\n            or self.fsdp is not None\n        ):\n            state_dict = self.model.state_dict()\n\n            if self.args.should_save:\n                self._save(output_dir, state_dict=state_dict)\n        elif self.deepspeed:\n            # this takes care of everything as long as we aren't under zero3\n            if self.args.should_save:\n                self._save(output_dir)\n\n            if is_deepspeed_zero3_enabled():\n                # It's too complicated to try to override different places where the weights dump gets\n                # saved, so since under zero3 the file is bogus, simply delete it. The user should\n                # either user deepspeed checkpoint to resume or to recover full weights use\n                # zero_to_fp32.py stored in the checkpoint.\n                if self.args.should_save:\n                    file = os.path.join(output_dir, WEIGHTS_NAME)\n                    if os.path.isfile(file):\n                        # logger.info(f\"deepspeed zero3: removing {file}, see zero_to_fp32.py to recover weights\")\n                        os.remove(file)\n\n                # now save the real model if stage3_gather_16bit_weights_on_model_save=True\n                # if false it will not be saved.\n                # This must be called on all ranks\n                if not self.deepspeed.save_16bit_model(output_dir, WEIGHTS_NAME):\n                    logger.warning(\n                        \"deepspeed.save_16bit_model didn't save the model, since\"\n                        \" stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use\"\n                        \" zero_to_fp32.py to recover weights\"\n                    )\n                    self.deepspeed.save_checkpoint(output_dir)\n\n        elif self.args.should_save:\n            self._save(output_dir)\n\n        # Push to the Hub when `save_model` is called by the user.\n        if self.args.push_to_hub and not _internal_call:\n            self.push_to_hub(commit_message=\"Model save\")\n\n    def _save_tpu(self, output_dir: Optional[str] = None):\n        output_dir = output_dir if output_dir is not None else self.args.output_dir\n        logger.info(f\"Saving model checkpoint to {output_dir}\")\n\n        if xm.is_master_ordinal():\n            os.makedirs(output_dir, exist_ok=True)\n            torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n\n        # Save a trained model and configuration using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        xm.rendezvous(\"saving_checkpoint\")\n        if not isinstance(self.model, PreTrainedModel):\n            if isinstance(unwrap_model(self.model), PreTrainedModel):\n                unwrap_model(self.model).save_pretrained(\n                    output_dir,\n                    is_main_process=self.args.should_save,\n                    state_dict=self.model.state_dict(),\n                    save_function=xm.save,\n                )\n            else:\n                logger.info(\"Trainer.model is not a `PreTrainedModel`, only saving its state dict.\")\n                state_dict = self.model.state_dict()\n                xm.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n        else:\n            self.model.save_pretrained(output_dir, is_main_process=self.args.should_save, save_function=xm.save)\n        if self.tokenizer is not None and self.args.should_save:\n            self.tokenizer.save_pretrained(output_dir)\n\n    def _save(self, output_dir: Optional[str] = None, state_dict=None):\n        # If we are executing this function, we are the process zero, so we don't check for that.\n        output_dir = output_dir if output_dir is not None else self.args.output_dir\n        os.makedirs(output_dir, exist_ok=True)\n        logger.info(f\"Saving model checkpoint to {output_dir}\")\n        # Save a trained model and configuration using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        if not isinstance(self.model, PreTrainedModel):\n            if isinstance(unwrap_model(self.model), PreTrainedModel):\n                if state_dict is None:\n                    state_dict = self.model.state_dict()\n                unwrap_model(self.model).save_pretrained(output_dir, state_dict=filtered_state_dict)\n            else:\n                logger.info(\"Trainer.model is not a `PreTrainedModel`, only saving its state dict.\")\n                if state_dict is None:\n                    state_dict = self.model.state_dict()\n                torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n        else:\n            if self.save_prefixencoder:\n                print(\"Saving PrefixEncoder\")\n                state_dict = self.model.state_dict()\n                filtered_state_dict = {}\n                for k, v in self.model.named_parameters():\n                    if v.requires_grad:\n                        filtered_state_dict[k] = state_dict[k]\n                self.model.save_pretrained(output_dir, state_dict=filtered_state_dict)\n            else:\n                print(\"Saving the whole model\")\n                self.model.save_pretrained(output_dir, state_dict=state_dict)\n        if self.tokenizer is not None:\n            self.tokenizer.save_pretrained(output_dir)\n\n        # Good practice: save your training arguments together with the trained model\n        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n\n    def store_flos(self):\n        # Storing the number of floating-point operations that went into the model\n        if self.args.local_rank != -1:\n            self.state.total_flos += (\n                distributed_broadcast_scalars([self.current_flos], device=self.args.device).sum().item()\n            )\n            self.current_flos = 0\n        else:\n            self.state.total_flos += self.current_flos\n            self.current_flos = 0\n\n    def _sorted_checkpoints(\n        self, output_dir=None, checkpoint_prefix=PREFIX_CHECKPOINT_DIR, use_mtime=False\n    ) -> List[str]:\n        ordering_and_checkpoint_path = []\n\n        glob_checkpoints = [str(x) for x in Path(output_dir).glob(f\"{checkpoint_prefix}-*\") if os.path.isdir(x)]\n\n        for path in glob_checkpoints:\n            if use_mtime:\n                ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n            else:\n                regex_match = re.match(f\".*{checkpoint_prefix}-([0-9]+)\", path)\n                if regex_match is not None and regex_match.groups() is not None:\n                    ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n\n        checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n        checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n        # Make sure we don't delete the best model.\n        if self.state.best_model_checkpoint is not None:\n            best_model_index = checkpoints_sorted.index(str(Path(self.state.best_model_checkpoint)))\n            for i in range(best_model_index, len(checkpoints_sorted) - 2):\n                checkpoints_sorted[i], checkpoints_sorted[i + 1] = checkpoints_sorted[i + 1], checkpoints_sorted[i]\n        return checkpoints_sorted\n\n    def _rotate_checkpoints(self, use_mtime=False, output_dir=None) -> None:\n        if self.args.save_total_limit is None or self.args.save_total_limit <= 0:\n            return\n\n        # Check if we should delete older checkpoint(s)\n        checkpoints_sorted = self._sorted_checkpoints(use_mtime=use_mtime, output_dir=output_dir)\n        if len(checkpoints_sorted) <= self.args.save_total_limit:\n            return\n\n        # If save_total_limit=1 with load_best_model_at_end=True, we could end up deleting the last checkpoint, which\n        # we don't do to allow resuming.\n        save_total_limit = self.args.save_total_limit\n        if (\n            self.state.best_model_checkpoint is not None\n            and self.args.save_total_limit == 1\n            and checkpoints_sorted[-1] != self.state.best_model_checkpoint\n        ):\n            save_total_limit = 2\n\n        number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - save_total_limit)\n        checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n        for checkpoint in checkpoints_to_be_deleted:\n            logger.info(f\"Deleting older checkpoint [{checkpoint}] due to args.save_total_limit\")\n            shutil.rmtree(checkpoint, ignore_errors=True)\n\n    def evaluate(\n        self,\n        eval_dataset: Optional[Dataset] = None,\n        ignore_keys: Optional[List[str]] = None,\n        metric_key_prefix: str = \"eval\",\n    ) -> Dict[str, float]:\n        \"\"\"\n        Run evaluation and returns metrics.\n\n        The calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n        (pass it to the init `compute_metrics` argument).\n\n        You can also subclass and override this method to inject custom behavior.\n\n        Args:\n            eval_dataset (`Dataset`, *optional*):\n                Pass a dataset if you wish to override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns\n                not accepted by the `model.forward()` method are automatically removed. It must implement the `__len__`\n                method.\n            ignore_keys (`Lst[str]`, *optional*):\n                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n                gathering predictions.\n            metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n                An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n                \"eval_bleu\" if the prefix is \"eval\" (default)\n\n        Returns:\n            A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n            dictionary also contains the epoch number which comes from the training state.\n        \"\"\"\n        # memory metrics - must set up as early as possible\n        self._memory_tracker.start()\n\n        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n        start_time = time.time()\n\n        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n        output = eval_loop(\n            eval_dataloader,\n            description=\"Evaluation\",\n            # No point gathering the predictions if there are no metrics, otherwise we defer to\n            # self.args.prediction_loss_only\n            prediction_loss_only=True if self.compute_metrics is None else None,\n            ignore_keys=ignore_keys,\n            metric_key_prefix=metric_key_prefix,\n        )\n\n        total_batch_size = self.args.eval_batch_size * self.args.world_size\n        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n        output.metrics.update(\n            speed_metrics(\n                metric_key_prefix,\n                start_time,\n                num_samples=output.num_samples,\n                num_steps=math.ceil(output.num_samples / total_batch_size),\n            )\n        )\n\n        self.log(output.metrics)\n\n        if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n            # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n            xm.master_print(met.metrics_report())\n\n        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, output.metrics)\n\n        self._memory_tracker.stop_and_update_metrics(output.metrics)\n\n        return output.metrics\n\n    def predict(\n        self, test_dataset: Dataset, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = \"test\"\n    ) -> PredictionOutput:\n        \"\"\"\n        Run prediction and returns predictions and potential metrics.\n\n        Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method\n        will also return metrics, like in `evaluate()`.\n\n        Args:\n            test_dataset (`Dataset`):\n                Dataset to run the predictions on. If it is an `datasets.Dataset`, columns not accepted by the\n                `model.forward()` method are automatically removed. Has to implement the method `__len__`\n            ignore_keys (`Lst[str]`, *optional*):\n                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n                gathering predictions.\n            metric_key_prefix (`str`, *optional*, defaults to `\"test\"`):\n                An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n                \"test_bleu\" if the prefix is \"test\" (default)\n\n        <Tip>\n\n        If your predictions or labels have different sequence length (for instance because you're doing dynamic padding\n        in a token classification task) the predictions will be padded (on the right) to allow for concatenation into\n        one array. The padding index is -100.\n\n        </Tip>\n\n        Returns: *NamedTuple* A namedtuple with the following keys:\n\n            - predictions (`np.ndarray`): The predictions on `test_dataset`.\n            - label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained some).\n            - metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics (if the dataset contained\n              labels).\n        \"\"\"\n        # memory metrics - must set up as early as possible\n        self._memory_tracker.start()\n\n        test_dataloader = self.get_test_dataloader(test_dataset)\n        start_time = time.time()\n\n        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n        output = eval_loop(\n            test_dataloader, description=\"Prediction\", ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix\n        )\n        total_batch_size = self.args.eval_batch_size * self.args.world_size\n        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n        output.metrics.update(\n            speed_metrics(\n                metric_key_prefix,\n                start_time,\n                num_samples=output.num_samples,\n                num_steps=math.ceil(output.num_samples / total_batch_size),\n            )\n        )\n\n        self.control = self.callback_handler.on_predict(self.args, self.state, self.control, output.metrics)\n        self._memory_tracker.stop_and_update_metrics(output.metrics)\n\n        return PredictionOutput(predictions=output.predictions, label_ids=output.label_ids, metrics=output.metrics)\n\n    def evaluation_loop(\n        self,\n        dataloader: DataLoader,\n        description: str,\n        prediction_loss_only: Optional[bool] = None,\n        ignore_keys: Optional[List[str]] = None,\n        metric_key_prefix: str = \"eval\",\n    ) -> EvalLoopOutput:\n        \"\"\"\n        Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n\n        Works both with or without labels.\n        \"\"\"\n        args = self.args\n\n        prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n\n        # if eval is called w/o train init deepspeed here\n        if args.deepspeed and not self.deepspeed:\n            # XXX: eval doesn't have `resume_from_checkpoint` arg but we should be able to do eval\n            # from the checkpoint eventually\n            deepspeed_engine, _, _ = deepspeed_init(\n                self, num_training_steps=0, resume_from_checkpoint=None, inference=True\n            )\n            self.model = deepspeed_engine.module\n            self.model_wrapped = deepspeed_engine\n            self.deepspeed = deepspeed_engine\n\n        model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n\n        # if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn't called\n        # while ``train`` is running, cast it to the right dtype first and then put on device\n        if not self.is_in_train:\n            if args.fp16_full_eval:\n                model = model.to(dtype=torch.float16, device=args.device)\n            elif args.bf16_full_eval:\n                model = model.to(dtype=torch.bfloat16, device=args.device)\n\n        batch_size = self.args.eval_batch_size\n\n        logger.info(f\"***** Running {description} *****\")\n        if has_length(dataloader):\n            logger.info(f\"  Num examples = {self.num_examples(dataloader)}\")\n        else:\n            logger.info(\"  Num examples: Unknown\")\n        logger.info(f\"  Batch size = {batch_size}\")\n\n        model.eval()\n\n        self.callback_handler.eval_dataloader = dataloader\n        # Do this before wrapping.\n        eval_dataset = getattr(dataloader, \"dataset\", None)\n\n        if is_torch_tpu_available():\n            dataloader = pl.ParallelLoader(dataloader, [args.device]).per_device_loader(args.device)\n\n        if args.past_index >= 0:\n            self._past = None\n\n        # Initialize containers\n        # losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)\n        losses_host = None\n        preds_host = None\n        labels_host = None\n        inputs_host = None\n\n        # losses/preds/labels on CPU (final containers)\n        all_losses = None\n        all_preds = None\n        all_labels = None\n        all_inputs = None\n        # Will be useful when we have an iterable dataset so don't know its length.\n\n        observed_num_examples = 0\n        # Main evaluation loop\n        for step, inputs in enumerate(dataloader):\n            # Update the observed num examples\n            observed_batch_size = find_batch_size(inputs)\n            if observed_batch_size is not None:\n                observed_num_examples += observed_batch_size\n                # For batch samplers, batch_size is not known by the dataloader in advance.\n                if batch_size is None:\n                    batch_size = observed_batch_size\n\n            # Prediction step\n            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n            inputs_decode = self._prepare_input(inputs[\"input_ids\"]) if args.include_inputs_for_metrics else None\n\n            if is_torch_tpu_available():\n                xm.mark_step()\n\n            # Update containers on host\n            if loss is not None:\n                losses = self._nested_gather(loss.repeat(batch_size))\n                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)\n            if labels is not None:\n                labels = self._pad_across_processes(labels)\n                labels = self._nested_gather(labels)\n                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)\n            if inputs_decode is not None:\n                inputs_decode = self._pad_across_processes(inputs_decode)\n                inputs_decode = self._nested_gather(inputs_decode)\n                inputs_host = (\n                    inputs_decode\n                    if inputs_host is None\n                    else nested_concat(inputs_host, inputs_decode, padding_index=-100)\n                )\n            if logits is not None:\n                logits = self._pad_across_processes(logits)\n                logits = self._nested_gather(logits)\n                if self.preprocess_logits_for_metrics is not None:\n                    logits = self.preprocess_logits_for_metrics(logits, labels)\n                preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\n            self.control = self.callback_handler.on_prediction_step(args, self.state, self.control)\n\n            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\n            if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:\n                if losses_host is not None:\n                    losses = nested_numpify(losses_host)\n                    all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n                if preds_host is not None:\n                    logits = nested_numpify(preds_host)\n                    all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n                if inputs_host is not None:\n                    inputs_decode = nested_numpify(inputs_host)\n                    all_inputs = (\n                        inputs_decode\n                        if all_inputs is None\n                        else nested_concat(all_inputs, inputs_decode, padding_index=-100)\n                    )\n                if labels_host is not None:\n                    labels = nested_numpify(labels_host)\n                    all_labels = (\n                        labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n                    )\n\n                # Set back to None to begin a new accumulation\n                losses_host, preds_host, inputs_host, labels_host = None, None, None, None\n\n        if args.past_index and hasattr(self, \"_past\"):\n            # Clean the state at the end of the evaluation loop\n            delattr(self, \"_past\")\n\n        # Gather all remaining tensors and put them back on the CPU\n        if losses_host is not None:\n            losses = nested_numpify(losses_host)\n            all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n        if preds_host is not None:\n            logits = nested_numpify(preds_host)\n            all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n        if inputs_host is not None:\n            inputs_decode = nested_numpify(inputs_host)\n            all_inputs = (\n                inputs_decode if all_inputs is None else nested_concat(all_inputs, inputs_decode, padding_index=-100)\n            )\n        if labels_host is not None:\n            labels = nested_numpify(labels_host)\n            all_labels = labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n\n        # Number of samples\n        if has_length(eval_dataset):\n            num_samples = len(eval_dataset)\n        # The instance check is weird and does not actually check for the type, but whether the dataset has the right\n        # methods. Therefore we need to make sure it also has the attribute.\n        elif isinstance(eval_dataset, IterableDatasetShard) and getattr(eval_dataset, \"num_examples\", 0) > 0:\n            num_samples = eval_dataset.num_examples\n        else:\n            if has_length(dataloader):\n                num_samples = self.num_examples(dataloader)\n            else:  # both len(dataloader.dataset) and len(dataloader) fail\n                num_samples = observed_num_examples\n        if num_samples == 0 and observed_num_examples > 0:\n            num_samples = observed_num_examples\n\n        # Number of losses has been rounded to a multiple of batch_size and in a distributed training, the number of\n        # samplers has been rounded to a multiple of batch_size, so we truncate.\n        if all_losses is not None:\n            all_losses = all_losses[:num_samples]\n        if all_preds is not None:\n            all_preds = nested_truncate(all_preds, num_samples)\n        if all_labels is not None:\n            all_labels = nested_truncate(all_labels, num_samples)\n        if all_inputs is not None:\n            all_inputs = nested_truncate(all_inputs, num_samples)\n\n        # Metrics!\n        if self.compute_metrics is not None and all_preds is not None and all_labels is not None:\n            if args.include_inputs_for_metrics:\n                metrics = self.compute_metrics(\n                    EvalPrediction(predictions=all_preds, label_ids=all_labels, inputs=all_inputs)\n                )\n            else:\n                metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))\n        else:\n            metrics = {}\n\n        # To be JSON-serializable, we need to remove numpy types or zero-d tensors\n        metrics = denumpify_detensorize(metrics)\n\n        if all_losses is not None:\n            metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n        if hasattr(self, \"jit_compilation_time\"):\n            metrics[f\"{metric_key_prefix}_jit_compilation_time\"] = self.jit_compilation_time\n\n        # Prefix all keys with metric_key_prefix + '_'\n        for key in list(metrics.keys()):\n            if not key.startswith(f\"{metric_key_prefix}_\"):\n                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n\n        return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=num_samples)\n\n    def _nested_gather(self, tensors, name=None):\n        \"\"\"\n        Gather value of `tensors` (tensor or list/tuple of nested tensors) and convert them to numpy before\n        concatenating them to `gathered`\n        \"\"\"\n        if tensors is None:\n            return\n        if is_torch_tpu_available():\n            if name is None:\n                name = \"nested_gather\"\n            tensors = nested_xla_mesh_reduce(tensors, name)\n        elif is_sagemaker_mp_enabled():\n            tensors = smp_gather(tensors)\n        elif self.args.local_rank != -1:\n            tensors = distributed_concat(tensors)\n        return tensors\n\n    # Copied from Accelerate.\n    def _pad_across_processes(self, tensor, pad_index=-100):\n        \"\"\"\n        Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\n        they can safely be gathered.\n        \"\"\"\n        if isinstance(tensor, (list, tuple)):\n            return type(tensor)(self._pad_across_processes(t, pad_index=pad_index) for t in tensor)\n        elif isinstance(tensor, dict):\n            return type(tensor)({k: self._pad_across_processes(v, pad_index=pad_index) for k, v in tensor.items()})\n        elif not isinstance(tensor, torch.Tensor):\n            raise TypeError(\n                f\"Can't pad the values of type {type(tensor)}, only of nested list/tuple/dicts of tensors.\"\n            )\n\n        if len(tensor.shape) < 2:\n            return tensor\n        # Gather all sizes\n        size = torch.tensor(tensor.shape, device=tensor.device)[None]\n        sizes = self._nested_gather(size).cpu()\n\n        max_size = max(s[1] for s in sizes)\n        # When extracting XLA graphs for compilation, max_size is 0,\n        # so use inequality to avoid errors.\n        if tensor.shape[1] >= max_size:\n            return tensor\n\n        # Then pad to the maximum size\n        old_size = tensor.shape\n        new_size = list(old_size)\n        new_size[1] = max_size\n        new_tensor = tensor.new_zeros(tuple(new_size)) + pad_index\n        new_tensor[:, : old_size[1]] = tensor\n        return new_tensor\n\n    def prediction_step(\n        self,\n        model: nn.Module,\n        inputs: Dict[str, Union[torch.Tensor, Any]],\n        prediction_loss_only: bool,\n        ignore_keys: Optional[List[str]] = None,\n    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n        \"\"\"\n        Perform an evaluation step on `model` using `inputs`.\n\n        Subclass and override to inject custom behavior.\n\n        Args:\n            model (`nn.Module`):\n                The model to evaluate.\n            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n\n                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n                argument `labels`. Check your model's documentation for all accepted arguments.\n            prediction_loss_only (`bool`):\n                Whether or not to return the loss only.\n            ignore_keys (`Lst[str]`, *optional*):\n                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n                gathering predictions.\n\n        Return:\n            Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n            logits and labels (each being optional).\n        \"\"\"\n        has_labels = False if len(self.label_names) == 0 else all(inputs.get(k) is not None for k in self.label_names)\n        # For CLIP-like models capable of returning loss values.\n        # If `return_loss` is not specified or being `None` in `inputs`, we check if the default value of `return_loss`\n        # is `True` in `model.forward`.\n        return_loss = inputs.get(\"return_loss\", None)\n        if return_loss is None:\n            return_loss = self.can_return_loss\n        loss_without_labels = True if len(self.label_names) == 0 and return_loss else False\n\n        inputs = self._prepare_inputs(inputs)\n        if ignore_keys is None:\n            if hasattr(self.model, \"config\"):\n                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n            else:\n                ignore_keys = []\n\n        # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.\n        if has_labels or loss_without_labels:\n            labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))\n            if len(labels) == 1:\n                labels = labels[0]\n        else:\n            labels = None\n\n        with torch.no_grad():\n            if is_sagemaker_mp_enabled():\n                raw_outputs = smp_forward_only(model, inputs)\n                if has_labels or loss_without_labels:\n                    if isinstance(raw_outputs, dict):\n                        loss_mb = raw_outputs[\"loss\"]\n                        logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys + [\"loss\"])\n                    else:\n                        loss_mb = raw_outputs[0]\n                        logits_mb = raw_outputs[1:]\n\n                    loss = loss_mb.reduce_mean().detach().cpu()\n                    logits = smp_nested_concat(logits_mb)\n                else:\n                    loss = None\n                    if isinstance(raw_outputs, dict):\n                        logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys)\n                    else:\n                        logits_mb = raw_outputs\n                    logits = smp_nested_concat(logits_mb)\n            else:\n                if has_labels or loss_without_labels:\n                    with self.compute_loss_context_manager():\n                        loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n                    loss = loss.mean().detach()\n\n                    if isinstance(outputs, dict):\n                        logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + [\"loss\"])\n                    else:\n                        logits = outputs[1:]\n                else:\n                    loss = None\n                    with self.compute_loss_context_manager():\n                        outputs = model(**inputs)\n                    if isinstance(outputs, dict):\n                        logits = tuple(v for k, v in outputs.items() if k not in ignore_keys)\n                    else:\n                        logits = outputs\n                    # TODO: this needs to be fixed and made cleaner later.\n                    if self.args.past_index >= 0:\n                        self._past = outputs[self.args.past_index - 1]\n\n        if prediction_loss_only:\n            return (loss, None, None)\n\n        logits = nested_detach(logits)\n        if len(logits) == 1:\n            logits = logits[0]\n\n        return (loss, logits, labels)\n\n    def floating_point_ops(self, inputs: Dict[str, Union[torch.Tensor, Any]]):\n        \"\"\"\n        For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\n        operations for every backward + forward pass. If using another model, either implement such a method in the\n        model or subclass and override this method.\n\n        Args:\n            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n\n        Returns:\n            `int`: The number of floating-point operations.\n        \"\"\"\n        if hasattr(self.model, \"floating_point_ops\"):\n            return self.model.floating_point_ops(inputs)\n        else:\n            return 0\n\n    def init_git_repo(self, at_init: bool = False):\n        \"\"\"\n        Initializes a git repo in `self.args.hub_model_id`.\n\n        Args:\n            at_init (`bool`, *optional*, defaults to `False`):\n                Whether this function is called before any training or not. If `self.args.overwrite_output_dir` is\n                `True` and `at_init` is `True`, the path to the repo (which is `self.args.output_dir`) might be wiped\n                out.\n        \"\"\"\n        if not self.is_world_process_zero():\n            return\n        if self.args.hub_model_id is None:\n            repo_name = Path(self.args.output_dir).absolute().name\n        else:\n            repo_name = self.args.hub_model_id\n        if \"/\" not in repo_name:\n            repo_name = get_full_repo_name(repo_name, token=self.args.hub_token)\n\n        # Make sure the repo exists.\n        create_repo(repo_name, token=self.args.hub_token, private=self.args.hub_private_repo, exist_ok=True)\n        try:\n            self.repo = Repository(self.args.output_dir, clone_from=repo_name, token=self.args.hub_token)\n        except EnvironmentError:\n            if self.args.overwrite_output_dir and at_init:\n                # Try again after wiping output_dir\n                shutil.rmtree(self.args.output_dir)\n                self.repo = Repository(self.args.output_dir, clone_from=repo_name, token=self.args.hub_token)\n            else:\n                raise\n\n        self.repo.git_pull()\n\n        # By default, ignore the checkpoint folders\n        if (\n            not os.path.exists(os.path.join(self.args.output_dir, \".gitignore\"))\n            and self.args.hub_strategy != HubStrategy.ALL_CHECKPOINTS\n        ):\n            with open(os.path.join(self.args.output_dir, \".gitignore\"), \"w\", encoding=\"utf-8\") as writer:\n                writer.writelines([\"checkpoint-*/\"])\n\n        # Add \"*.sagemaker\" to .gitignore if using SageMaker\n        if os.environ.get(\"SM_TRAINING_ENV\"):\n            self._add_sm_patterns_to_gitignore()\n\n        self.push_in_progress = None\n\n    def create_model_card(\n        self,\n        language: Optional[str] = None,\n        license: Optional[str] = None,\n        tags: Union[str, List[str], None] = None,\n        model_name: Optional[str] = None,\n        finetuned_from: Optional[str] = None,\n        tasks: Union[str, List[str], None] = None,\n        dataset_tags: Union[str, List[str], None] = None,\n        dataset: Union[str, List[str], None] = None,\n        dataset_args: Union[str, List[str], None] = None,\n    ):\n        \"\"\"\n        Creates a draft of a model card using the information available to the `Trainer`.\n\n        Args:\n            language (`str`, *optional*):\n                The language of the model (if applicable)\n            license (`str`, *optional*):\n                The license of the model. Will default to the license of the pretrained model used, if the original\n                model given to the `Trainer` comes from a repo on the Hub.\n            tags (`str` or `List[str]`, *optional*):\n                Some tags to be included in the metadata of the model card.\n            model_name (`str`, *optional*):\n                The name of the model.\n            finetuned_from (`str`, *optional*):\n                The name of the model used to fine-tune this one (if applicable). Will default to the name of the repo\n                of the original model given to the `Trainer` (if it comes from the Hub).\n            tasks (`str` or `List[str]`, *optional*):\n                One or several task identifiers, to be included in the metadata of the model card.\n            dataset_tags (`str` or `List[str]`, *optional*):\n                One or several dataset tags, to be included in the metadata of the model card.\n            dataset (`str` or `List[str]`, *optional*):\n                One or several dataset identifiers, to be included in the metadata of the model card.\n            dataset_args (`str` or `List[str]`, *optional*):\n               One or several dataset arguments, to be included in the metadata of the model card.\n        \"\"\"\n        if not self.is_world_process_zero():\n            return\n\n        training_summary = TrainingSummary.from_trainer(\n            self,\n            language=language,\n            license=license,\n            tags=tags,\n            model_name=model_name,\n            finetuned_from=finetuned_from,\n            tasks=tasks,\n            dataset_tags=dataset_tags,\n            dataset=dataset,\n            dataset_args=dataset_args,\n        )\n        model_card = training_summary.to_model_card()\n        with open(os.path.join(self.args.output_dir, \"README.md\"), \"w\") as f:\n            f.write(model_card)\n\n    def _push_from_checkpoint(self, checkpoint_folder):\n        # Only push from one node.\n        if not self.is_world_process_zero() or self.args.hub_strategy == HubStrategy.END:\n            return\n        # If we haven't finished the last push, we don't do this one.\n        if self.push_in_progress is not None and not self.push_in_progress.is_done:\n            return\n\n        output_dir = self.args.output_dir\n        # To avoid a new synchronization of all model weights, we just copy the file from the checkpoint folder\n        modeling_files = [CONFIG_NAME, WEIGHTS_NAME]\n        for modeling_file in modeling_files:\n            if os.path.isfile(os.path.join(checkpoint_folder, modeling_file)):\n                shutil.copy(os.path.join(checkpoint_folder, modeling_file), os.path.join(output_dir, modeling_file))\n        # Saving the tokenizer is fast and we don't know how many files it may have spawned, so we resave it to be sure.\n        if self.tokenizer is not None:\n            self.tokenizer.save_pretrained(output_dir)\n        # Same for the training arguments\n        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n\n        try:\n            if self.args.hub_strategy == HubStrategy.CHECKPOINT:\n                # Temporarily move the checkpoint just saved for the push\n                tmp_checkpoint = os.path.join(output_dir, \"last-checkpoint\")\n                # We have to remove the \"last-checkpoint\" dir if it exists, otherwise the checkpoint is moved as a\n                # subfolder.\n                if os.path.isdir(tmp_checkpoint):\n                    shutil.rmtree(tmp_checkpoint)\n                shutil.move(checkpoint_folder, tmp_checkpoint)\n\n            if self.args.save_strategy == IntervalStrategy.STEPS:\n                commit_message = f\"Training in progress, step {self.state.global_step}\"\n            else:\n                commit_message = f\"Training in progress, epoch {int(self.state.epoch)}\"\n            _, self.push_in_progress = self.repo.push_to_hub(\n                commit_message=commit_message, blocking=False, auto_lfs_prune=True\n            )\n        finally:\n            if self.args.hub_strategy == HubStrategy.CHECKPOINT:\n                # Move back the checkpoint to its place\n                shutil.move(tmp_checkpoint, checkpoint_folder)\n\n    def push_to_hub(self, commit_message: Optional[str] = \"End of training\", blocking: bool = True, **kwargs) -> str:\n        \"\"\"\n        Upload *self.model* and *self.tokenizer* to the  model hub on the repo *self.args.hub_model_id*.\n\n        Parameters:\n            commit_message (`str`, *optional*, defaults to `\"End of training\"`):\n                Message to commit while pushing.\n            blocking (`bool`, *optional*, defaults to `True`):\n                Whether the function should return only when the `git push` has finished.\n            kwargs:\n                Additional keyword arguments passed along to [`~Trainer.create_model_card`].\n\n        Returns:\n            The url of the commit of your model in the given repository if `blocking=False`, a tuple with the url of\n            the commit and an object to track the progress of the commit if `blocking=True`\n        \"\"\"\n        # If a user calls manually `push_to_hub` with `self.args.push_to_hub = False`, we try to create the repo but\n        # it might fail.\n        if not hasattr(self, \"repo\"):\n            self.init_git_repo()\n\n        model_name = kwargs.pop(\"model_name\", None)\n        if model_name is None and self.args.should_save:\n            if self.args.hub_model_id is None:\n                model_name = Path(self.args.output_dir).name\n            else:\n                model_name = self.args.hub_model_id.split(\"/\")[-1]\n\n        # Needs to be executed on all processes for TPU training, but will only save on the processed determined by\n        # self.args.should_save.\n        self.save_model(_internal_call=True)\n\n        # Only push from one node.\n        if not self.is_world_process_zero():\n            return\n\n        # Cancel any async push in progress if blocking=True. The commits will all be pushed together.\n        if blocking and self.push_in_progress is not None and not self.push_in_progress.is_done:\n            self.push_in_progress._process.kill()\n            self.push_in_progress = None\n\n        git_head_commit_url = self.repo.push_to_hub(\n            commit_message=commit_message, blocking=blocking, auto_lfs_prune=True\n        )\n        # push separately the model card to be independant from the rest of the model\n        if self.args.should_save:\n            self.create_model_card(model_name=model_name, **kwargs)\n            try:\n                self.repo.push_to_hub(\n                    commit_message=\"update model card README.md\", blocking=blocking, auto_lfs_prune=True\n                )\n            except EnvironmentError as exc:\n                logger.error(f\"Error pushing update to the model card. Please read logs and retry.\\n${exc}\")\n\n        return git_head_commit_url\n\n    #\n    # Deprecated code\n    #\n\n    def prediction_loop(\n        self,\n        dataloader: DataLoader,\n        description: str,\n        prediction_loss_only: Optional[bool] = None,\n        ignore_keys: Optional[List[str]] = None,\n        metric_key_prefix: str = \"eval\",\n    ) -> EvalLoopOutput:\n        \"\"\"\n        Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n\n        Works both with or without labels.\n        \"\"\"\n        args = self.args\n\n        if not has_length(dataloader):\n            raise ValueError(\"dataloader must implement a working __len__\")\n\n        prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n\n        # if eval is called w/o train init deepspeed here\n        if args.deepspeed and not self.deepspeed:\n            # XXX: eval doesn't have `resume_from_checkpoint` arg but we should be able to do eval\n            # from the checkpoint eventually\n            deepspeed_engine, _, _ = deepspeed_init(self, num_training_steps=0, resume_from_checkpoint=None)\n            self.model = deepspeed_engine.module\n            self.model_wrapped = deepspeed_engine\n            self.deepspeed = deepspeed_engine\n            # XXX: we don't need optim/sched for inference, but this needs to be sorted out, since\n            # for example the Z3-optimizer is a must for zero3 to work even for inference - what we\n            # don't need is the deepspeed basic optimizer which is self.optimizer.optimizer\n            deepspeed_engine.optimizer.optimizer = None\n            deepspeed_engine.lr_scheduler = None\n\n        model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n\n        # if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn't called\n        # while ``train`` is running, cast it to the right dtype first and then put on device\n        if not self.is_in_train:\n            if args.fp16_full_eval:\n                model = model.to(dtype=torch.float16, device=args.device)\n            elif args.bf16_full_eval:\n                model = model.to(dtype=torch.bfloat16, device=args.device)\n\n        batch_size = dataloader.batch_size\n        num_examples = self.num_examples(dataloader)\n        logger.info(f\"***** Running {description} *****\")\n        logger.info(f\"  Num examples = {num_examples}\")\n        logger.info(f\"  Batch size = {batch_size}\")\n        losses_host: torch.Tensor = None\n        preds_host: Union[torch.Tensor, List[torch.Tensor]] = None\n        labels_host: Union[torch.Tensor, List[torch.Tensor]] = None\n        inputs_host: Union[torch.Tensor, List[torch.Tensor]] = None\n\n        world_size = max(1, args.world_size)\n\n        eval_losses_gatherer = DistributedTensorGatherer(world_size, num_examples, make_multiple_of=batch_size)\n        if not prediction_loss_only:\n            # The actual number of eval_sample can be greater than num_examples in distributed settings (when we pass\n            # a batch size to the sampler)\n            make_multiple_of = None\n            if hasattr(dataloader, \"sampler\") and isinstance(dataloader.sampler, SequentialDistributedSampler):\n                make_multiple_of = dataloader.sampler.batch_size\n            preds_gatherer = DistributedTensorGatherer(world_size, num_examples, make_multiple_of=make_multiple_of)\n            labels_gatherer = DistributedTensorGatherer(world_size, num_examples, make_multiple_of=make_multiple_of)\n            inputs_gatherer = DistributedTensorGatherer(world_size, num_examples, make_multiple_of=make_multiple_of)\n\n        model.eval()\n\n        if is_torch_tpu_available():\n            dataloader = pl.ParallelLoader(dataloader, [args.device]).per_device_loader(args.device)\n\n        if args.past_index >= 0:\n            self._past = None\n\n        self.callback_handler.eval_dataloader = dataloader\n\n        for step, inputs in enumerate(dataloader):\n            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n            inputs_decode = self._prepare_input(inputs[\"input_ids\"]) if args.include_inputs_for_metrics else None\n\n            if loss is not None:\n                losses = loss.repeat(batch_size)\n                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)\n            if logits is not None:\n                preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\n            if labels is not None:\n                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)\n            if inputs_decode is not None:\n                inputs_host = (\n                    inputs_decode\n                    if inputs_host is None\n                    else nested_concat(inputs_host, inputs_decode, padding_index=-100)\n                )\n            self.control = self.callback_handler.on_prediction_step(args, self.state, self.control)\n\n            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\n            if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:\n                eval_losses_gatherer.add_arrays(self._gather_and_numpify(losses_host, \"eval_losses\"))\n                if not prediction_loss_only:\n                    preds_gatherer.add_arrays(self._gather_and_numpify(preds_host, \"eval_preds\"))\n                    labels_gatherer.add_arrays(self._gather_and_numpify(labels_host, \"eval_label_ids\"))\n                    inputs_gatherer.add_arrays(self._gather_and_numpify(inputs_host, \"eval_inputs_ids\"))\n\n                # Set back to None to begin a new accumulation\n                losses_host, preds_host, labels_host, inputs_host = None, None, None, None\n\n        if args.past_index and hasattr(self, \"_past\"):\n            # Clean the state at the end of the evaluation loop\n            delattr(self, \"_past\")\n\n        # Gather all remaining tensors and put them back on the CPU\n        eval_losses_gatherer.add_arrays(self._gather_and_numpify(losses_host, \"eval_losses\"))\n        if not prediction_loss_only:\n            preds_gatherer.add_arrays(self._gather_and_numpify(preds_host, \"eval_preds\"))\n            labels_gatherer.add_arrays(self._gather_and_numpify(labels_host, \"eval_label_ids\"))\n            inputs_gatherer.add_arrays(self._gather_and_numpify(inputs_host, \"eval_inputs_ids\"))\n\n        eval_loss = eval_losses_gatherer.finalize()\n        preds = preds_gatherer.finalize() if not prediction_loss_only else None\n        label_ids = labels_gatherer.finalize() if not prediction_loss_only else None\n        inputs_ids = inputs_gatherer.finalize() if not prediction_loss_only else None\n\n        if self.compute_metrics is not None and preds is not None and label_ids is not None:\n            if args.include_inputs_for_metrics:\n                metrics = self.compute_metrics(\n                    EvalPrediction(predictions=preds, label_ids=label_ids, inputs=inputs_ids)\n                )\n            else:\n                metrics = self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))\n        else:\n            metrics = {}\n\n        # To be JSON-serializable, we need to remove numpy types or zero-d tensors\n        metrics = denumpify_detensorize(metrics)\n\n        if eval_loss is not None:\n            metrics[f\"{metric_key_prefix}_loss\"] = eval_loss.mean().item()\n\n        # Prefix all keys with metric_key_prefix + '_'\n        for key in list(metrics.keys()):\n            if not key.startswith(f\"{metric_key_prefix}_\"):\n                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n\n        return EvalLoopOutput(predictions=preds, label_ids=label_ids, metrics=metrics, num_samples=num_examples)\n\n    def _gather_and_numpify(self, tensors, name):\n        \"\"\"\n        Gather value of `tensors` (tensor or list/tuple of nested tensors) and convert them to numpy before\n        concatenating them to `gathered`\n        \"\"\"\n        if tensors is None:\n            return\n        if is_torch_tpu_available():\n            tensors = nested_xla_mesh_reduce(tensors, name)\n        elif is_sagemaker_mp_enabled():\n            tensors = smp_gather(tensors)\n        elif self.args.local_rank != -1:\n            tensors = distributed_concat(tensors)\n\n        return nested_numpify(tensors)\n\n    def _add_sm_patterns_to_gitignore(self) -> None:\n        \"\"\"Add SageMaker Checkpointing patterns to .gitignore file.\"\"\"\n        # Make sure we only do this on the main process\n        if not self.is_world_process_zero():\n            return\n\n        patterns = [\"*.sagemaker-uploading\", \"*.sagemaker-uploaded\"]\n\n        # Get current .gitignore content\n        if os.path.exists(os.path.join(self.repo.local_dir, \".gitignore\")):\n            with open(os.path.join(self.repo.local_dir, \".gitignore\"), \"r\") as f:\n                current_content = f.read()\n        else:\n            current_content = \"\"\n\n        # Add the patterns to .gitignore\n        content = current_content\n        for pattern in patterns:\n            if pattern not in content:\n                if content.endswith(\"\\n\"):\n                    content += pattern\n                else:\n                    content += f\"\\n{pattern}\"\n\n        # Write the .gitignore file if it has changed\n        if content != current_content:\n            with open(os.path.join(self.repo.local_dir, \".gitignore\"), \"w\") as f:\n                logger.debug(f\"Writing .gitignore file. Content: {content}\")\n                f.write(content)\n\n        self.repo.git_add(\".gitignore\")\n\n        # avoid race condition with git status\n        time.sleep(0.5)\n\n        if not self.repo.is_repo_clean():\n            self.repo.git_commit(\"Add *.sagemaker patterns to .gitignore.\")\n            self.repo.git_push()",
    "Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for  Transformers.\n\nArgs:\n    model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):\n        The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.\n\n        <Tip>\n\n        [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use\n        your own models defined as `torch.nn.Module` as long as they work the same way as the  Transformers\n        models.\n\n        </Tip>\n\n    args ([`TrainingArguments`], *optional*):\n        The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the\n        `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.\n    data_collator (`DataCollator`, *optional*):\n        The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will\n        default to [`default_data_collator`] if no `tokenizer` is provided, an instance of\n        [`DataCollatorWithPadding`] otherwise.\n    train_dataset (`torch.utils.data.Dataset` or `torch.utils.data.IterableDataset`, *optional*):\n        The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the\n        `model.forward()` method are automatically removed.\n\n        Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a\n        distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a\n        `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will\n        manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally\n        sets the seed of the RNGs used.\n    eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`]), *optional*):\n         The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the\n         `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each\n         dataset prepending the dictionary key to the metric name.\n    tokenizer ([`PreTrainedTokenizerBase`], *optional*):\n        The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs to the\n        maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an\n        interrupted training or reuse the fine-tuned model.\n    model_init (`Callable[[], PreTrainedModel]`, *optional*):\n        A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start\n        from a new instance of the model as given by this function.\n\n        The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to\n        be able to choose different architectures according to hyper parameters (such as layer count, sizes of\n        inner layers, dropout probabilities etc).\n    compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):\n        The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return\n        a dictionary string to metric values.\n    callbacks (List of [`TrainerCallback`], *optional*):\n        A list of callbacks to customize the training loop. Will add those to the list of default callbacks\n        detailed in [here](callback).\n\n        If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.\n    optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*): A tuple\n        containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your model\n        and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.\n    preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):\n        A function that preprocess the logits right before caching them at each evaluation step. Must take two\n        tensors, the logits and the labels, and return the logits once processed as desired. The modifications made\n        by this function will be reflected in the predictions received by `compute_metrics`.\n\n        Note that the labels (second parameter) will be `None` if the dataset does not have them.\n\nImportant attributes:\n\n    - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]\n      subclass.\n    - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the\n      original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,\n      the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner\n      model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.\n    - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from\n      data parallelism, this means some of the model layers are split on different GPUs).\n    - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set\n      to `False` if model parallel or deepspeed is used, or if the default\n      `TrainingArguments.place_model_on_device` is overridden to return `False` .\n    - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while\n      in `train`)"
  ],
  [
    "def add_callback(self, callback):\n        \"\"\"\n        Add a callback to the current list of [`~transformer.TrainerCallback`].\n\n        Args:\n           callback (`type` or [`~transformer.TrainerCallback`]):\n               A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n               first case, will instantiate a member of that class.\n        \"\"\"\n        self.callback_handler.add_callback(callback)",
    "Add a callback to the current list of [`~transformer.TrainerCallback`].\n\nArgs:\n   callback (`type` or [`~transformer.TrainerCallback`]):\n       A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n       first case, will instantiate a member of that class."
  ],
  [
    "def pop_callback(self, callback):\n        \"\"\"\n        Remove a callback from the current list of [`~transformer.TrainerCallback`] and returns it.\n\n        If the callback is not found, returns `None` (and no error is raised).\n\n        Args:\n           callback (`type` or [`~transformer.TrainerCallback`]):\n               A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n               first case, will pop the first member of that class found in the list of callbacks.\n\n        Returns:\n            [`~transformer.TrainerCallback`]: The callback removed, if found.\n        \"\"\"\n        return self.callback_handler.pop_callback(callback)",
    "Remove a callback from the current list of [`~transformer.TrainerCallback`] and returns it.\n\nIf the callback is not found, returns `None` (and no error is raised).\n\nArgs:\n   callback (`type` or [`~transformer.TrainerCallback`]):\n       A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n       first case, will pop the first member of that class found in the list of callbacks.\n\nReturns:\n    [`~transformer.TrainerCallback`]: The callback removed, if found."
  ],
  [
    "def remove_callback(self, callback):\n        \"\"\"\n        Remove a callback from the current list of [`~transformer.TrainerCallback`].\n\n        Args:\n           callback (`type` or [`~transformer.TrainerCallback`]):\n               A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n               first case, will remove the first member of that class found in the list of callbacks.\n        \"\"\"\n        self.callback_handler.remove_callback(callback)",
    "Remove a callback from the current list of [`~transformer.TrainerCallback`].\n\nArgs:\n   callback (`type` or [`~transformer.TrainerCallback`]):\n       A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n       first case, will remove the first member of that class found in the list of callbacks."
  ],
  [
    "def _get_collator_with_removed_columns(\n        self, data_collator: Callable, description: Optional[str] = None\n    ) -> Callable:\n        \n        if not self.args.remove_unused_columns:\n            return data_collator\n        self._set_signature_columns_if_needed()\n        signature_columns = self._signature_columns\n\n        remove_columns_collator = RemoveColumnsCollator(\n            data_collator=data_collator,\n            signature_columns=signature_columns,\n            logger=logger,\n            description=description,\n            model_name=self.model.__class__.__name__,\n        )\n        return remove_columns_collator",
    "Wrap the data collator in a callable removing unused columns."
  ],
  [
    "def get_train_dataloader(self) -> DataLoader:\n        \"\"\"\n        Returns the training [`~torch.utils.data.DataLoader`].\n\n        Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed\n        training if necessary) otherwise.\n\n        Subclass and override this method if you want to inject some custom behavior.\n        \"\"\"\n        if self.train_dataset is None:\n            raise ValueError(\"Trainer: training requires a train_dataset.\")\n\n        train_dataset = self.train_dataset\n        data_collator = self.data_collator\n        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n            train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n        else:\n            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\")\n\n        if isinstance(train_dataset, torch.utils.data.IterableDataset):\n            if self.args.world_size > 1:\n                train_dataset = IterableDatasetShard(\n                    train_dataset,\n                    batch_size=self._train_batch_size,\n                    drop_last=self.args.dataloader_drop_last,\n                    num_processes=self.args.world_size,\n                    process_index=self.args.process_index,\n                )\n\n            return DataLoader(\n                train_dataset,\n                batch_size=self._train_batch_size,\n                collate_fn=data_collator,\n                num_workers=self.args.dataloader_num_workers,\n                pin_memory=self.args.dataloader_pin_memory,\n            )\n\n        train_sampler = self._get_train_sampler()\n\n        return DataLoader(\n            train_dataset,\n            batch_size=self._train_batch_size,\n            sampler=train_sampler,\n            collate_fn=data_collator,\n            drop_last=self.args.dataloader_drop_last,\n            num_workers=self.args.dataloader_num_workers,\n            pin_memory=self.args.dataloader_pin_memory,\n            worker_init_fn=seed_worker,\n        )",
    "Returns the training [`~torch.utils.data.DataLoader`].\n\nWill use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed\ntraining if necessary) otherwise.\n\nSubclass and override this method if you want to inject some custom behavior."
  ],
  [
    "def get_eval_dataloader(self, eval_dataset: Optional[Dataset] = None) -> DataLoader:\n        \"\"\"\n        Returns the evaluation [`~torch.utils.data.DataLoader`].\n\n        Subclass and override this method if you want to inject some custom behavior.\n\n        Args:\n            eval_dataset (`torch.utils.data.Dataset`, *optional*):\n                If provided, will override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns not accepted\n                by the `model.forward()` method are automatically removed. It must implement `__len__`.\n        \"\"\"\n        if eval_dataset is None and self.eval_dataset is None:\n            raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n        data_collator = self.data_collator\n\n        if is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):\n            eval_dataset = self._remove_unused_columns(eval_dataset, description=\"evaluation\")\n        else:\n            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"evaluation\")\n\n        if isinstance(eval_dataset, torch.utils.data.IterableDataset):\n            if self.args.world_size > 1:\n                eval_dataset = IterableDatasetShard(\n                    eval_dataset,\n                    batch_size=self.args.per_device_eval_batch_size,\n                    drop_last=self.args.dataloader_drop_last,\n                    num_processes=self.args.world_size,\n                    process_index=self.args.process_index,\n                )\n            return DataLoader(\n                eval_dataset,\n                batch_size=self.args.eval_batch_size,\n                collate_fn=data_collator,\n                num_workers=self.args.dataloader_num_workers,\n                pin_memory=self.args.dataloader_pin_memory,\n            )\n\n        eval_sampler = self._get_eval_sampler(eval_dataset)\n\n        return DataLoader(\n            eval_dataset,\n            sampler=eval_sampler,\n            batch_size=self.args.eval_batch_size,\n            collate_fn=data_collator,\n            drop_last=self.args.dataloader_drop_last,\n            num_workers=self.args.dataloader_num_workers,\n            pin_memory=self.args.dataloader_pin_memory,\n        )",
    "Returns the evaluation [`~torch.utils.data.DataLoader`].\n\nSubclass and override this method if you want to inject some custom behavior.\n\nArgs:\n    eval_dataset (`torch.utils.data.Dataset`, *optional*):\n        If provided, will override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns not accepted\n        by the `model.forward()` method are automatically removed. It must implement `__len__`."
  ],
  [
    "def get_test_dataloader(self, test_dataset: Dataset) -> DataLoader:\n        \"\"\"\n        Returns the test [`~torch.utils.data.DataLoader`].\n\n        Subclass and override this method if you want to inject some custom behavior.\n\n        Args:\n            test_dataset (`torch.utils.data.Dataset`, *optional*):\n                The test dataset to use. If it is a [`~datasets.Dataset`], columns not accepted by the\n                `model.forward()` method are automatically removed. It must implement `__len__`.\n        \"\"\"\n        data_collator = self.data_collator\n\n        if is_datasets_available() and isinstance(test_dataset, datasets.Dataset):\n            test_dataset = self._remove_unused_columns(test_dataset, description=\"test\")\n        else:\n            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"test\")\n\n        if isinstance(test_dataset, torch.utils.data.IterableDataset):\n            if self.args.world_size > 1:\n                test_dataset = IterableDatasetShard(\n                    test_dataset,\n                    batch_size=self.args.eval_batch_size,\n                    drop_last=self.args.dataloader_drop_last,\n                    num_processes=self.args.world_size,\n                    process_index=self.args.process_index,\n                )\n            return DataLoader(\n                test_dataset,\n                batch_size=self.args.eval_batch_size,\n                collate_fn=data_collator,\n                num_workers=self.args.dataloader_num_workers,\n                pin_memory=self.args.dataloader_pin_memory,\n            )\n\n        test_sampler = self._get_eval_sampler(test_dataset)\n\n        # We use the same batch_size as for eval.\n        return DataLoader(\n            test_dataset,\n            sampler=test_sampler,\n            batch_size=self.args.eval_batch_size,\n            collate_fn=data_collator,\n            drop_last=self.args.dataloader_drop_last,\n            num_workers=self.args.dataloader_num_workers,\n            pin_memory=self.args.dataloader_pin_memory,\n        )",
    "Returns the test [`~torch.utils.data.DataLoader`].\n\nSubclass and override this method if you want to inject some custom behavior.\n\nArgs:\n    test_dataset (`torch.utils.data.Dataset`, *optional*):\n        The test dataset to use. If it is a [`~datasets.Dataset`], columns not accepted by the\n        `model.forward()` method are automatically removed. It must implement `__len__`."
  ],
  [
    "def create_optimizer_and_scheduler(self, num_training_steps: int):\n        \"\"\"\n        Setup the optimizer and the learning rate scheduler.\n\n        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through `optimizers`, or subclass and override this method (or `create_optimizer` and/or\n        `create_scheduler`) in a subclass.\n        \"\"\"\n        self.create_optimizer()\n        if IS_SAGEMAKER_MP_POST_1_10 and smp.state.cfg.fp16:\n            # If smp >= 1.10 and fp16 is enabled, we unwrap the optimizer\n            optimizer = self.optimizer.optimizer\n        else:\n            optimizer = self.optimizer\n        self.create_scheduler(num_training_steps=num_training_steps, optimizer=optimizer)",
    "Setup the optimizer and the learning rate scheduler.\n\nWe provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\nTrainer's init through `optimizers`, or subclass and override this method (or `create_optimizer` and/or\n`create_scheduler`) in a subclass."
  ],
  [
    "def create_optimizer(self):\n        \"\"\"\n        Setup the optimizer.\n\n        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n        Trainer's init through `optimizers`, or subclass and override this method in a subclass.\n        \"\"\"\n        opt_model = self.model_wrapped if is_sagemaker_mp_enabled() else self.model\n\n        if self.optimizer is None:\n            decay_parameters = get_parameter_names(opt_model, ALL_LAYERNORM_LAYERS)\n            decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n            optimizer_grouped_parameters = [\n                {\n                    \"params\": [\n                        p for n, p in opt_model.named_parameters() if (n in decay_parameters and p.requires_grad)\n                    ],\n                    \"weight_decay\": self.args.weight_decay,\n                },\n                {\n                    \"params\": [\n                        p for n, p in opt_model.named_parameters() if (n not in decay_parameters and p.requires_grad)\n                    ],\n                    \"weight_decay\": 0.0,\n                },\n            ]\n\n            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args)\n\n            if self.sharded_ddp == ShardedDDPOption.SIMPLE:\n                self.optimizer = OSS(\n                    params=optimizer_grouped_parameters,\n                    optim=optimizer_cls,\n                    **optimizer_kwargs,\n                )\n            else:\n                self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)\n                if optimizer_cls.__name__ == \"Adam8bit\":\n                    import bitsandbytes\n\n                    manager = bitsandbytes.optim.GlobalOptimManager.get_instance()\n\n                    skipped = 0\n                    for module in opt_model.modules():\n                        if isinstance(module, nn.Embedding):\n                            skipped += sum({p.data_ptr(): p.numel() for p in module.parameters()}.values())\n                            print(f\"skipped {module}: {skipped/2**20}M params\")\n                            manager.register_module_override(module, \"weight\", {\"optim_bits\": 32})\n                            logger.debug(f\"bitsandbytes: will optimize {module} in fp32\")\n                    print(f\"skipped: {skipped/2**20}M params\")\n\n        if is_sagemaker_mp_enabled():\n            self.optimizer = smp.DistributedOptimizer(self.optimizer)\n\n        return self.optimizer",
    "Setup the optimizer.\n\nWe provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\nTrainer's init through `optimizers`, or subclass and override this method in a subclass."
  ],
  [
    "def get_optimizer_cls_and_kwargs(args: TrainingArguments) -> Tuple[Any, Any]:\n        \"\"\"\n        Returns the optimizer class and optimizer parameters based on the training arguments.\n\n        Args:\n            args (`transformers.training_args.TrainingArguments`):\n                The training arguments for the training session.\n\n        \"\"\"\n\n        # parse args.optim_args\n        optim_args = {}\n        if args.optim_args:\n            for mapping in args.optim_args.replace(\" \", \"\").split(\",\"):\n                key, value = mapping.split(\"=\")\n                optim_args[key] = value\n\n        optimizer_kwargs = {\"lr\": args.learning_rate}\n\n        adam_kwargs = {\n            \"betas\": (args.adam_beta1, args.adam_beta2),\n            \"eps\": args.adam_epsilon,\n        }\n        if args.optim == OptimizerNames.ADAFACTOR:\n            optimizer_cls = Adafactor\n            optimizer_kwargs.update({\"scale_parameter\": False, \"relative_step\": False})\n        elif args.optim == OptimizerNames.ADAMW_HF:\n            from transformers.optimization import AdamW\n\n            optimizer_cls = AdamW\n            optimizer_kwargs.update(adam_kwargs)\n        elif args.optim in [OptimizerNames.ADAMW_TORCH, OptimizerNames.ADAMW_TORCH_FUSED]:\n            from torch.optim import AdamW\n\n            optimizer_cls = AdamW\n            optimizer_kwargs.update(adam_kwargs)\n            if args.optim == OptimizerNames.ADAMW_TORCH_FUSED:\n                optimizer_kwargs.update({\"fused\": True})\n        elif args.optim == OptimizerNames.ADAMW_TORCH_XLA:\n            try:\n                from torch_xla.amp.syncfree import AdamW\n\n                optimizer_cls = AdamW\n                optimizer_kwargs.update(adam_kwargs)\n            except ImportError:\n                raise ValueError(\"Trainer failed to import syncfree AdamW from torch_xla.\")\n        elif args.optim == OptimizerNames.ADAMW_APEX_FUSED:\n            try:\n                from apex.optimizers import FusedAdam\n\n                optimizer_cls = FusedAdam\n                optimizer_kwargs.update(adam_kwargs)\n            except ImportError:\n                raise ValueError(\"Trainer tried to instantiate apex FusedAdam but apex is not installed!\")\n        elif args.optim == OptimizerNames.ADAMW_BNB:\n            try:\n                from bitsandbytes.optim import Adam8bit\n\n                optimizer_cls = Adam8bit\n                optimizer_kwargs.update(adam_kwargs)\n            except ImportError:\n                raise ValueError(\"Trainer tried to instantiate bnb Adam8bit but bnb is not installed!\")\n        elif args.optim == OptimizerNames.ADAMW_ANYPRECISION:\n            try:\n                from torchdistx.optimizers import AnyPrecisionAdamW\n\n                optimizer_cls = AnyPrecisionAdamW\n                optimizer_kwargs.update(adam_kwargs)\n\n                # TODO Change dtypes back to M=FP32, Var = BF16, Kahan = False once they can be cast together in torchdistx.\n                optimizer_kwargs.update(\n                    {\n                        \"use_kahan_summation\": strtobool(optim_args.get(\"use_kahan_summation\", \"False\")),\n                        \"momentum_dtype\": getattr(torch, optim_args.get(\"momentum_dtype\", \"float32\")),\n                        \"variance_dtype\": getattr(torch, optim_args.get(\"variance_dtype\", \"float32\")),\n                        \"compensation_buffer_dtype\": getattr(\n                            torch, optim_args.get(\"compensation_buffer_dtype\", \"bfloat16\")\n                        ),\n                    }\n                )\n            except ImportError:\n                raise ValueError(\"Please install https://github.com/pytorch/torchdistx\")\n        elif args.optim == OptimizerNames.SGD:\n            optimizer_cls = torch.optim.SGD\n        elif args.optim == OptimizerNames.ADAGRAD:\n            optimizer_cls = torch.optim.Adagrad\n        else:\n            raise ValueError(f\"Trainer cannot instantiate unsupported optimizer: {args.optim}\")\n        return optimizer_cls, optimizer_kwargs",
    "Returns the optimizer class and optimizer parameters based on the training arguments.\n\nArgs:\n    args (`transformers.training_args.TrainingArguments`):\n        The training arguments for the training session."
  ],
  [
    "def create_scheduler(self, num_training_steps: int, optimizer: torch.optim.Optimizer = None):\n        \"\"\"\n        Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or\n        passed as an argument.\n\n        Args:\n            num_training_steps (int): The number of training steps to do.\n        \"\"\"\n        if self.lr_scheduler is None:\n            self.lr_scheduler = get_scheduler(\n                self.args.lr_scheduler_type,\n                optimizer=self.optimizer if optimizer is None else optimizer,\n                num_warmup_steps=self.args.get_warmup_steps(num_training_steps),\n                num_training_steps=num_training_steps,\n            )\n        return self.lr_scheduler",
    "Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or\npassed as an argument.\n\nArgs:\n    num_training_steps (int): The number of training steps to do."
  ],
  [
    "def num_examples(self, dataloader: DataLoader) -> int:\n        \"\"\"\n        Helper to get number of samples in a [`~torch.utils.data.DataLoader`] by accessing its dataset. When\n        dataloader.dataset does not exist or has no length, estimates as best it can\n        \"\"\"\n        try:\n            dataset = dataloader.dataset\n            # Special case for IterableDatasetShard, we need to dig deeper\n            if isinstance(dataset, IterableDatasetShard):\n                return len(dataloader.dataset.dataset)\n            return len(dataloader.dataset)\n        except (NameError, AttributeError, TypeError):  # no dataset or length, estimate by length of dataloader\n            return len(dataloader) * self.args.per_device_train_batch_size",
    "Helper to get number of samples in a [`~torch.utils.data.DataLoader`] by accessing its dataset. When\ndataloader.dataset does not exist or has no length, estimates as best it can"
  ],
  [
    "def train(\n        self,\n        resume_from_checkpoint: Optional[Union[str, bool]] = None,\n        trial: Union[\"optuna.Trial\", Dict[str, Any]] = None,\n        ignore_keys_for_eval: Optional[List[str]] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Main training entry point.\n\n        Args:\n            resume_from_checkpoint (`str` or `bool`, *optional*):\n                If a `str`, local path to a saved checkpoint as saved by a previous instance of [`Trainer`]. If a\n                `bool` and equals `True`, load the last checkpoint in *args.output_dir* as saved by a previous instance\n                of [`Trainer`]. If present, training will resume from the model/optimizer/scheduler states loaded here.\n            trial (`optuna.Trial` or `Dict[str, Any]`, *optional*):\n                The trial run or the hyperparameter dictionary for hyperparameter search.\n            ignore_keys_for_eval (`List[str]`, *optional*)\n                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n                gathering predictions for evaluation during the training.\n            kwargs:\n                Additional keyword arguments used to hide deprecated arguments\n        \"\"\"\n        if resume_from_checkpoint is False:\n            resume_from_checkpoint = None\n\n        # memory metrics - must set up as early as possible\n        self._memory_tracker.start()\n\n        args = self.args\n\n        self.is_in_train = True\n\n        # do_train is not a reliable argument, as it might not be set and .train() still called, so\n        # the following is a workaround:\n        if (args.fp16_full_eval or args.bf16_full_eval) and not args.do_train:\n            self._move_model_to_device(self.model, args.device)\n\n        if \"model_path\" in kwargs:\n            resume_from_checkpoint = kwargs.pop(\"model_path\")\n            warnings.warn(\n                \"`model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` \"\n                \"instead.\",\n                FutureWarning,\n            )\n        if len(kwargs) > 0:\n            raise TypeError(f\"train() received got unexpected keyword arguments: {', '.join(list(kwargs.keys()))}.\")\n        # This might change the seed so needs to run first.\n        self._hp_search_setup(trial)\n        self._train_batch_size = self.args.train_batch_size\n\n        # Model re-init\n        model_reloaded = False\n        if self.model_init is not None:\n            # Seed must be set before instantiating the model when using model_init.\n            enable_full_determinism(self.args.seed) if self.args.full_determinism else set_seed(self.args.seed)\n            self.model = self.call_model_init(trial)\n            model_reloaded = True\n            # Reinitializes optimizer and scheduler\n            self.optimizer, self.lr_scheduler = None, None\n\n        # Load potential model checkpoint\n        if isinstance(resume_from_checkpoint, bool) and resume_from_checkpoint:\n            resume_from_checkpoint = get_last_checkpoint(args.output_dir)\n            if resume_from_checkpoint is None:\n                raise ValueError(f\"No valid checkpoint found in output directory ({args.output_dir})\")\n\n        if resume_from_checkpoint is not None and not is_sagemaker_mp_enabled() and args.deepspeed is None:\n            self._load_from_checkpoint(resume_from_checkpoint)\n\n        # If model was re-initialized, put it on the right device and update self.model_wrapped\n        if model_reloaded:\n            if self.place_model_on_device:\n                self._move_model_to_device(self.model, args.device)\n            self.model_wrapped = self.model\n\n        inner_training_loop = find_executable_batch_size(\n            self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size\n        )\n        return inner_training_loop(\n            args=args,\n            resume_from_checkpoint=resume_from_checkpoint,\n            trial=trial,\n            ignore_keys_for_eval=ignore_keys_for_eval,\n        )",
    "Main training entry point.\n\nArgs:\n    resume_from_checkpoint (`str` or `bool`, *optional*):\n        If a `str`, local path to a saved checkpoint as saved by a previous instance of [`Trainer`]. If a\n        `bool` and equals `True`, load the last checkpoint in *args.output_dir* as saved by a previous instance\n        of [`Trainer`]. If present, training will resume from the model/optimizer/scheduler states loaded here.\n    trial (`optuna.Trial` or `Dict[str, Any]`, *optional*):\n        The trial run or the hyperparameter dictionary for hyperparameter search.\n    ignore_keys_for_eval (`List[str]`, *optional*)\n        A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n        gathering predictions for evaluation during the training.\n    kwargs:\n        Additional keyword arguments used to hide deprecated arguments"
  ],
  [
    "def _load_optimizer_and_scheduler(self, checkpoint):\n        \n        if checkpoint is None:\n            return\n\n        if self.deepspeed:\n            # deepspeed loads optimizer/lr_scheduler together with the model in deepspeed_init\n            return\n\n        checkpoint_file_exists = (\n            glob.glob(os.path.join(checkpoint, OPTIMIZER_NAME) + \"_*\")\n            if is_sagemaker_mp_enabled()\n            else os.path.isfile(os.path.join(checkpoint, OPTIMIZER_NAME))\n        )\n        if checkpoint_file_exists and os.path.isfile(os.path.join(checkpoint, SCHEDULER_NAME)):\n            # Load in optimizer and scheduler states\n            if is_torch_tpu_available():\n                # On TPU we have to take some extra precautions to properly load the states on the right device.\n                optimizer_state = torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=\"cpu\")\n                with warnings.catch_warnings(record=True) as caught_warnings:\n                    lr_scheduler_state = torch.load(os.path.join(checkpoint, SCHEDULER_NAME), map_location=\"cpu\")\n                reissue_pt_warnings(caught_warnings)\n\n                xm.send_cpu_data_to_device(optimizer_state, self.args.device)\n                xm.send_cpu_data_to_device(lr_scheduler_state, self.args.device)\n\n                self.optimizer.load_state_dict(optimizer_state)\n                self.lr_scheduler.load_state_dict(lr_scheduler_state)\n            else:\n                map_location = \"cpu\" if is_sagemaker_mp_enabled() else self.args.device\n                if is_sagemaker_mp_enabled():\n                    if os.path.isfile(os.path.join(checkpoint, \"user_content.pt\")):\n                        # Optimizer checkpoint was saved with smp >= 1.10\n                        def opt_load_hook(mod, opt):\n                            opt.load_state_dict(smp.load(os.path.join(checkpoint, OPTIMIZER_NAME), partial=True))\n\n                    else:\n                        # Optimizer checkpoint was saved with smp < 1.10\n                        def opt_load_hook(mod, opt):\n                            if IS_SAGEMAKER_MP_POST_1_10:\n                                opt.load_state_dict(\n                                    smp.load(os.path.join(checkpoint, OPTIMIZER_NAME), partial=True, back_compat=True)\n                                )\n                            else:\n                                opt.load_state_dict(smp.load(os.path.join(checkpoint, OPTIMIZER_NAME), partial=True))\n\n                    self.model_wrapped.register_post_step_hook(opt_load_hook)\n                else:\n                    self.optimizer.load_state_dict(\n                        torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n                    )\n                with warnings.catch_warnings(record=True) as caught_warnings:\n                    self.lr_scheduler.load_state_dict(torch.load(os.path.join(checkpoint, SCHEDULER_NAME)))\n                reissue_pt_warnings(caught_warnings)\n                if self.do_grad_scaling and os.path.isfile(os.path.join(checkpoint, SCALER_NAME)):\n                    self.scaler.load_state_dict(torch.load(os.path.join(checkpoint, SCALER_NAME)))",
    "If optimizer and scheduler states exist, load them."
  ],
  [
    "def hyperparameter_search(\n        self,\n        hp_space: Optional[Callable[[\"optuna.Trial\"], Dict[str, float]]] = None,\n        compute_objective: Optional[Callable[[Dict[str, float]], float]] = None,\n        n_trials: int = 20,\n        direction: str = \"minimize\",\n        backend: Optional[Union[\"str\", HPSearchBackend]] = None,\n        hp_name: Optional[Callable[[\"optuna.Trial\"], str]] = None,\n        **kwargs,\n    ) -> BestRun:\n        \"\"\"\n        Launch an hyperparameter search using `optuna` or `Ray Tune` or `SigOpt`. The optimized quantity is determined\n        by `compute_objective`, which defaults to a function returning the evaluation loss when no metric is provided,\n        the sum of all metrics otherwise.\n\n        <Tip warning={true}>\n\n        To use this method, you need to have provided a `model_init` when initializing your [`Trainer`]: we need to\n        reinitialize the model at each new run. This is incompatible with the `optimizers` argument, so you need to\n        subclass [`Trainer`] and override the method [`~Trainer.create_optimizer_and_scheduler`] for custom\n        optimizer/scheduler.\n\n        </Tip>\n\n        Args:\n            hp_space (`Callable[[\"optuna.Trial\"], Dict[str, float]]`, *optional*):\n                A function that defines the hyperparameter search space. Will default to\n                [`~trainer_utils.default_hp_space_optuna`] or [`~trainer_utils.default_hp_space_ray`] or\n                [`~trainer_utils.default_hp_space_sigopt`] depending on your backend.\n            compute_objective (`Callable[[Dict[str, float]], float]`, *optional*):\n                A function computing the objective to minimize or maximize from the metrics returned by the `evaluate`\n                method. Will default to [`~trainer_utils.default_compute_objective`].\n            n_trials (`int`, *optional*, defaults to 100):\n                The number of trial runs to test.\n            direction (`str`, *optional*, defaults to `\"minimize\"`):\n                Whether to optimize greater or lower objects. Can be `\"minimize\"` or `\"maximize\"`, you should pick\n                `\"minimize\"` when optimizing the validation loss, `\"maximize\"` when optimizing one or several metrics.\n            backend (`str` or [`~training_utils.HPSearchBackend`], *optional*):\n                The backend to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt, depending\n                on which one is installed. If all are installed, will default to optuna.\n            hp_name (`Callable[[\"optuna.Trial\"], str]]`, *optional*):\n                A function that defines the trial/run name. Will default to None.\n            kwargs (`Dict[str, Any]`, *optional*):\n                Additional keyword arguments passed along to `optuna.create_study` or `ray.tune.run`. For more\n                information see:\n\n                - the documentation of\n                  [optuna.create_study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html)\n                - the documentation of [tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run)\n                - the documentation of [sigopt](https://app.sigopt.com/docs/endpoints/experiments/create)\n\n        Returns:\n            [`trainer_utils.BestRun`]: All the information about the best run. Experiment summary can be found in\n            `run_summary` attribute for Ray backend.\n        \"\"\"\n        if backend is None:\n            backend = default_hp_search_backend()\n            if backend is None:\n                raise RuntimeError(\n                    \"At least one of optuna or ray should be installed. \"\n                    \"To install optuna run `pip install optuna`. \"\n                    \"To install ray run `pip install ray[tune]`. \"\n                    \"To install sigopt run `pip install sigopt`.\"\n                )\n        backend = HPSearchBackend(backend)\n        if backend == HPSearchBackend.OPTUNA and not is_optuna_available():\n            raise RuntimeError(\"You picked the optuna backend, but it is not installed. Use `pip install optuna`.\")\n        if backend == HPSearchBackend.RAY and not is_ray_tune_available():\n            raise RuntimeError(\n                \"You picked the Ray Tune backend, but it is not installed. Use `pip install 'ray[tune]'`.\"\n            )\n        if backend == HPSearchBackend.SIGOPT and not is_sigopt_available():\n            raise RuntimeError(\"You picked the sigopt backend, but it is not installed. Use `pip install sigopt`.\")\n        if backend == HPSearchBackend.WANDB and not is_wandb_available():\n            raise RuntimeError(\"You picked the wandb backend, but it is not installed. Use `pip install wandb`.\")\n        self.hp_search_backend = backend\n        if self.model_init is None:\n            raise RuntimeError(\n                \"To use hyperparameter search, you need to pass your model through a model_init function.\"\n            )\n\n        self.hp_space = default_hp_space[backend] if hp_space is None else hp_space\n        self.hp_name = hp_name\n        self.compute_objective = default_compute_objective if compute_objective is None else compute_objective\n\n        backend_dict = {\n            HPSearchBackend.OPTUNA: run_hp_search_optuna,\n            HPSearchBackend.RAY: run_hp_search_ray,\n            HPSearchBackend.SIGOPT: run_hp_search_sigopt,\n            HPSearchBackend.WANDB: run_hp_search_wandb,\n        }\n        best_run = backend_dict[backend](self, n_trials, direction, **kwargs)\n\n        self.hp_search_backend = None\n        return best_run",
    "Launch an hyperparameter search using `optuna` or `Ray Tune` or `SigOpt`. The optimized quantity is determined\nby `compute_objective`, which defaults to a function returning the evaluation loss when no metric is provided,\nthe sum of all metrics otherwise.\n\n<Tip warning={true}>\n\nTo use this method, you need to have provided a `model_init` when initializing your [`Trainer`]: we need to\nreinitialize the model at each new run. This is incompatible with the `optimizers` argument, so you need to\nsubclass [`Trainer`] and override the method [`~Trainer.create_optimizer_and_scheduler`] for custom\noptimizer/scheduler.\n\n</Tip>\n\nArgs:\n    hp_space (`Callable[[\"optuna.Trial\"], Dict[str, float]]`, *optional*):\n        A function that defines the hyperparameter search space. Will default to\n        [`~trainer_utils.default_hp_space_optuna`] or [`~trainer_utils.default_hp_space_ray`] or\n        [`~trainer_utils.default_hp_space_sigopt`] depending on your backend.\n    compute_objective (`Callable[[Dict[str, float]], float]`, *optional*):\n        A function computing the objective to minimize or maximize from the metrics returned by the `evaluate`\n        method. Will default to [`~trainer_utils.default_compute_objective`].\n    n_trials (`int`, *optional*, defaults to 100):\n        The number of trial runs to test.\n    direction (`str`, *optional*, defaults to `\"minimize\"`):\n        Whether to optimize greater or lower objects. Can be `\"minimize\"` or `\"maximize\"`, you should pick\n        `\"minimize\"` when optimizing the validation loss, `\"maximize\"` when optimizing one or several metrics.\n    backend (`str` or [`~training_utils.HPSearchBackend`], *optional*):\n        The backend to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt, depending\n        on which one is installed. If all are installed, will default to optuna.\n    hp_name (`Callable[[\"optuna.Trial\"], str]]`, *optional*):\n        A function that defines the trial/run name. Will default to None.\n    kwargs (`Dict[str, Any]`, *optional*):\n        Additional keyword arguments passed along to `optuna.create_study` or `ray.tune.run`. For more\n        information see:\n\n        - the documentation of\n          [optuna.create_study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html)\n        - the documentation of [tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run)\n        - the documentation of [sigopt](https://app.sigopt.com/docs/endpoints/experiments/create)\n\nReturns:\n    [`trainer_utils.BestRun`]: All the information about the best run. Experiment summary can be found in\n    `run_summary` attribute for Ray backend."
  ],
  [
    "def log(self, logs: Dict[str, float]) -> None:\n        \"\"\"\n        Log `logs` on the various objects watching training.\n\n        Subclass and override this method to inject custom behavior.\n\n        Args:\n            logs (`Dict[str, float]`):\n                The values to log.\n        \"\"\"\n        if self.state.epoch is not None:\n            logs[\"epoch\"] = round(self.state.epoch, 2)\n\n        output = {**logs, **{\"step\": self.state.global_step}}\n        self.state.log_history.append(output)\n        self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)",
    "Log `logs` on the various objects watching training.\n\nSubclass and override this method to inject custom behavior.\n\nArgs:\n    logs (`Dict[str, float]`):\n        The values to log."
  ],
  [
    "def _prepare_input(self, data: Union[torch.Tensor, Any]) -> Union[torch.Tensor, Any]:\n        \"\"\"\n        Prepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.\n        \"\"\"\n        if isinstance(data, Mapping):\n            return type(data)({k: self._prepare_input(v) for k, v in data.items()})\n        elif isinstance(data, (tuple, list)):\n            return type(data)(self._prepare_input(v) for v in data)\n        elif isinstance(data, torch.Tensor):\n            kwargs = {\"device\": self.args.device}\n            if self.deepspeed and (torch.is_floating_point(data) or torch.is_complex(data)):\n                # NLP models inputs are int/uint and those get adjusted to the right dtype of the\n                # embedding. Other models such as wav2vec2's inputs are already float and thus\n                # may need special handling to match the dtypes of the model\n                kwargs.update({\"dtype\": self.args.hf_deepspeed_config.dtype()})\n            return data.to(**kwargs)\n        return data",
    "Prepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors."
  ],
  [
    "def _prepare_inputs(self, inputs: Dict[str, Union[torch.Tensor, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:\n        \"\"\"\n        Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and\n        handling potential state.\n        \"\"\"\n        inputs = self._prepare_input(inputs)\n        if len(inputs) == 0:\n            raise ValueError(\n                \"The batch received was empty, your model won't be able to train on it. Double-check that your \"\n                f\"training dataset contains keys expected by the model: {','.join(self._signature_columns)}.\"\n            )\n        if self.args.past_index >= 0 and self._past is not None:\n            inputs[\"mems\"] = self._past\n\n        return inputs",
    "Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and\nhandling potential state."
  ],
  [
    "def compute_loss_context_manager(self):\n        \"\"\"\n        A helper wrapper to group together context managers.\n        \"\"\"\n        return self.autocast_smart_context_manager()",
    "A helper wrapper to group together context managers."
  ],
  [
    "def autocast_smart_context_manager(self, cache_enabled: Optional[bool] = True):\n        \"\"\"\n        A helper wrapper that creates an appropriate context manager for `autocast` while feeding it the desired\n        arguments, depending on the situation.\n        \"\"\"\n        if self.use_cuda_amp or self.use_cpu_amp:\n            if is_torch_greater_or_equal_than_1_10:\n                ctx_manager = (\n                    torch.cpu.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n                    if self.use_cpu_amp\n                    else torch.cuda.amp.autocast(cache_enabled=cache_enabled, dtype=self.amp_dtype)\n                )\n            else:\n                ctx_manager = torch.cuda.amp.autocast()\n        else:\n            ctx_manager = contextlib.nullcontext() if sys.version_info >= (3, 7) else contextlib.suppress()\n\n        return ctx_manager",
    "A helper wrapper that creates an appropriate context manager for `autocast` while feeding it the desired\narguments, depending on the situation."
  ],
  [
    "def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        \"\"\"\n        Perform a training step on a batch of inputs.\n\n        Subclass and override to inject custom behavior.\n\n        Args:\n            model (`nn.Module`):\n                The model to train.\n            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n\n                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n                argument `labels`. Check your model's documentation for all accepted arguments.\n\n        Return:\n            `torch.Tensor`: The tensor with training loss on this batch.\n        \"\"\"\n        model.train()\n        inputs = self._prepare_inputs(inputs)\n\n        if is_sagemaker_mp_enabled():\n            loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n            return loss_mb.reduce_mean().detach().to(self.args.device)\n\n        with self.compute_loss_context_manager():\n            loss = self.compute_loss(model, inputs)\n\n        if self.args.n_gpu > 1:\n            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n\n        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:\n            # deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\n            loss = loss / self.args.gradient_accumulation_steps\n\n        if self.do_grad_scaling:\n            self.scaler.scale(loss).backward()\n        elif self.use_apex:\n            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                scaled_loss.backward()\n        elif self.deepspeed:\n            # loss gets scaled under gradient_accumulation_steps in deepspeed\n            loss = self.deepspeed.backward(loss)\n        else:\n            loss.backward()\n\n        return loss.detach()",
    "Perform a training step on a batch of inputs.\n\nSubclass and override to inject custom behavior.\n\nArgs:\n    model (`nn.Module`):\n        The model to train.\n    inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n        The inputs and targets of the model.\n\n        The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n        argument `labels`. Check your model's documentation for all accepted arguments.\n\nReturn:\n    `torch.Tensor`: The tensor with training loss on this batch."
  ],
  [
    "def compute_loss(self, model, inputs, return_outputs=False):\n        \"\"\"\n        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n\n        Subclass and override for custom behavior.\n        \"\"\"\n        if self.label_smoother is not None and \"labels\" in inputs:\n            labels = inputs.pop(\"labels\")\n        else:\n            labels = None\n        outputs = model(**inputs)\n        # Save past state if it exists\n        # TODO: this needs to be fixed and made cleaner later.\n        if self.args.past_index >= 0:\n            self._past = outputs[self.args.past_index]\n\n        if labels is not None:\n            if unwrap_model(model)._get_name() in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES.values():\n                loss = self.label_smoother(outputs, labels, shift_labels=True)\n            else:\n                loss = self.label_smoother(outputs, labels)\n        else:\n            if isinstance(outputs, dict) and \"loss\" not in outputs:\n                raise ValueError(\n                    \"The model did not return a loss from the inputs, only the following keys: \"\n                    f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n                )\n            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n\n        return (loss, outputs) if return_outputs else loss",
    "How the loss is computed by Trainer. By default, all models return the loss in the first element.\n\nSubclass and override for custom behavior."
  ],
  [
    "def is_local_process_zero(self) -> bool:\n        \"\"\"\n        Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several\n        machines) main process.\n        \"\"\"\n        return self.args.local_process_index == 0",
    "Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several\nmachines) main process."
  ],
  [
    "def is_world_process_zero(self) -> bool:\n        \"\"\"\n        Whether or not this process is the global main process (when training in a distributed fashion on several\n        machines, this is only going to be `True` for one process).\n        \"\"\"\n        # Special case for SageMaker ModelParallel since there process_index is dp_process_index, not the global\n        # process index.\n        if is_sagemaker_mp_enabled():\n            return smp.rank() == 0\n        else:\n            return self.args.process_index == 0",
    "Whether or not this process is the global main process (when training in a distributed fashion on several\nmachines, this is only going to be `True` for one process)."
  ],
  [
    "def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False):\n        \"\"\"\n        Will save the model, so you can reload it using `from_pretrained()`.\n\n        Will only save from the main process.\n        \"\"\"\n\n        if output_dir is None:\n            output_dir = self.args.output_dir\n\n        if is_torch_tpu_available():\n            self._save_tpu(output_dir)\n        elif is_sagemaker_mp_enabled():\n            # Calling the state_dict needs to be done on the wrapped model and on all processes.\n            os.makedirs(output_dir, exist_ok=True)\n            state_dict = self.model_wrapped.state_dict()\n            if self.args.should_save:\n                self._save(output_dir, state_dict=state_dict)\n            if IS_SAGEMAKER_MP_POST_1_10:\n                # 'user_content.pt' indicates model state_dict saved with smp >= 1.10\n                Path(os.path.join(output_dir, \"user_content.pt\")).touch()\n        elif (\n            ShardedDDPOption.ZERO_DP_2 in self.args.sharded_ddp\n            or ShardedDDPOption.ZERO_DP_3 in self.args.sharded_ddp\n            or self.fsdp is not None\n        ):\n            state_dict = self.model.state_dict()\n\n            if self.args.should_save:\n                self._save(output_dir, state_dict=state_dict)\n        elif self.deepspeed:\n            # this takes care of everything as long as we aren't under zero3\n            if self.args.should_save:\n                self._save(output_dir)\n\n            if is_deepspeed_zero3_enabled():\n                # It's too complicated to try to override different places where the weights dump gets\n                # saved, so since under zero3 the file is bogus, simply delete it. The user should\n                # either user deepspeed checkpoint to resume or to recover full weights use\n                # zero_to_fp32.py stored in the checkpoint.\n                if self.args.should_save:\n                    file = os.path.join(output_dir, WEIGHTS_NAME)\n                    if os.path.isfile(file):\n                        # logger.info(f\"deepspeed zero3: removing {file}, see zero_to_fp32.py to recover weights\")\n                        os.remove(file)\n\n                # now save the real model if stage3_gather_16bit_weights_on_model_save=True\n                # if false it will not be saved.\n                # This must be called on all ranks\n                if not self.deepspeed.save_16bit_model(output_dir, WEIGHTS_NAME):\n                    logger.warning(\n                        \"deepspeed.save_16bit_model didn't save the model, since\"\n                        \" stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead, use\"\n                        \" zero_to_fp32.py to recover weights\"\n                    )\n                    self.deepspeed.save_checkpoint(output_dir)\n\n        elif self.args.should_save:\n            self._save(output_dir)\n\n        # Push to the Hub when `save_model` is called by the user.\n        if self.args.push_to_hub and not _internal_call:\n            self.push_to_hub(commit_message=\"Model save\")",
    "Will save the model, so you can reload it using `from_pretrained()`.\n\nWill only save from the main process."
  ],
  [
    "def evaluate(\n        self,\n        eval_dataset: Optional[Dataset] = None,\n        ignore_keys: Optional[List[str]] = None,\n        metric_key_prefix: str = \"eval\",\n    ) -> Dict[str, float]:\n        \"\"\"\n        Run evaluation and returns metrics.\n\n        The calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n        (pass it to the init `compute_metrics` argument).\n\n        You can also subclass and override this method to inject custom behavior.\n\n        Args:\n            eval_dataset (`Dataset`, *optional*):\n                Pass a dataset if you wish to override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns\n                not accepted by the `model.forward()` method are automatically removed. It must implement the `__len__`\n                method.\n            ignore_keys (`Lst[str]`, *optional*):\n                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n                gathering predictions.\n            metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n                An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n                \"eval_bleu\" if the prefix is \"eval\" (default)\n\n        Returns:\n            A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n            dictionary also contains the epoch number which comes from the training state.\n        \"\"\"\n        # memory metrics - must set up as early as possible\n        self._memory_tracker.start()\n\n        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n        start_time = time.time()\n\n        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n        output = eval_loop(\n            eval_dataloader,\n            description=\"Evaluation\",\n            # No point gathering the predictions if there are no metrics, otherwise we defer to\n            # self.args.prediction_loss_only\n            prediction_loss_only=True if self.compute_metrics is None else None,\n            ignore_keys=ignore_keys,\n            metric_key_prefix=metric_key_prefix,\n        )\n\n        total_batch_size = self.args.eval_batch_size * self.args.world_size\n        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n        output.metrics.update(\n            speed_metrics(\n                metric_key_prefix,\n                start_time,\n                num_samples=output.num_samples,\n                num_steps=math.ceil(output.num_samples / total_batch_size),\n            )\n        )\n\n        self.log(output.metrics)\n\n        if DebugOption.TPU_METRICS_DEBUG in self.args.debug:\n            # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n            xm.master_print(met.metrics_report())\n\n        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, output.metrics)\n\n        self._memory_tracker.stop_and_update_metrics(output.metrics)\n\n        return output.metrics",
    "Run evaluation and returns metrics.\n\nThe calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n(pass it to the init `compute_metrics` argument).\n\nYou can also subclass and override this method to inject custom behavior.\n\nArgs:\n    eval_dataset (`Dataset`, *optional*):\n        Pass a dataset if you wish to override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns\n        not accepted by the `model.forward()` method are automatically removed. It must implement the `__len__`\n        method.\n    ignore_keys (`Lst[str]`, *optional*):\n        A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n        gathering predictions.\n    metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n        An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n        \"eval_bleu\" if the prefix is \"eval\" (default)\n\nReturns:\n    A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n    dictionary also contains the epoch number which comes from the training state."
  ],
  [
    "def predict(\n        self, test_dataset: Dataset, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = \"test\"\n    ) -> PredictionOutput:\n        \"\"\"\n        Run prediction and returns predictions and potential metrics.\n\n        Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method\n        will also return metrics, like in `evaluate()`.\n\n        Args:\n            test_dataset (`Dataset`):\n                Dataset to run the predictions on. If it is an `datasets.Dataset`, columns not accepted by the\n                `model.forward()` method are automatically removed. Has to implement the method `__len__`\n            ignore_keys (`Lst[str]`, *optional*):\n                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n                gathering predictions.\n            metric_key_prefix (`str`, *optional*, defaults to `\"test\"`):\n                An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n                \"test_bleu\" if the prefix is \"test\" (default)\n\n        <Tip>\n\n        If your predictions or labels have different sequence length (for instance because you're doing dynamic padding\n        in a token classification task) the predictions will be padded (on the right) to allow for concatenation into\n        one array. The padding index is -100.\n\n        </Tip>\n\n        Returns: *NamedTuple* A namedtuple with the following keys:\n\n            - predictions (`np.ndarray`): The predictions on `test_dataset`.\n            - label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained some).\n            - metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics (if the dataset contained\n              labels).\n        \"\"\"\n        # memory metrics - must set up as early as possible\n        self._memory_tracker.start()\n\n        test_dataloader = self.get_test_dataloader(test_dataset)\n        start_time = time.time()\n\n        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n        output = eval_loop(\n            test_dataloader, description=\"Prediction\", ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix\n        )\n        total_batch_size = self.args.eval_batch_size * self.args.world_size\n        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n        output.metrics.update(\n            speed_metrics(\n                metric_key_prefix,\n                start_time,\n                num_samples=output.num_samples,\n                num_steps=math.ceil(output.num_samples / total_batch_size),\n            )\n        )\n\n        self.control = self.callback_handler.on_predict(self.args, self.state, self.control, output.metrics)\n        self._memory_tracker.stop_and_update_metrics(output.metrics)\n\n        return PredictionOutput(predictions=output.predictions, label_ids=output.label_ids, metrics=output.metrics)",
    "Run prediction and returns predictions and potential metrics.\n\nDepending on the dataset and your use case, your test dataset may contain labels. In that case, this method\nwill also return metrics, like in `evaluate()`.\n\nArgs:\n    test_dataset (`Dataset`):\n        Dataset to run the predictions on. If it is an `datasets.Dataset`, columns not accepted by the\n        `model.forward()` method are automatically removed. Has to implement the method `__len__`\n    ignore_keys (`Lst[str]`, *optional*):\n        A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n        gathering predictions.\n    metric_key_prefix (`str`, *optional*, defaults to `\"test\"`):\n        An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n        \"test_bleu\" if the prefix is \"test\" (default)\n\n<Tip>\n\nIf your predictions or labels have different sequence length (for instance because you're doing dynamic padding\nin a token classification task) the predictions will be padded (on the right) to allow for concatenation into\none array. The padding index is -100.\n\n</Tip>\n\nReturns: *NamedTuple* A namedtuple with the following keys:\n\n    - predictions (`np.ndarray`): The predictions on `test_dataset`.\n    - label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained some).\n    - metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics (if the dataset contained\n      labels)."
  ],
  [
    "def evaluation_loop(\n        self,\n        dataloader: DataLoader,\n        description: str,\n        prediction_loss_only: Optional[bool] = None,\n        ignore_keys: Optional[List[str]] = None,\n        metric_key_prefix: str = \"eval\",\n    ) -> EvalLoopOutput:\n        \"\"\"\n        Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n\n        Works both with or without labels.\n        \"\"\"\n        args = self.args\n\n        prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n\n        # if eval is called w/o train init deepspeed here\n        if args.deepspeed and not self.deepspeed:\n            # XXX: eval doesn't have `resume_from_checkpoint` arg but we should be able to do eval\n            # from the checkpoint eventually\n            deepspeed_engine, _, _ = deepspeed_init(\n                self, num_training_steps=0, resume_from_checkpoint=None, inference=True\n            )\n            self.model = deepspeed_engine.module\n            self.model_wrapped = deepspeed_engine\n            self.deepspeed = deepspeed_engine\n\n        model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n\n        # if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn't called\n        # while ``train`` is running, cast it to the right dtype first and then put on device\n        if not self.is_in_train:\n            if args.fp16_full_eval:\n                model = model.to(dtype=torch.float16, device=args.device)\n            elif args.bf16_full_eval:\n                model = model.to(dtype=torch.bfloat16, device=args.device)\n\n        batch_size = self.args.eval_batch_size\n\n        logger.info(f\"***** Running {description} *****\")\n        if has_length(dataloader):\n            logger.info(f\"  Num examples = {self.num_examples(dataloader)}\")\n        else:\n            logger.info(\"  Num examples: Unknown\")\n        logger.info(f\"  Batch size = {batch_size}\")\n\n        model.eval()\n\n        self.callback_handler.eval_dataloader = dataloader\n        # Do this before wrapping.\n        eval_dataset = getattr(dataloader, \"dataset\", None)\n\n        if is_torch_tpu_available():\n            dataloader = pl.ParallelLoader(dataloader, [args.device]).per_device_loader(args.device)\n\n        if args.past_index >= 0:\n            self._past = None\n\n        # Initialize containers\n        # losses/preds/labels on GPU/TPU (accumulated for eval_accumulation_steps)\n        losses_host = None\n        preds_host = None\n        labels_host = None\n        inputs_host = None\n\n        # losses/preds/labels on CPU (final containers)\n        all_losses = None\n        all_preds = None\n        all_labels = None\n        all_inputs = None\n        # Will be useful when we have an iterable dataset so don't know its length.\n\n        observed_num_examples = 0\n        # Main evaluation loop\n        for step, inputs in enumerate(dataloader):\n            # Update the observed num examples\n            observed_batch_size = find_batch_size(inputs)\n            if observed_batch_size is not None:\n                observed_num_examples += observed_batch_size\n                # For batch samplers, batch_size is not known by the dataloader in advance.\n                if batch_size is None:\n                    batch_size = observed_batch_size\n\n            # Prediction step\n            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n            inputs_decode = self._prepare_input(inputs[\"input_ids\"]) if args.include_inputs_for_metrics else None\n\n            if is_torch_tpu_available():\n                xm.mark_step()\n\n            # Update containers on host\n            if loss is not None:\n                losses = self._nested_gather(loss.repeat(batch_size))\n                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)\n            if labels is not None:\n                labels = self._pad_across_processes(labels)\n                labels = self._nested_gather(labels)\n                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)\n            if inputs_decode is not None:\n                inputs_decode = self._pad_across_processes(inputs_decode)\n                inputs_decode = self._nested_gather(inputs_decode)\n                inputs_host = (\n                    inputs_decode\n                    if inputs_host is None\n                    else nested_concat(inputs_host, inputs_decode, padding_index=-100)\n                )\n            if logits is not None:\n                logits = self._pad_across_processes(logits)\n                logits = self._nested_gather(logits)\n                if self.preprocess_logits_for_metrics is not None:\n                    logits = self.preprocess_logits_for_metrics(logits, labels)\n                preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\n            self.control = self.callback_handler.on_prediction_step(args, self.state, self.control)\n\n            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\n            if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:\n                if losses_host is not None:\n                    losses = nested_numpify(losses_host)\n                    all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n                if preds_host is not None:\n                    logits = nested_numpify(preds_host)\n                    all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n                if inputs_host is not None:\n                    inputs_decode = nested_numpify(inputs_host)\n                    all_inputs = (\n                        inputs_decode\n                        if all_inputs is None\n                        else nested_concat(all_inputs, inputs_decode, padding_index=-100)\n                    )\n                if labels_host is not None:\n                    labels = nested_numpify(labels_host)\n                    all_labels = (\n                        labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n                    )\n\n                # Set back to None to begin a new accumulation\n                losses_host, preds_host, inputs_host, labels_host = None, None, None, None\n\n        if args.past_index and hasattr(self, \"_past\"):\n            # Clean the state at the end of the evaluation loop\n            delattr(self, \"_past\")\n\n        # Gather all remaining tensors and put them back on the CPU\n        if losses_host is not None:\n            losses = nested_numpify(losses_host)\n            all_losses = losses if all_losses is None else np.concatenate((all_losses, losses), axis=0)\n        if preds_host is not None:\n            logits = nested_numpify(preds_host)\n            all_preds = logits if all_preds is None else nested_concat(all_preds, logits, padding_index=-100)\n        if inputs_host is not None:\n            inputs_decode = nested_numpify(inputs_host)\n            all_inputs = (\n                inputs_decode if all_inputs is None else nested_concat(all_inputs, inputs_decode, padding_index=-100)\n            )\n        if labels_host is not None:\n            labels = nested_numpify(labels_host)\n            all_labels = labels if all_labels is None else nested_concat(all_labels, labels, padding_index=-100)\n\n        # Number of samples\n        if has_length(eval_dataset):\n            num_samples = len(eval_dataset)\n        # The instance check is weird and does not actually check for the type, but whether the dataset has the right\n        # methods. Therefore we need to make sure it also has the attribute.\n        elif isinstance(eval_dataset, IterableDatasetShard) and getattr(eval_dataset, \"num_examples\", 0) > 0:\n            num_samples = eval_dataset.num_examples\n        else:\n            if has_length(dataloader):\n                num_samples = self.num_examples(dataloader)\n            else:  # both len(dataloader.dataset) and len(dataloader) fail\n                num_samples = observed_num_examples\n        if num_samples == 0 and observed_num_examples > 0:\n            num_samples = observed_num_examples\n\n        # Number of losses has been rounded to a multiple of batch_size and in a distributed training, the number of\n        # samplers has been rounded to a multiple of batch_size, so we truncate.\n        if all_losses is not None:\n            all_losses = all_losses[:num_samples]\n        if all_preds is not None:\n            all_preds = nested_truncate(all_preds, num_samples)\n        if all_labels is not None:\n            all_labels = nested_truncate(all_labels, num_samples)\n        if all_inputs is not None:\n            all_inputs = nested_truncate(all_inputs, num_samples)\n\n        # Metrics!\n        if self.compute_metrics is not None and all_preds is not None and all_labels is not None:\n            if args.include_inputs_for_metrics:\n                metrics = self.compute_metrics(\n                    EvalPrediction(predictions=all_preds, label_ids=all_labels, inputs=all_inputs)\n                )\n            else:\n                metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))\n        else:\n            metrics = {}\n\n        # To be JSON-serializable, we need to remove numpy types or zero-d tensors\n        metrics = denumpify_detensorize(metrics)\n\n        if all_losses is not None:\n            metrics[f\"{metric_key_prefix}_loss\"] = all_losses.mean().item()\n        if hasattr(self, \"jit_compilation_time\"):\n            metrics[f\"{metric_key_prefix}_jit_compilation_time\"] = self.jit_compilation_time\n\n        # Prefix all keys with metric_key_prefix + '_'\n        for key in list(metrics.keys()):\n            if not key.startswith(f\"{metric_key_prefix}_\"):\n                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n\n        return EvalLoopOutput(predictions=all_preds, label_ids=all_labels, metrics=metrics, num_samples=num_samples)",
    "Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n\nWorks both with or without labels."
  ],
  [
    "def _nested_gather(self, tensors, name=None):\n        \"\"\"\n        Gather value of `tensors` (tensor or list/tuple of nested tensors) and convert them to numpy before\n        concatenating them to `gathered`\n        \"\"\"\n        if tensors is None:\n            return\n        if is_torch_tpu_available():\n            if name is None:\n                name = \"nested_gather\"\n            tensors = nested_xla_mesh_reduce(tensors, name)\n        elif is_sagemaker_mp_enabled():\n            tensors = smp_gather(tensors)\n        elif self.args.local_rank != -1:\n            tensors = distributed_concat(tensors)\n        return tensors",
    "Gather value of `tensors` (tensor or list/tuple of nested tensors) and convert them to numpy before\nconcatenating them to `gathered`"
  ],
  [
    "def _pad_across_processes(self, tensor, pad_index=-100):\n        \"\"\"\n        Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\n        they can safely be gathered.\n        \"\"\"\n        if isinstance(tensor, (list, tuple)):\n            return type(tensor)(self._pad_across_processes(t, pad_index=pad_index) for t in tensor)\n        elif isinstance(tensor, dict):\n            return type(tensor)({k: self._pad_across_processes(v, pad_index=pad_index) for k, v in tensor.items()})\n        elif not isinstance(tensor, torch.Tensor):\n            raise TypeError(\n                f\"Can't pad the values of type {type(tensor)}, only of nested list/tuple/dicts of tensors.\"\n            )\n\n        if len(tensor.shape) < 2:\n            return tensor\n        # Gather all sizes\n        size = torch.tensor(tensor.shape, device=tensor.device)[None]\n        sizes = self._nested_gather(size).cpu()\n\n        max_size = max(s[1] for s in sizes)\n        # When extracting XLA graphs for compilation, max_size is 0,\n        # so use inequality to avoid errors.\n        if tensor.shape[1] >= max_size:\n            return tensor\n\n        # Then pad to the maximum size\n        old_size = tensor.shape\n        new_size = list(old_size)\n        new_size[1] = max_size\n        new_tensor = tensor.new_zeros(tuple(new_size)) + pad_index\n        new_tensor[:, : old_size[1]] = tensor\n        return new_tensor",
    "Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\nthey can safely be gathered."
  ],
  [
    "def prediction_step(\n        self,\n        model: nn.Module,\n        inputs: Dict[str, Union[torch.Tensor, Any]],\n        prediction_loss_only: bool,\n        ignore_keys: Optional[List[str]] = None,\n    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n        \"\"\"\n        Perform an evaluation step on `model` using `inputs`.\n\n        Subclass and override to inject custom behavior.\n\n        Args:\n            model (`nn.Module`):\n                The model to evaluate.\n            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n\n                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n                argument `labels`. Check your model's documentation for all accepted arguments.\n            prediction_loss_only (`bool`):\n                Whether or not to return the loss only.\n            ignore_keys (`Lst[str]`, *optional*):\n                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n                gathering predictions.\n\n        Return:\n            Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n            logits and labels (each being optional).\n        \"\"\"\n        has_labels = False if len(self.label_names) == 0 else all(inputs.get(k) is not None for k in self.label_names)\n        # For CLIP-like models capable of returning loss values.\n        # If `return_loss` is not specified or being `None` in `inputs`, we check if the default value of `return_loss`\n        # is `True` in `model.forward`.\n        return_loss = inputs.get(\"return_loss\", None)\n        if return_loss is None:\n            return_loss = self.can_return_loss\n        loss_without_labels = True if len(self.label_names) == 0 and return_loss else False\n\n        inputs = self._prepare_inputs(inputs)\n        if ignore_keys is None:\n            if hasattr(self.model, \"config\"):\n                ignore_keys = getattr(self.model.config, \"keys_to_ignore_at_inference\", [])\n            else:\n                ignore_keys = []\n\n        # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.\n        if has_labels or loss_without_labels:\n            labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))\n            if len(labels) == 1:\n                labels = labels[0]\n        else:\n            labels = None\n\n        with torch.no_grad():\n            if is_sagemaker_mp_enabled():\n                raw_outputs = smp_forward_only(model, inputs)\n                if has_labels or loss_without_labels:\n                    if isinstance(raw_outputs, dict):\n                        loss_mb = raw_outputs[\"loss\"]\n                        logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys + [\"loss\"])\n                    else:\n                        loss_mb = raw_outputs[0]\n                        logits_mb = raw_outputs[1:]\n\n                    loss = loss_mb.reduce_mean().detach().cpu()\n                    logits = smp_nested_concat(logits_mb)\n                else:\n                    loss = None\n                    if isinstance(raw_outputs, dict):\n                        logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys)\n                    else:\n                        logits_mb = raw_outputs\n                    logits = smp_nested_concat(logits_mb)\n            else:\n                if has_labels or loss_without_labels:\n                    with self.compute_loss_context_manager():\n                        loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n                    loss = loss.mean().detach()\n\n                    if isinstance(outputs, dict):\n                        logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + [\"loss\"])\n                    else:\n                        logits = outputs[1:]\n                else:\n                    loss = None\n                    with self.compute_loss_context_manager():\n                        outputs = model(**inputs)\n                    if isinstance(outputs, dict):\n                        logits = tuple(v for k, v in outputs.items() if k not in ignore_keys)\n                    else:\n                        logits = outputs\n                    # TODO: this needs to be fixed and made cleaner later.\n                    if self.args.past_index >= 0:\n                        self._past = outputs[self.args.past_index - 1]\n\n        if prediction_loss_only:\n            return (loss, None, None)\n\n        logits = nested_detach(logits)\n        if len(logits) == 1:\n            logits = logits[0]\n\n        return (loss, logits, labels)",
    "Perform an evaluation step on `model` using `inputs`.\n\nSubclass and override to inject custom behavior.\n\nArgs:\n    model (`nn.Module`):\n        The model to evaluate.\n    inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n        The inputs and targets of the model.\n\n        The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n        argument `labels`. Check your model's documentation for all accepted arguments.\n    prediction_loss_only (`bool`):\n        Whether or not to return the loss only.\n    ignore_keys (`Lst[str]`, *optional*):\n        A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n        gathering predictions.\n\nReturn:\n    Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n    logits and labels (each being optional)."
  ],
  [
    "def floating_point_ops(self, inputs: Dict[str, Union[torch.Tensor, Any]]):\n        \"\"\"\n        For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\n        operations for every backward + forward pass. If using another model, either implement such a method in the\n        model or subclass and override this method.\n\n        Args:\n            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n\n        Returns:\n            `int`: The number of floating-point operations.\n        \"\"\"\n        if hasattr(self.model, \"floating_point_ops\"):\n            return self.model.floating_point_ops(inputs)\n        else:\n            return 0",
    "For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\noperations for every backward + forward pass. If using another model, either implement such a method in the\nmodel or subclass and override this method.\n\nArgs:\n    inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n        The inputs and targets of the model.\n\nReturns:\n    `int`: The number of floating-point operations."
  ],
  [
    "def init_git_repo(self, at_init: bool = False):\n        \"\"\"\n        Initializes a git repo in `self.args.hub_model_id`.\n\n        Args:\n            at_init (`bool`, *optional*, defaults to `False`):\n                Whether this function is called before any training or not. If `self.args.overwrite_output_dir` is\n                `True` and `at_init` is `True`, the path to the repo (which is `self.args.output_dir`) might be wiped\n                out.\n        \"\"\"\n        if not self.is_world_process_zero():\n            return\n        if self.args.hub_model_id is None:\n            repo_name = Path(self.args.output_dir).absolute().name\n        else:\n            repo_name = self.args.hub_model_id\n        if \"/\" not in repo_name:\n            repo_name = get_full_repo_name(repo_name, token=self.args.hub_token)\n\n        # Make sure the repo exists.\n        create_repo(repo_name, token=self.args.hub_token, private=self.args.hub_private_repo, exist_ok=True)\n        try:\n            self.repo = Repository(self.args.output_dir, clone_from=repo_name, token=self.args.hub_token)\n        except EnvironmentError:\n            if self.args.overwrite_output_dir and at_init:\n                # Try again after wiping output_dir\n                shutil.rmtree(self.args.output_dir)\n                self.repo = Repository(self.args.output_dir, clone_from=repo_name, token=self.args.hub_token)\n            else:\n                raise\n\n        self.repo.git_pull()\n\n        # By default, ignore the checkpoint folders\n        if (\n            not os.path.exists(os.path.join(self.args.output_dir, \".gitignore\"))\n            and self.args.hub_strategy != HubStrategy.ALL_CHECKPOINTS\n        ):\n            with open(os.path.join(self.args.output_dir, \".gitignore\"), \"w\", encoding=\"utf-8\") as writer:\n                writer.writelines([\"checkpoint-*/\"])\n\n        # Add \"*.sagemaker\" to .gitignore if using SageMaker\n        if os.environ.get(\"SM_TRAINING_ENV\"):\n            self._add_sm_patterns_to_gitignore()\n\n        self.push_in_progress = None",
    "Initializes a git repo in `self.args.hub_model_id`.\n\nArgs:\n    at_init (`bool`, *optional*, defaults to `False`):\n        Whether this function is called before any training or not. If `self.args.overwrite_output_dir` is\n        `True` and `at_init` is `True`, the path to the repo (which is `self.args.output_dir`) might be wiped\n        out."
  ],
  [
    "def create_model_card(\n        self,\n        language: Optional[str] = None,\n        license: Optional[str] = None,\n        tags: Union[str, List[str], None] = None,\n        model_name: Optional[str] = None,\n        finetuned_from: Optional[str] = None,\n        tasks: Union[str, List[str], None] = None,\n        dataset_tags: Union[str, List[str], None] = None,\n        dataset: Union[str, List[str], None] = None,\n        dataset_args: Union[str, List[str], None] = None,\n    ):\n        \"\"\"\n        Creates a draft of a model card using the information available to the `Trainer`.\n\n        Args:\n            language (`str`, *optional*):\n                The language of the model (if applicable)\n            license (`str`, *optional*):\n                The license of the model. Will default to the license of the pretrained model used, if the original\n                model given to the `Trainer` comes from a repo on the Hub.\n            tags (`str` or `List[str]`, *optional*):\n                Some tags to be included in the metadata of the model card.\n            model_name (`str`, *optional*):\n                The name of the model.\n            finetuned_from (`str`, *optional*):\n                The name of the model used to fine-tune this one (if applicable). Will default to the name of the repo\n                of the original model given to the `Trainer` (if it comes from the Hub).\n            tasks (`str` or `List[str]`, *optional*):\n                One or several task identifiers, to be included in the metadata of the model card.\n            dataset_tags (`str` or `List[str]`, *optional*):\n                One or several dataset tags, to be included in the metadata of the model card.\n            dataset (`str` or `List[str]`, *optional*):\n                One or several dataset identifiers, to be included in the metadata of the model card.\n            dataset_args (`str` or `List[str]`, *optional*):\n               One or several dataset arguments, to be included in the metadata of the model card.\n        \"\"\"\n        if not self.is_world_process_zero():\n            return\n\n        training_summary = TrainingSummary.from_trainer(\n            self,\n            language=language,\n            license=license,\n            tags=tags,\n            model_name=model_name,\n            finetuned_from=finetuned_from,\n            tasks=tasks,\n            dataset_tags=dataset_tags,\n            dataset=dataset,\n            dataset_args=dataset_args,\n        )\n        model_card = training_summary.to_model_card()\n        with open(os.path.join(self.args.output_dir, \"README.md\"), \"w\") as f:\n            f.write(model_card)",
    "Creates a draft of a model card using the information available to the `Trainer`.\n\nArgs:\n    language (`str`, *optional*):\n        The language of the model (if applicable)\n    license (`str`, *optional*):\n        The license of the model. Will default to the license of the pretrained model used, if the original\n        model given to the `Trainer` comes from a repo on the Hub.\n    tags (`str` or `List[str]`, *optional*):\n        Some tags to be included in the metadata of the model card.\n    model_name (`str`, *optional*):\n        The name of the model.\n    finetuned_from (`str`, *optional*):\n        The name of the model used to fine-tune this one (if applicable). Will default to the name of the repo\n        of the original model given to the `Trainer` (if it comes from the Hub).\n    tasks (`str` or `List[str]`, *optional*):\n        One or several task identifiers, to be included in the metadata of the model card.\n    dataset_tags (`str` or `List[str]`, *optional*):\n        One or several dataset tags, to be included in the metadata of the model card.\n    dataset (`str` or `List[str]`, *optional*):\n        One or several dataset identifiers, to be included in the metadata of the model card.\n    dataset_args (`str` or `List[str]`, *optional*):\n       One or several dataset arguments, to be included in the metadata of the model card."
  ],
  [
    "def push_to_hub(self, commit_message: Optional[str] = \"End of training\", blocking: bool = True, **kwargs) -> str:\n        \"\"\"\n        Upload *self.model* and *self.tokenizer* to the  model hub on the repo *self.args.hub_model_id*.\n\n        Parameters:\n            commit_message (`str`, *optional*, defaults to `\"End of training\"`):\n                Message to commit while pushing.\n            blocking (`bool`, *optional*, defaults to `True`):\n                Whether the function should return only when the `git push` has finished.\n            kwargs:\n                Additional keyword arguments passed along to [`~Trainer.create_model_card`].\n\n        Returns:\n            The url of the commit of your model in the given repository if `blocking=False`, a tuple with the url of\n            the commit and an object to track the progress of the commit if `blocking=True`\n        \"\"\"\n        # If a user calls manually `push_to_hub` with `self.args.push_to_hub = False`, we try to create the repo but\n        # it might fail.\n        if not hasattr(self, \"repo\"):\n            self.init_git_repo()\n\n        model_name = kwargs.pop(\"model_name\", None)\n        if model_name is None and self.args.should_save:\n            if self.args.hub_model_id is None:\n                model_name = Path(self.args.output_dir).name\n            else:\n                model_name = self.args.hub_model_id.split(\"/\")[-1]\n\n        # Needs to be executed on all processes for TPU training, but will only save on the processed determined by\n        # self.args.should_save.\n        self.save_model(_internal_call=True)\n\n        # Only push from one node.\n        if not self.is_world_process_zero():\n            return\n\n        # Cancel any async push in progress if blocking=True. The commits will all be pushed together.\n        if blocking and self.push_in_progress is not None and not self.push_in_progress.is_done:\n            self.push_in_progress._process.kill()\n            self.push_in_progress = None\n\n        git_head_commit_url = self.repo.push_to_hub(\n            commit_message=commit_message, blocking=blocking, auto_lfs_prune=True\n        )\n        # push separately the model card to be independant from the rest of the model\n        if self.args.should_save:\n            self.create_model_card(model_name=model_name, **kwargs)\n            try:\n                self.repo.push_to_hub(\n                    commit_message=\"update model card README.md\", blocking=blocking, auto_lfs_prune=True\n                )\n            except EnvironmentError as exc:\n                logger.error(f\"Error pushing update to the model card. Please read logs and retry.\\n${exc}\")\n\n        return git_head_commit_url",
    "Upload *self.model* and *self.tokenizer* to the  model hub on the repo *self.args.hub_model_id*.\n\nParameters:\n    commit_message (`str`, *optional*, defaults to `\"End of training\"`):\n        Message to commit while pushing.\n    blocking (`bool`, *optional*, defaults to `True`):\n        Whether the function should return only when the `git push` has finished.\n    kwargs:\n        Additional keyword arguments passed along to [`~Trainer.create_model_card`].\n\nReturns:\n    The url of the commit of your model in the given repository if `blocking=False`, a tuple with the url of\n    the commit and an object to track the progress of the commit if `blocking=True`"
  ],
  [
    "def prediction_loop(\n        self,\n        dataloader: DataLoader,\n        description: str,\n        prediction_loss_only: Optional[bool] = None,\n        ignore_keys: Optional[List[str]] = None,\n        metric_key_prefix: str = \"eval\",\n    ) -> EvalLoopOutput:\n        \"\"\"\n        Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n\n        Works both with or without labels.\n        \"\"\"\n        args = self.args\n\n        if not has_length(dataloader):\n            raise ValueError(\"dataloader must implement a working __len__\")\n\n        prediction_loss_only = prediction_loss_only if prediction_loss_only is not None else args.prediction_loss_only\n\n        # if eval is called w/o train init deepspeed here\n        if args.deepspeed and not self.deepspeed:\n            # XXX: eval doesn't have `resume_from_checkpoint` arg but we should be able to do eval\n            # from the checkpoint eventually\n            deepspeed_engine, _, _ = deepspeed_init(self, num_training_steps=0, resume_from_checkpoint=None)\n            self.model = deepspeed_engine.module\n            self.model_wrapped = deepspeed_engine\n            self.deepspeed = deepspeed_engine\n            # XXX: we don't need optim/sched for inference, but this needs to be sorted out, since\n            # for example the Z3-optimizer is a must for zero3 to work even for inference - what we\n            # don't need is the deepspeed basic optimizer which is self.optimizer.optimizer\n            deepspeed_engine.optimizer.optimizer = None\n            deepspeed_engine.lr_scheduler = None\n\n        model = self._wrap_model(self.model, training=False, dataloader=dataloader)\n\n        # if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn't called\n        # while ``train`` is running, cast it to the right dtype first and then put on device\n        if not self.is_in_train:\n            if args.fp16_full_eval:\n                model = model.to(dtype=torch.float16, device=args.device)\n            elif args.bf16_full_eval:\n                model = model.to(dtype=torch.bfloat16, device=args.device)\n\n        batch_size = dataloader.batch_size\n        num_examples = self.num_examples(dataloader)\n        logger.info(f\"***** Running {description} *****\")\n        logger.info(f\"  Num examples = {num_examples}\")\n        logger.info(f\"  Batch size = {batch_size}\")\n        losses_host: torch.Tensor = None\n        preds_host: Union[torch.Tensor, List[torch.Tensor]] = None\n        labels_host: Union[torch.Tensor, List[torch.Tensor]] = None\n        inputs_host: Union[torch.Tensor, List[torch.Tensor]] = None\n\n        world_size = max(1, args.world_size)\n\n        eval_losses_gatherer = DistributedTensorGatherer(world_size, num_examples, make_multiple_of=batch_size)\n        if not prediction_loss_only:\n            # The actual number of eval_sample can be greater than num_examples in distributed settings (when we pass\n            # a batch size to the sampler)\n            make_multiple_of = None\n            if hasattr(dataloader, \"sampler\") and isinstance(dataloader.sampler, SequentialDistributedSampler):\n                make_multiple_of = dataloader.sampler.batch_size\n            preds_gatherer = DistributedTensorGatherer(world_size, num_examples, make_multiple_of=make_multiple_of)\n            labels_gatherer = DistributedTensorGatherer(world_size, num_examples, make_multiple_of=make_multiple_of)\n            inputs_gatherer = DistributedTensorGatherer(world_size, num_examples, make_multiple_of=make_multiple_of)\n\n        model.eval()\n\n        if is_torch_tpu_available():\n            dataloader = pl.ParallelLoader(dataloader, [args.device]).per_device_loader(args.device)\n\n        if args.past_index >= 0:\n            self._past = None\n\n        self.callback_handler.eval_dataloader = dataloader\n\n        for step, inputs in enumerate(dataloader):\n            loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n            inputs_decode = self._prepare_input(inputs[\"input_ids\"]) if args.include_inputs_for_metrics else None\n\n            if loss is not None:\n                losses = loss.repeat(batch_size)\n                losses_host = losses if losses_host is None else torch.cat((losses_host, losses), dim=0)\n            if logits is not None:\n                preds_host = logits if preds_host is None else nested_concat(preds_host, logits, padding_index=-100)\n            if labels is not None:\n                labels_host = labels if labels_host is None else nested_concat(labels_host, labels, padding_index=-100)\n            if inputs_decode is not None:\n                inputs_host = (\n                    inputs_decode\n                    if inputs_host is None\n                    else nested_concat(inputs_host, inputs_decode, padding_index=-100)\n                )\n            self.control = self.callback_handler.on_prediction_step(args, self.state, self.control)\n\n            # Gather all tensors and put them back on the CPU if we have done enough accumulation steps.\n            if args.eval_accumulation_steps is not None and (step + 1) % args.eval_accumulation_steps == 0:\n                eval_losses_gatherer.add_arrays(self._gather_and_numpify(losses_host, \"eval_losses\"))\n                if not prediction_loss_only:\n                    preds_gatherer.add_arrays(self._gather_and_numpify(preds_host, \"eval_preds\"))\n                    labels_gatherer.add_arrays(self._gather_and_numpify(labels_host, \"eval_label_ids\"))\n                    inputs_gatherer.add_arrays(self._gather_and_numpify(inputs_host, \"eval_inputs_ids\"))\n\n                # Set back to None to begin a new accumulation\n                losses_host, preds_host, labels_host, inputs_host = None, None, None, None\n\n        if args.past_index and hasattr(self, \"_past\"):\n            # Clean the state at the end of the evaluation loop\n            delattr(self, \"_past\")\n\n        # Gather all remaining tensors and put them back on the CPU\n        eval_losses_gatherer.add_arrays(self._gather_and_numpify(losses_host, \"eval_losses\"))\n        if not prediction_loss_only:\n            preds_gatherer.add_arrays(self._gather_and_numpify(preds_host, \"eval_preds\"))\n            labels_gatherer.add_arrays(self._gather_and_numpify(labels_host, \"eval_label_ids\"))\n            inputs_gatherer.add_arrays(self._gather_and_numpify(inputs_host, \"eval_inputs_ids\"))\n\n        eval_loss = eval_losses_gatherer.finalize()\n        preds = preds_gatherer.finalize() if not prediction_loss_only else None\n        label_ids = labels_gatherer.finalize() if not prediction_loss_only else None\n        inputs_ids = inputs_gatherer.finalize() if not prediction_loss_only else None\n\n        if self.compute_metrics is not None and preds is not None and label_ids is not None:\n            if args.include_inputs_for_metrics:\n                metrics = self.compute_metrics(\n                    EvalPrediction(predictions=preds, label_ids=label_ids, inputs=inputs_ids)\n                )\n            else:\n                metrics = self.compute_metrics(EvalPrediction(predictions=preds, label_ids=label_ids))\n        else:\n            metrics = {}\n\n        # To be JSON-serializable, we need to remove numpy types or zero-d tensors\n        metrics = denumpify_detensorize(metrics)\n\n        if eval_loss is not None:\n            metrics[f\"{metric_key_prefix}_loss\"] = eval_loss.mean().item()\n\n        # Prefix all keys with metric_key_prefix + '_'\n        for key in list(metrics.keys()):\n            if not key.startswith(f\"{metric_key_prefix}_\"):\n                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n\n        return EvalLoopOutput(predictions=preds, label_ids=label_ids, metrics=metrics, num_samples=num_examples)",
    "Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n\nWorks both with or without labels."
  ],
  [
    "def _gather_and_numpify(self, tensors, name):\n        \"\"\"\n        Gather value of `tensors` (tensor or list/tuple of nested tensors) and convert them to numpy before\n        concatenating them to `gathered`\n        \"\"\"\n        if tensors is None:\n            return\n        if is_torch_tpu_available():\n            tensors = nested_xla_mesh_reduce(tensors, name)\n        elif is_sagemaker_mp_enabled():\n            tensors = smp_gather(tensors)\n        elif self.args.local_rank != -1:\n            tensors = distributed_concat(tensors)\n\n        return nested_numpify(tensors)",
    "Gather value of `tensors` (tensor or list/tuple of nested tensors) and convert them to numpy before\nconcatenating them to `gathered`"
  ],
  [
    "def _add_sm_patterns_to_gitignore(self) -> None:\n        \n        # Make sure we only do this on the main process\n        if not self.is_world_process_zero():\n            return\n\n        patterns = [\"*.sagemaker-uploading\", \"*.sagemaker-uploaded\"]\n\n        # Get current .gitignore content\n        if os.path.exists(os.path.join(self.repo.local_dir, \".gitignore\")):\n            with open(os.path.join(self.repo.local_dir, \".gitignore\"), \"r\") as f:\n                current_content = f.read()\n        else:\n            current_content = \"\"\n\n        # Add the patterns to .gitignore\n        content = current_content\n        for pattern in patterns:\n            if pattern not in content:\n                if content.endswith(\"\\n\"):\n                    content += pattern\n                else:\n                    content += f\"\\n{pattern}\"\n\n        # Write the .gitignore file if it has changed\n        if content != current_content:\n            with open(os.path.join(self.repo.local_dir, \".gitignore\"), \"w\") as f:\n                logger.debug(f\"Writing .gitignore file. Content: {content}\")\n                f.write(content)\n\n        self.repo.git_add(\".gitignore\")\n\n        # avoid race condition with git status\n        time.sleep(0.5)\n\n        if not self.repo.is_repo_clean():\n            self.repo.git_commit(\"Add *.sagemaker patterns to .gitignore.\")\n            self.repo.git_push()",
    "Add SageMaker Checkpointing patterns to .gitignore file."
  ],
  [
    "def evaluate(\n        self,\n        eval_dataset: Optional[Dataset] = None,\n        ignore_keys: Optional[List[str]] = None,\n        metric_key_prefix: str = \"eval\",\n        **gen_kwargs\n    ) -> Dict[str, float]:\n        \"\"\"\n        Run evaluation and returns metrics.\n\n        The calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n        (pass it to the init `compute_metrics` argument).\n\n        You can also subclass and override this method to inject custom behavior.\n\n        Args:\n            eval_dataset (`Dataset`, *optional*):\n                Pass a dataset if you wish to override `self.eval_dataset`. If it is an [`~datasets.Dataset`], columns\n                not accepted by the `model.forward()` method are automatically removed. It must implement the `__len__`\n                method.\n            ignore_keys (`List[str]`, *optional*):\n                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n                gathering predictions.\n            metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n                An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n                \"eval_bleu\" if the prefix is `\"eval\"` (default)\n            max_length (`int`, *optional*):\n                The maximum target length to use when predicting with the generate method.\n            num_beams (`int`, *optional*):\n                Number of beams for beam search that will be used when predicting with the generate method. 1 means no\n                beam search.\n            gen_kwargs:\n                Additional `generate` specific kwargs.\n\n        Returns:\n            A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n            dictionary also contains the epoch number which comes from the training state.\n        \"\"\"\n\n        gen_kwargs = gen_kwargs.copy()\n        if gen_kwargs.get(\"max_length\") is None and gen_kwargs.get(\"max_new_tokens\") is None:\n            gen_kwargs[\"max_length\"] = self.args.generation_max_length\n        gen_kwargs[\"num_beams\"] = (\n            gen_kwargs[\"num_beams\"] if gen_kwargs.get(\"num_beams\") is not None else self.args.generation_num_beams\n        )\n        self._gen_kwargs = gen_kwargs\n\n        return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)",
    "Run evaluation and returns metrics.\n\nThe calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n(pass it to the init `compute_metrics` argument).\n\nYou can also subclass and override this method to inject custom behavior.\n\nArgs:\n    eval_dataset (`Dataset`, *optional*):\n        Pass a dataset if you wish to override `self.eval_dataset`. If it is an [`~datasets.Dataset`], columns\n        not accepted by the `model.forward()` method are automatically removed. It must implement the `__len__`\n        method.\n    ignore_keys (`List[str]`, *optional*):\n        A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n        gathering predictions.\n    metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n        An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n        \"eval_bleu\" if the prefix is `\"eval\"` (default)\n    max_length (`int`, *optional*):\n        The maximum target length to use when predicting with the generate method.\n    num_beams (`int`, *optional*):\n        Number of beams for beam search that will be used when predicting with the generate method. 1 means no\n        beam search.\n    gen_kwargs:\n        Additional `generate` specific kwargs.\n\nReturns:\n    A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n    dictionary also contains the epoch number which comes from the training state."
  ],
  [
    "def predict(\n        self,\n        test_dataset: Dataset,\n        ignore_keys: Optional[List[str]] = None,\n        metric_key_prefix: str = \"test\",\n        **gen_kwargs\n    ) -> PredictionOutput:\n        \"\"\"\n        Run prediction and returns predictions and potential metrics.\n\n        Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method\n        will also return metrics, like in `evaluate()`.\n\n        Args:\n            test_dataset (`Dataset`):\n                Dataset to run the predictions on. If it is a [`~datasets.Dataset`], columns not accepted by the\n                `model.forward()` method are automatically removed. Has to implement the method `__len__`\n            ignore_keys (`List[str]`, *optional*):\n                A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n                gathering predictions.\n            metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n                An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n                \"eval_bleu\" if the prefix is `\"eval\"` (default)\n            max_length (`int`, *optional*):\n                The maximum target length to use when predicting with the generate method.\n            num_beams (`int`, *optional*):\n                Number of beams for beam search that will be used when predicting with the generate method. 1 means no\n                beam search.\n            gen_kwargs:\n                Additional `generate` specific kwargs.\n\n        <Tip>\n\n        If your predictions or labels have different sequence lengths (for instance because you're doing dynamic\n        padding in a token classification task) the predictions will be padded (on the right) to allow for\n        concatenation into one array. The padding index is -100.\n\n        </Tip>\n\n        Returns: *NamedTuple* A namedtuple with the following keys:\n\n            - predictions (`np.ndarray`): The predictions on `test_dataset`.\n            - label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained some).\n            - metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics (if the dataset contained\n              labels).\n        \"\"\"\n\n        gen_kwargs = gen_kwargs.copy()\n        if gen_kwargs.get(\"max_length\") is None and gen_kwargs.get(\"max_new_tokens\") is None:\n            gen_kwargs[\"max_length\"] = self.args.generation_max_length\n        gen_kwargs[\"num_beams\"] = (\n            gen_kwargs[\"num_beams\"] if gen_kwargs.get(\"num_beams\") is not None else self.args.generation_num_beams\n        )\n        self._gen_kwargs = gen_kwargs\n\n\n        return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)",
    "Run prediction and returns predictions and potential metrics.\n\nDepending on the dataset and your use case, your test dataset may contain labels. In that case, this method\nwill also return metrics, like in `evaluate()`.\n\nArgs:\n    test_dataset (`Dataset`):\n        Dataset to run the predictions on. If it is a [`~datasets.Dataset`], columns not accepted by the\n        `model.forward()` method are automatically removed. Has to implement the method `__len__`\n    ignore_keys (`List[str]`, *optional*):\n        A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n        gathering predictions.\n    metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n        An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n        \"eval_bleu\" if the prefix is `\"eval\"` (default)\n    max_length (`int`, *optional*):\n        The maximum target length to use when predicting with the generate method.\n    num_beams (`int`, *optional*):\n        Number of beams for beam search that will be used when predicting with the generate method. 1 means no\n        beam search.\n    gen_kwargs:\n        Additional `generate` specific kwargs.\n\n<Tip>\n\nIf your predictions or labels have different sequence lengths (for instance because you're doing dynamic\npadding in a token classification task) the predictions will be padded (on the right) to allow for\nconcatenation into one array. The padding index is -100.\n\n</Tip>\n\nReturns: *NamedTuple* A namedtuple with the following keys:\n\n    - predictions (`np.ndarray`): The predictions on `test_dataset`.\n    - label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained some).\n    - metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics (if the dataset contained\n      labels)."
  ],
  [
    "def prediction_step(\n        self,\n        model: nn.Module,\n        inputs: Dict[str, Union[torch.Tensor, Any]],\n        prediction_loss_only: bool,\n        ignore_keys: Optional[List[str]] = None,\n    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n        \"\"\"\n        Perform an evaluation step on `model` using `inputs`.\n\n        Subclass and override to inject custom behavior.\n\n        Args:\n            model (`nn.Module`):\n                The model to evaluate.\n            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.\n\n                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n                argument `labels`. Check your model's documentation for all accepted arguments.\n            prediction_loss_only (`bool`):\n                Whether or not to return the loss only.\n\n        Return:\n            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, logits and\n            labels (each being optional).\n        \"\"\"\n\n        if not self.args.predict_with_generate or prediction_loss_only:\n            return super().prediction_step(\n                model, inputs, prediction_loss_only=prediction_loss_only, ignore_keys=ignore_keys\n            )\n\n        has_labels = \"labels\" in inputs\n        inputs = self._prepare_inputs(inputs)\n\n        # XXX: adapt synced_gpus for fairscale as well\n        gen_kwargs = self._gen_kwargs.copy()\n        if gen_kwargs.get(\"max_length\") is None and gen_kwargs.get(\"max_new_tokens\") is None:\n            gen_kwargs[\"max_length\"] = self.model.config.max_length\n        gen_kwargs[\"num_beams\"] = (\n            gen_kwargs[\"num_beams\"] if gen_kwargs.get(\"num_beams\") is not None else self.model.config.num_beams\n        )\n        default_synced_gpus = True if is_deepspeed_zero3_enabled() else False\n        gen_kwargs[\"synced_gpus\"] = (\n            gen_kwargs[\"synced_gpus\"] if gen_kwargs.get(\"synced_gpus\") is not None else default_synced_gpus\n        )\n\n        if \"attention_mask\" in inputs:\n            gen_kwargs[\"attention_mask\"] = inputs.get(\"attention_mask\", None)\n        if \"position_ids\" in inputs:\n            gen_kwargs[\"position_ids\"] = inputs.get(\"position_ids\", None)\n        if \"global_attention_mask\" in inputs:\n            gen_kwargs[\"global_attention_mask\"] = inputs.get(\"global_attention_mask\", None)\n\n        # prepare generation inputs\n        # some encoder-decoder models can have varying encoder's and thus\n        # varying model input names\n        if hasattr(self.model, \"encoder\") and self.model.encoder.main_input_name != self.model.main_input_name:\n            generation_inputs = inputs[self.model.encoder.main_input_name]\n        else:\n            generation_inputs = inputs[self.model.main_input_name]\n\n        gen_kwargs[\"input_ids\"] = generation_inputs\n        generated_tokens = self.model.generate(**gen_kwargs)\n        generated_tokens = generated_tokens[:, generation_inputs.size()[-1]:]\n\n        # in case the batch is shorter than max length, the output should be padded\n        if gen_kwargs.get(\"max_length\") is not None and generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n        elif gen_kwargs.get(\"max_new_tokens\") is not None and generated_tokens.shape[-1] < (\n            gen_kwargs[\"max_new_tokens\"] + 1\n        ):\n            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_new_tokens\"] + 1)\n\n        loss = None\n\n        if self.args.prediction_loss_only:\n            return (loss, None, None)\n\n        if has_labels:\n            labels = inputs[\"labels\"]\n            if gen_kwargs.get(\"max_length\") is not None and labels.shape[-1] < gen_kwargs[\"max_length\"]:\n                labels = self._pad_tensors_to_max_len(labels, gen_kwargs[\"max_length\"])\n            elif gen_kwargs.get(\"max_new_tokens\") is not None and labels.shape[-1] < (\n                gen_kwargs[\"max_new_tokens\"] + 1\n            ):\n                labels = self._pad_tensors_to_max_len(labels, (gen_kwargs[\"max_new_tokens\"] + 1))\n        else:\n            labels = None\n\n        return (loss, generated_tokens, labels)",
    "Perform an evaluation step on `model` using `inputs`.\n\nSubclass and override to inject custom behavior.\n\nArgs:\n    model (`nn.Module`):\n        The model to evaluate.\n    inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n        The inputs and targets of the model.\n\n        The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n        argument `labels`. Check your model's documentation for all accepted arguments.\n    prediction_loss_only (`bool`):\n        Whether or not to return the loss only.\n\nReturn:\n    Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, logits and\n    labels (each being optional)."
  ],
  [
    "def determine_version_specifier() -> str:\n    \"\"\"Determine the version of Black to install.\n\n    The version can be specified either via the `with.version` input or via the\n    pyproject.toml file if `with.use_pyproject` is set to `true`.\n    \"\"\"\n    if USE_PYPROJECT and VERSION:\n        print(\n            \"::error::'with.version' and 'with.use_pyproject' inputs are \"\n            \"mutually exclusive.\",\n            file=sys.stderr,\n            flush=True,\n        )\n        sys.exit(1)\n    if USE_PYPROJECT:\n        return read_version_specifier_from_pyproject()\n    elif VERSION and VERSION[0] in \"0123456789\":\n        return f\"=={VERSION}\"\n    else:\n        return VERSION",
    "Determine the version of Black to install.\n\nThe version can be specified either via the `with.version` input or via the\npyproject.toml file if `with.use_pyproject` is set to `true`."
  ],
  [
    "def replace_pr_numbers_with_links(content: str) -> str:\n    \n    return re.sub(r\"#(\\d+)\", r\"[#\\1](https://github.com/psf/black/pull/\\1)\", content)",
    "Replaces all PR numbers with the corresponding GitHub link."
  ],
  [
    "def handle_include_read(\n    app: Sphinx,\n    relative_path: Path,\n    parent_docname: str,\n    content: list[str],\n) -> None:\n    \n    if parent_docname == \"change_log\":\n        content[0] = replace_pr_numbers_with_links(content[0])",
    "Handler for the include-read sphinx event."
  ],
  [
    "def setup(app: Sphinx) -> None:\n    \n    app.connect(\"include-read\", handle_include_read)",
    "Sets up a minimal sphinx extension."
  ],
  [
    "def get_git_tags(versions_only: bool = True) -> list[str]:\n    \n    cp = run([\"git\", \"tag\"], capture_output=True, check=True, encoding=\"utf8\")\n    if not cp.stdout:\n        LOG.error(f\"Returned no git tags stdout: {cp.stderr}\")\n        raise NoGitTagsError\n    git_tags = cp.stdout.splitlines()\n    if versions_only:\n        return [t for t in git_tags if t[0].isdigit()]\n    return git_tags",
    "Pull out all tags or calvers only"
  ],
  [
    "def tuple_calver(calver: str) -> tuple[int, ...]:  # mypy can't notice maxsplit below\n    \n    try:\n        return tuple(map(int, calver.split(\".\", maxsplit=2)))\n    except ValueError:\n        return (0, 0, 0)",
    "Convert a calver string into a tuple of ints for sorting"
  ],
  [
    "def _handle_debug(debug: bool) -> None:\n    \n    log_level = logging.DEBUG if debug else logging.INFO\n    logging.basicConfig(\n        format=\"[%(asctime)s] %(levelname)s: %(message)s (%(filename)s:%(lineno)d)\",\n        level=log_level,\n    )",
    "Turn on debugging if asked otherwise INFO default"
  ],
  [
    "def add_template_to_changes(self) -> int:\n        \n        LOG.info(f\"Adding template to {self.changes_path}\")\n\n        with self.changes_path.open(\"r\") as cfp:\n            changes_string = cfp.read()\n\n        if \"## Unreleased\" in changes_string:\n            LOG.error(f\"{self.changes_path} already has unreleased template\")\n            return 1\n\n        templated_changes_string = changes_string.replace(\n            \"# Change Log\\n\",\n            f\"# Change Log\\n\\n{NEW_VERSION_CHANGELOG_TEMPLATE}\",\n        )\n\n        with self.changes_path.open(\"w\") as cfp:\n            cfp.write(templated_changes_string)\n\n        LOG.info(f\"Added template to {self.changes_path}\")\n        return 0",
    "Add the template to CHANGES.md if it does not exist"
  ],
  [
    "def get_current_version(self) -> str:\n        \n        return sorted(get_git_tags(), key=lambda k: tuple_calver(k))[-1]",
    "Get the latest git (version) tag as latest version"
  ],
  [
    "def get_next_version(self) -> str:\n        \n        base_calver = datetime.today().strftime(\"%y.%m\")\n        calver_parts = base_calver.split(\".\")\n        base_calver = f\"{calver_parts[0]}.{int(calver_parts[1])}\"  # Remove leading 0\n        git_tags = get_git_tags()\n        same_month_releases = [\n            t for t in git_tags if t.startswith(base_calver) and \"a\" not in t\n        ]\n        if len(same_month_releases) < 1:\n            return f\"{base_calver}.0\"\n        same_month_version = same_month_releases[-1].split(\".\", 2)[-1]\n        return f\"{base_calver}.{int(same_month_version) + 1}\"",
    "Workout the year and month + version number we need to move to"
  ],
  [
    "def update_repo_for_release(self) -> int:\n        \n        self.cleanup_changes_template_for_release()\n        self.update_version_in_docs()\n        return 0  # return 0 if no exceptions hit",
    "Update CHANGES.md + doc files ready for release"
  ],
  [
    "class FakeDateTime:\n    \n\n    def today(*args: Any, **kwargs: Any) -> \"FakeDateTime\":  # noqa\n        return FakeDateTime()\n\n    # Add leading 0 on purpose to ensure we remove it\n    def strftime(*args: Any, **kwargs: Any) -> str:  # noqa\n        return \"69.01\"",
    "Used to mock the date to test generating next calver function"
  ],
  [
    "def run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()",
    "Run migrations in 'offline' mode.\n\nThis configures the context with just a URL\nand not an Engine, though an Engine is acceptable\nhere as well.  By skipping the Engine creation\nwe don't even need a DBAPI to be available.\n\nCalls to context.execute() here emit the given string to the\nscript output."
  ],
  [
    "def run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section, {}),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection, target_metadata=target_metadata\n        )\n\n        with context.begin_transaction():\n            context.run_migrations()",
    "Run migrations in 'online' mode.\n\nIn this scenario we need to create an Engine\nand associate a connection with the context."
  ],
  [
    "class HedgeFundFlow(Base):\n    \n    __tablename__ = \"hedge_fund_flows\"\n    \n    id = Column(Integer, primary_key=True, index=True)\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n    \n    # Flow metadata\n    name = Column(String(200), nullable=False)\n    description = Column(Text, nullable=True)\n    \n    # React Flow state\n    nodes = Column(JSON, nullable=False)  # Store React Flow nodes as JSON\n    edges = Column(JSON, nullable=False)  # Store React Flow edges as JSON\n    viewport = Column(JSON, nullable=True)  # Store viewport state (zoom, x, y)\n    data = Column(JSON, nullable=True)  # Store node internal states (tickers, models, etc.)\n    \n    # Additional metadata\n    is_template = Column(Boolean, default=False)  # Mark as template for reuse\n    tags = Column(JSON, nullable=True)  # Store tags for categorization",
    "Table to store React Flow configurations (nodes, edges, viewport)"
  ],
  [
    "class HedgeFundFlowRun(Base):\n    \n    __tablename__ = \"hedge_fund_flow_runs\"\n    \n    id = Column(Integer, primary_key=True, index=True)\n    flow_id = Column(Integer, ForeignKey(\"hedge_fund_flows.id\"), nullable=False, index=True)\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n    \n    # Run execution tracking\n    status = Column(String(50), nullable=False, default=\"IDLE\")  # IDLE, IN_PROGRESS, COMPLETE, ERROR\n    started_at = Column(DateTime(timezone=True), nullable=True)\n    completed_at = Column(DateTime(timezone=True), nullable=True)\n    \n    # Run configuration\n    trading_mode = Column(String(50), nullable=False, default=\"one-time\")  # one-time, continuous, advisory\n    schedule = Column(String(50), nullable=True)  # hourly, daily, weekly (for continuous mode)\n    duration = Column(String(50), nullable=True)  # 1day, 1week, 1month (for continuous mode)\n    \n    # Run data\n    request_data = Column(JSON, nullable=True)  # Store the request parameters (tickers, agents, models, etc.)\n    initial_portfolio = Column(JSON, nullable=True)  # Store initial portfolio state\n    final_portfolio = Column(JSON, nullable=True)  # Store final portfolio state\n    results = Column(JSON, nullable=True)  # Store the output/results from the run\n    error_message = Column(Text, nullable=True)  # Store error details if run failed\n    \n    # Metadata\n    run_number = Column(Integer, nullable=False, default=1)  # Sequential run number for this flow",
    "Table to track individual execution runs of a hedge fund flow"
  ],
  [
    "class HedgeFundFlowRunCycle(Base):\n    \n    __tablename__ = \"hedge_fund_flow_run_cycles\"\n    \n    id = Column(Integer, primary_key=True, index=True)\n    flow_run_id = Column(Integer, ForeignKey(\"hedge_fund_flow_runs.id\"), nullable=False, index=True)\n    cycle_number = Column(Integer, nullable=False)  # 1, 2, 3, etc. within the run\n    \n    # Timing\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    started_at = Column(DateTime(timezone=True), nullable=False)\n    completed_at = Column(DateTime(timezone=True), nullable=True)\n    \n    # Analysis results\n    analyst_signals = Column(JSON, nullable=True)  # All agent decisions/signals\n    trading_decisions = Column(JSON, nullable=True)  # Portfolio manager decisions\n    executed_trades = Column(JSON, nullable=True)  # Actual trades executed (paper trading)\n    \n    # Portfolio state after this cycle\n    portfolio_snapshot = Column(JSON, nullable=True)  # Cash, positions, performance metrics\n    \n    # Performance metrics for this cycle\n    performance_metrics = Column(JSON, nullable=True)  # Returns, sharpe ratio, etc.\n    \n    # Execution tracking\n    status = Column(String(50), nullable=False, default=\"IN_PROGRESS\")  # IN_PROGRESS, COMPLETED, ERROR\n    error_message = Column(Text, nullable=True)  # Store error details if cycle failed\n    \n    # Cost tracking\n    llm_calls_count = Column(Integer, nullable=True, default=0)  # Number of LLM calls made\n    api_calls_count = Column(Integer, nullable=True, default=0)  # Number of financial API calls made\n    estimated_cost = Column(String(20), nullable=True)  # Estimated cost in USD\n    \n    # Metadata\n    trigger_reason = Column(String(100), nullable=True)  # scheduled, manual, market_event, etc.\n    market_conditions = Column(JSON, nullable=True)  # Market data snapshot at cycle start",
    "Individual analysis cycles within a trading session"
  ],
  [
    "async def startup_event():\n    \n    try:\n        logger.info(\"Checking Ollama availability...\")\n        status = await ollama_service.check_ollama_status()\n        \n        if status[\"installed\"]:\n            if status[\"running\"]:\n                logger.info(f\" Ollama is installed and running at {status['server_url']}\")\n                if status[\"available_models\"]:\n                    logger.info(f\" Available models: {', '.join(status['available_models'])}\")\n                else:\n                    logger.info(\" No models are currently downloaded\")\n            else:\n                logger.info(\" Ollama is installed but not running\")\n                logger.info(\" You can start it from the Settings page or manually with 'ollama serve'\")\n        else:\n            logger.info(\" Ollama is not installed. Install it to use local models.\")\n            logger.info(\" Visit https://ollama.com to download and install Ollama\")\n            \n    except Exception as e:\n        logger.warning(f\"Could not check Ollama status: {e}\")\n        logger.info(\" Ollama integration is available if you install it later\")",
    "Startup event to check Ollama availability."
  ],
  [
    "class BaseEvent(BaseModel):\n    \n\n    type: str\n\n    def to_sse(self) -> str:\n        \"\"\"Convert to Server-Sent Event format\"\"\"\n        event_type = self.type.lower()\n        return f\"event: {event_type}\\ndata: {self.model_dump_json()}\\n\\n\"",
    "Base class for all Server-Sent Event events"
  ],
  [
    "class StartEvent(BaseEvent):\n    \n\n    type: Literal[\"start\"] = \"start\"\n    timestamp: Optional[str] = None",
    "Event indicating the start of processing"
  ],
  [
    "class ProgressUpdateEvent(BaseEvent):\n    \n\n    type: Literal[\"progress\"] = \"progress\"\n    agent: str\n    ticker: Optional[str] = None\n    status: str\n    timestamp: Optional[str] = None\n    analysis: Optional[str] = None",
    "Event containing an agent's progress update"
  ],
  [
    "class CompleteEvent(BaseEvent):\n    \n\n    type: Literal[\"complete\"] = \"complete\"\n    data: Dict[str, Any]\n    timestamp: Optional[str] = None",
    "Event indicating successful completion with results"
  ],
  [
    "class FlowSummaryResponse(BaseModel):\n    \n    id: int\n    name: str\n    description: Optional[str]\n    is_template: bool\n    tags: Optional[List[str]]\n    created_at: datetime\n    updated_at: Optional[datetime]\n\n    class Config:\n        from_attributes = True",
    "Lightweight flow response without nodes/edges for listing"
  ],
  [
    "class FlowRunCreateRequest(BaseModel):\n    \n    request_data: Optional[Dict[str, Any]] = None",
    "Request to create a new flow run"
  ],
  [
    "class FlowRunUpdateRequest(BaseModel):\n    \n    status: Optional[FlowRunStatus] = None\n    results: Optional[Dict[str, Any]] = None\n    error_message: Optional[str] = None",
    "Request to update an existing flow run"
  ],
  [
    "class FlowRunSummaryResponse(BaseModel):\n    \n    id: int\n    flow_id: int\n    status: FlowRunStatus\n    run_number: int\n    created_at: datetime\n    started_at: Optional[datetime]\n    completed_at: Optional[datetime]\n    error_message: Optional[str]\n\n    class Config:\n        from_attributes = True",
    "Lightweight flow run response for listing"
  ],
  [
    "def get_agent_ids(self) -> List[str]:\n        \n        return [node.id for node in self.graph_nodes]",
    "Extract agent IDs from graph structure"
  ],
  [
    "def get_agent_model_config(self, agent_id: str) -> tuple[str, ModelProvider]:\n        \n        if self.agent_models:\n            # Extract base agent key from unique node ID for matching\n            base_agent_key = extract_base_agent_key(agent_id)\n            \n            for config in self.agent_models:\n                # Check both unique node ID and base agent key for matches\n                config_base_key = extract_base_agent_key(config.agent_id)\n                if config.agent_id == agent_id or config_base_key == base_agent_key:\n                    return (\n                        config.model_name or self.model_name,\n                        config.model_provider or self.model_provider\n                    )\n        # Fallback to global model settings\n        return self.model_name, self.model_provider",
    "Get model configuration for a specific agent"
  ],
  [
    "def get_start_date(self) -> str:\n        \n        if self.start_date:\n            return self.start_date\n        return (datetime.strptime(self.end_date, \"%Y-%m-%d\") - timedelta(days=90)).strftime(\"%Y-%m-%d\")",
    "Calculate start date if not provided"
  ],
  [
    "def create_flow(self, name: str, nodes: dict, edges: dict, description: str = None, \n                   viewport: dict = None, data: dict = None, is_template: bool = False, tags: List[str] = None) -> HedgeFundFlow:\n        \n        flow = HedgeFundFlow(\n            name=name,\n            description=description,\n            nodes=nodes,\n            edges=edges,\n            viewport=viewport,\n            data=data,\n            is_template=is_template,\n            tags=tags or []\n        )\n        self.db.add(flow)\n        self.db.commit()\n        self.db.refresh(flow)\n        return flow",
    "Create a new hedge fund flow"
  ],
  [
    "def get_flow_by_id(self, flow_id: int) -> Optional[HedgeFundFlow]:\n        \n        return self.db.query(HedgeFundFlow).filter(HedgeFundFlow.id == flow_id).first()",
    "Get a flow by its ID"
  ],
  [
    "def get_all_flows(self, include_templates: bool = True) -> List[HedgeFundFlow]:\n        \n        query = self.db.query(HedgeFundFlow)\n        if not include_templates:\n            query = query.filter(HedgeFundFlow.is_template == False)\n        return query.order_by(HedgeFundFlow.updated_at.desc()).all()",
    "Get all flows, optionally excluding templates"
  ],
  [
    "def get_flows_by_name(self, name: str) -> List[HedgeFundFlow]:\n        \n        return self.db.query(HedgeFundFlow).filter(\n            HedgeFundFlow.name.ilike(f\"%{name}%\")\n        ).order_by(HedgeFundFlow.updated_at.desc()).all()",
    "Search flows by name (case-insensitive partial match)"
  ],
  [
    "def duplicate_flow(self, flow_id: int, new_name: str = None) -> Optional[HedgeFundFlow]:\n        \n        original = self.get_flow_by_id(flow_id)\n        if not original:\n            return None\n        \n        copy_name = new_name or f\"{original.name} (Copy)\"\n        \n        return self.create_flow(\n            name=copy_name,\n            description=original.description,\n            nodes=original.nodes,\n            edges=original.edges,\n            viewport=original.viewport,\n            data=original.data,\n            is_template=False,  # Copies are not templates by default\n            tags=original.tags\n        )",
    "Create a copy of an existing flow"
  ],
  [
    "def get_flow_run_by_id(self, run_id: int) -> Optional[HedgeFundFlowRun]:\n        \n        return self.db.query(HedgeFundFlowRun).filter(HedgeFundFlowRun.id == run_id).first()",
    "Get a flow run by its ID"
  ],
  [
    "def get_flow_runs_by_flow_id(self, flow_id: int, limit: int = 50, offset: int = 0) -> List[HedgeFundFlowRun]:\n        \n        return (\n            self.db.query(HedgeFundFlowRun)\n            .filter(HedgeFundFlowRun.flow_id == flow_id)\n            .order_by(desc(HedgeFundFlowRun.created_at))\n            .limit(limit)\n            .offset(offset)\n            .all()\n        )",
    "Get all runs for a specific flow, ordered by most recent first"
  ],
  [
    "def get_active_flow_run(self, flow_id: int) -> Optional[HedgeFundFlowRun]:\n        \n        return (\n            self.db.query(HedgeFundFlowRun)\n            .filter(\n                HedgeFundFlowRun.flow_id == flow_id,\n                HedgeFundFlowRun.status == FlowRunStatus.IN_PROGRESS.value\n            )\n            .first()\n        )",
    "Get the current active (IN_PROGRESS) run for a flow"
  ],
  [
    "def get_latest_flow_run(self, flow_id: int) -> Optional[HedgeFundFlowRun]:\n        \n        return (\n            self.db.query(HedgeFundFlowRun)\n            .filter(HedgeFundFlowRun.flow_id == flow_id)\n            .order_by(desc(HedgeFundFlowRun.created_at))\n            .first()\n        )",
    "Get the most recent run for a flow"
  ],
  [
    "def delete_flow_run(self, run_id: int) -> bool:\n        \n        flow_run = self.get_flow_run_by_id(run_id)\n        if not flow_run:\n            return False\n        \n        self.db.delete(flow_run)\n        self.db.commit()\n        return True",
    "Delete a flow run by ID"
  ],
  [
    "def delete_flow_runs_by_flow_id(self, flow_id: int) -> int:\n        \n        deleted_count = (\n            self.db.query(HedgeFundFlowRun)\n            .filter(HedgeFundFlowRun.flow_id == flow_id)\n            .delete()\n        )\n        self.db.commit()\n        return deleted_count",
    "Delete all runs for a specific flow. Returns count of deleted runs."
  ],
  [
    "def get_flow_run_count(self, flow_id: int) -> int:\n        \n        return (\n            self.db.query(HedgeFundFlowRun)\n            .filter(HedgeFundFlowRun.flow_id == flow_id)\n            .count()\n        )",
    "Get total count of runs for a flow"
  ],
  [
    "def _get_next_run_number(self, flow_id: int) -> int:\n        \n        max_run_number = (\n            self.db.query(func.max(HedgeFundFlowRun.run_number))\n            .filter(HedgeFundFlowRun.flow_id == flow_id)\n            .scalar()\n        )\n        return (max_run_number or 0) + 1",
    "Get the next run number for a flow"
  ],
  [
    "async def create_flow_run(\n    flow_id: int, \n    request: FlowRunCreateRequest, \n    db: Session = Depends(get_db)\n):\n    \n    try:\n        # Verify flow exists\n        flow_repo = FlowRepository(db)\n        flow = flow_repo.get_flow_by_id(flow_id)\n        if not flow:\n            raise HTTPException(status_code=404, detail=\"Flow not found\")\n        \n        # Create the flow run\n        run_repo = FlowRunRepository(db)\n        flow_run = run_repo.create_flow_run(\n            flow_id=flow_id,\n            request_data=request.request_data\n        )\n        return FlowRunResponse.from_orm(flow_run)\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to create flow run: {str(e)}\")",
    "Create a new flow run for the specified flow"
  ],
  [
    "async def get_flow_runs(\n    flow_id: int,\n    limit: int = Query(50, ge=1, le=100, description=\"Maximum number of runs to return\"),\n    offset: int = Query(0, ge=0, description=\"Number of runs to skip\"),\n    db: Session = Depends(get_db)\n):\n    \n    try:\n        # Verify flow exists\n        flow_repo = FlowRepository(db)\n        flow = flow_repo.get_flow_by_id(flow_id)\n        if not flow:\n            raise HTTPException(status_code=404, detail=\"Flow not found\")\n        \n        # Get flow runs\n        run_repo = FlowRunRepository(db)\n        flow_runs = run_repo.get_flow_runs_by_flow_id(flow_id, limit=limit, offset=offset)\n        return [FlowRunSummaryResponse.from_orm(run) for run in flow_runs]\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to retrieve flow runs: {str(e)}\")",
    "Get all runs for the specified flow"
  ],
  [
    "async def get_active_flow_run(flow_id: int, db: Session = Depends(get_db)):\n    \n    try:\n        # Verify flow exists\n        flow_repo = FlowRepository(db)\n        flow = flow_repo.get_flow_by_id(flow_id)\n        if not flow:\n            raise HTTPException(status_code=404, detail=\"Flow not found\")\n        \n        # Get active flow run\n        run_repo = FlowRunRepository(db)\n        active_run = run_repo.get_active_flow_run(flow_id)\n        return FlowRunResponse.from_orm(active_run) if active_run else None\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to retrieve active flow run: {str(e)}\")",
    "Get the current active (IN_PROGRESS) run for the specified flow"
  ],
  [
    "async def get_latest_flow_run(flow_id: int, db: Session = Depends(get_db)):\n    \n    try:\n        # Verify flow exists\n        flow_repo = FlowRepository(db)\n        flow = flow_repo.get_flow_by_id(flow_id)\n        if not flow:\n            raise HTTPException(status_code=404, detail=\"Flow not found\")\n        \n        # Get latest flow run\n        run_repo = FlowRunRepository(db)\n        latest_run = run_repo.get_latest_flow_run(flow_id)\n        return FlowRunResponse.from_orm(latest_run) if latest_run else None\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to retrieve latest flow run: {str(e)}\")",
    "Get the most recent run for the specified flow"
  ],
  [
    "async def get_flow_run(flow_id: int, run_id: int, db: Session = Depends(get_db)):\n    \n    try:\n        # Verify flow exists\n        flow_repo = FlowRepository(db)\n        flow = flow_repo.get_flow_by_id(flow_id)\n        if not flow:\n            raise HTTPException(status_code=404, detail=\"Flow not found\")\n        \n        # Get flow run\n        run_repo = FlowRunRepository(db)\n        flow_run = run_repo.get_flow_run_by_id(run_id)\n        if not flow_run or flow_run.flow_id != flow_id:\n            raise HTTPException(status_code=404, detail=\"Flow run not found\")\n        \n        return FlowRunResponse.from_orm(flow_run)\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to retrieve flow run: {str(e)}\")",
    "Get a specific flow run by ID"
  ],
  [
    "async def delete_all_flow_runs(flow_id: int, db: Session = Depends(get_db)):\n    \n    try:\n        # Verify flow exists\n        flow_repo = FlowRepository(db)\n        flow = flow_repo.get_flow_by_id(flow_id)\n        if not flow:\n            raise HTTPException(status_code=404, detail=\"Flow not found\")\n        \n        # Delete all flow runs\n        run_repo = FlowRunRepository(db)\n        deleted_count = run_repo.delete_flow_runs_by_flow_id(flow_id)\n        \n        return {\"message\": f\"Deleted {deleted_count} flow runs successfully\"}\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to delete flow runs: {str(e)}\")",
    "Delete all runs for the specified flow"
  ],
  [
    "async def get_flow_run_count(flow_id: int, db: Session = Depends(get_db)):\n    \n    try:\n        # Verify flow exists\n        flow_repo = FlowRepository(db)\n        flow = flow_repo.get_flow_by_id(flow_id)\n        if not flow:\n            raise HTTPException(status_code=404, detail=\"Flow not found\")\n        \n        # Get run count\n        run_repo = FlowRunRepository(db)\n        count = run_repo.get_flow_run_count(flow_id)\n        \n        return {\"flow_id\": flow_id, \"total_runs\": count}\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to get flow run count: {str(e)}\")",
    "Get the total count of runs for the specified flow"
  ],
  [
    "async def create_flow(request: FlowCreateRequest, db: Session = Depends(get_db)):\n    \n    try:\n        repo = FlowRepository(db)\n        flow = repo.create_flow(\n            name=request.name,\n            description=request.description,\n            nodes=request.nodes,\n            edges=request.edges,\n            viewport=request.viewport,\n            data=request.data,\n            is_template=request.is_template,\n            tags=request.tags\n        )\n        return FlowResponse.from_orm(flow)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to create flow: {str(e)}\")",
    "Create a new hedge fund flow"
  ],
  [
    "async def get_flow(flow_id: int, db: Session = Depends(get_db)):\n    \n    try:\n        repo = FlowRepository(db)\n        flow = repo.get_flow_by_id(flow_id)\n        if not flow:\n            raise HTTPException(status_code=404, detail=\"Flow not found\")\n        return FlowResponse.from_orm(flow)\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to retrieve flow: {str(e)}\")",
    "Get a specific flow by ID"
  ],
  [
    "async def duplicate_flow(flow_id: int, new_name: str = None, db: Session = Depends(get_db)):\n    \n    try:\n        repo = FlowRepository(db)\n        flow = repo.duplicate_flow(flow_id, new_name)\n        if not flow:\n            raise HTTPException(status_code=404, detail=\"Flow not found\")\n        return FlowResponse.from_orm(flow)\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Failed to duplicate flow: {str(e)}\")",
    "Create a copy of an existing flow"
  ],
  [
    "def forward(self, x, x_mask, h, h_mask):\n        \"\"\"\n        x: decoder input\n        h: encoder output\n        \"\"\"\n        self_attn_mask = commons.subsequent_mask(x_mask.size(2)).to(\n            device=x.device, dtype=x.dtype\n        )\n        encdec_attn_mask = h_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n        x = x * x_mask\n        for i in range(self.n_layers):\n            y = self.self_attn_layers[i](x, x, self_attn_mask)\n            y = self.drop(y)\n            x = self.norm_layers_0[i](x + y)\n\n            y = self.encdec_attn_layers[i](x, h, encdec_attn_mask)\n            y = self.drop(y)\n            x = self.norm_layers_1[i](x + y)\n\n            y = self.ffn_layers[i](x, x_mask)\n            y = self.drop(y)\n            x = self.norm_layers_2[i](x + y)\n        x = x * x_mask\n        return x",
    "x: decoder input\nh: encoder output"
  ],
  [
    "def _matmul_with_relative_values(self, x, y):\n        \"\"\"\n        x: [b, h, l, m]\n        y: [h or 1, m, d]\n        ret: [b, h, l, d]\n        \"\"\"\n        ret = torch.matmul(x, y.unsqueeze(0))\n        return ret",
    "x: [b, h, l, m]\ny: [h or 1, m, d]\nret: [b, h, l, d]"
  ],
  [
    "def _matmul_with_relative_keys(self, x, y):\n        \"\"\"\n        x: [b, h, l, d]\n        y: [h or 1, m, d]\n        ret: [b, h, l, m]\n        \"\"\"\n        ret = torch.matmul(x, y.unsqueeze(0).transpose(-2, -1))\n        return ret",
    "x: [b, h, l, d]\ny: [h or 1, m, d]\nret: [b, h, l, m]"
  ],
  [
    "def _relative_position_to_absolute_position(self, x):\n        \"\"\"\n        x: [b, h, l, 2*l-1]\n        ret: [b, h, l, l]\n        \"\"\"\n        batch, heads, length, _ = x.size()\n        # Concat columns of pad to shift from relative to absolute indexing.\n        x = F.pad(x, commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, 1]]))\n\n        # Concat extra elements so to add up to shape (len+1, 2*len-1).\n        x_flat = x.view([batch, heads, length * 2 * length])\n        x_flat = F.pad(\n            x_flat, commons.convert_pad_shape([[0, 0], [0, 0], [0, length - 1]])\n        )\n\n        # Reshape and slice out the padded elements.\n        x_final = x_flat.view([batch, heads, length + 1, 2 * length - 1])[\n            :, :, :length, length - 1 :\n        ]\n        return x_final",
    "x: [b, h, l, 2*l-1]\nret: [b, h, l, l]"
  ],
  [
    "def _absolute_position_to_relative_position(self, x):\n        \"\"\"\n        x: [b, h, l, l]\n        ret: [b, h, l, 2*l-1]\n        \"\"\"\n        batch, heads, length, _ = x.size()\n        # pad along column\n        x = F.pad(\n            x, commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, length - 1]])\n        )\n        x_flat = x.view([batch, heads, length**2 + length * (length - 1)])\n        # add 0's in the beginning that will skew the elements after reshape\n        x_flat = F.pad(x_flat, commons.convert_pad_shape([[0, 0], [0, 0], [length, 0]]))\n        x_final = x_flat.view([batch, heads, length, 2 * length])[:, :, :, 1:]\n        return x_final",
    "x: [b, h, l, l]\nret: [b, h, l, 2*l-1]"
  ],
  [
    "def _attention_bias_proximal(self, length):\n        \"\"\"Bias for self-attention to encourage attention to close positions.\n        Args:\n          length: an integer scalar.\n        Returns:\n          a Tensor with shape [1, 1, length, length]\n        \"\"\"\n        r = torch.arange(length, dtype=torch.float32)\n        diff = torch.unsqueeze(r, 0) - torch.unsqueeze(r, 1)\n        return torch.unsqueeze(torch.unsqueeze(-torch.log1p(torch.abs(diff)), 0), 0)",
    "Bias for self-attention to encourage attention to close positions.\nArgs:\n  length: an integer scalar.\nReturns:\n  a Tensor with shape [1, 1, length, length]"
  ],
  [
    "def rand_gumbel(shape):\n    \n    uniform_samples = torch.rand(shape) * 0.99998 + 0.00001\n    return -torch.log(-torch.log(uniform_samples))",
    "Sample from the Gumbel distribution, protect from overflows."
  ],
  [
    "def generate_path(duration, mask):\n    \"\"\"\n    duration: [b, 1, t_x]\n    mask: [b, 1, t_y, t_x]\n    \"\"\"\n\n    b, _, t_y, t_x = mask.shape\n    cum_duration = torch.cumsum(duration, -1)\n\n    cum_duration_flat = cum_duration.view(b * t_x)\n    path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype)\n    path = path.view(b, t_x, t_y)\n    path = path - F.pad(path, convert_pad_shape([[0, 0], [1, 0], [0, 0]]))[:, :-1]\n    path = path.unsqueeze(1).transpose(2, 3) * mask\n    return path",
    "duration: [b, 1, t_x]\nmask: [b, 1, t_y, t_x]"
  ],
  [
    "def dynamic_range_decompression_torch(x, C=1):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor used to compress\n    \"\"\"\n    return torch.exp(x) / C",
    "PARAMS\n------\nC: compression factor used to compress"
  ],
  [
    "class ReferenceEncoder(nn.Module):\n    \"\"\"\n    inputs --- [N, Ty/r, n_mels*r]  mels\n    outputs --- [N, ref_enc_gru_size]\n    \"\"\"\n\n    def __init__(self, spec_channels, gin_channels=0, layernorm=True):\n        super().__init__()\n        self.spec_channels = spec_channels\n        ref_enc_filters = [32, 32, 64, 64, 128, 128]\n        K = len(ref_enc_filters)\n        filters = [1] + ref_enc_filters\n        convs = [\n            weight_norm(\n                nn.Conv2d(\n                    in_channels=filters[i],\n                    out_channels=filters[i + 1],\n                    kernel_size=(3, 3),\n                    stride=(2, 2),\n                    padding=(1, 1),\n                )\n            )\n            for i in range(K)\n        ]\n        self.convs = nn.ModuleList(convs)\n\n        out_channels = self.calculate_channels(spec_channels, 3, 2, 1, K)\n        self.gru = nn.GRU(\n            input_size=ref_enc_filters[-1] * out_channels,\n            hidden_size=256 // 2,\n            batch_first=True,\n        )\n        self.proj = nn.Linear(128, gin_channels)\n        if layernorm:\n            self.layernorm = nn.LayerNorm(self.spec_channels)\n        else:\n            self.layernorm = None\n\n    def forward(self, inputs, mask=None):\n        N = inputs.size(0)\n\n        out = inputs.view(N, 1, -1, self.spec_channels)  # [N, 1, Ty, n_freqs]\n        if self.layernorm is not None:\n            out = self.layernorm(out)\n\n        for conv in self.convs:\n            out = conv(out)\n            # out = wn(out)\n            out = F.relu(out)  # [N, 128, Ty//2^K, n_mels//2^K]\n\n        out = out.transpose(1, 2)  # [N, Ty//2^K, 128, n_mels//2^K]\n        T = out.size(1)\n        N = out.size(0)\n        out = out.contiguous().view(N, T, -1)  # [N, Ty//2^K, 128*n_mels//2^K]\n\n        self.gru.flatten_parameters()\n        memory, out = self.gru(out)  # out --- [1, N, 128]\n\n        return self.proj(out.squeeze(0))\n\n    def calculate_channels(self, L, kernel_size, stride, pad, n_convs):\n        for i in range(n_convs):\n            L = (L - kernel_size + 2 * pad) // stride + 1\n        return L",
    "inputs --- [N, Ty/r, n_mels*r]  mels\noutputs --- [N, ref_enc_gru_size]"
  ],
  [
    "def text_to_sequence(text, symbols, cleaner_names):\r\n  '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\r\n    Args:\r\n      text: string to convert to a sequence\r\n      cleaner_names: names of the cleaner functions to run the text through\r\n    Returns:\r\n      List of integers corresponding to the symbols in the text\r\n  '''\r\n  sequence = []\r\n  symbol_to_id = {s: i for i, s in enumerate(symbols)}\r\n  clean_text = _clean_text(text, cleaner_names)\r\n  print(clean_text)\r\n  print(f\" length:{len(clean_text)}\")\r\n  for symbol in clean_text:\r\n    if symbol not in symbol_to_id.keys():\r\n      continue\r\n    symbol_id = symbol_to_id[symbol]\r\n    sequence += [symbol_id]\r\n  print(f\" length:{len(sequence)}\")\r\n  return sequence",
    "Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\nArgs:\n  text: string to convert to a sequence\n  cleaner_names: names of the cleaner functions to run the text through\nReturns:\n  List of integers corresponding to the symbols in the text"
  ],
  [
    "def cleaned_text_to_sequence(cleaned_text, symbols):\r\n  '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\r\n    Args:\r\n      text: string to convert to a sequence\r\n    Returns:\r\n      List of integers corresponding to the symbols in the text\r\n  '''\r\n  symbol_to_id = {s: i for i, s in enumerate(symbols)}\r\n  sequence = [symbol_to_id[symbol] for symbol in cleaned_text if symbol in symbol_to_id.keys()]\r\n  return sequence",
    "Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\nArgs:\n  text: string to convert to a sequence\nReturns:\n  List of integers corresponding to the symbols in the text"
  ],
  [
    "def cleaned_text_to_sequence_vits2(cleaned_text, tones, language, symbols, languages):\r\n    \"\"\"Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\r\n    Args:\r\n      text: string to convert to a sequence\r\n    Returns:\r\n      List of integers corresponding to the symbols in the text\r\n    \"\"\"\r\n    symbol_to_id = {s: i for i, s in enumerate(symbols)}\r\n    language_id_map = {s: i for i, s in enumerate(languages)}\r\n    phones = [symbol_to_id[symbol] for symbol in cleaned_text]\r\n    tone_start = language_tone_start_map[language]\r\n    tones = [i + tone_start for i in tones]\r\n    lang_id = language_id_map[language]\r\n    lang_ids = [lang_id for i in phones]\r\n    return phones, tones, lang_ids",
    "Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\nArgs:\n  text: string to convert to a sequence\nReturns:\n  List of integers corresponding to the symbols in the text"
  ],
  [
    "def sequence_to_text(sequence):\r\n  \r\n  result = ''\r\n  for symbol_id in sequence:\r\n    s = _id_to_symbol[symbol_id]\r\n    result += s\r\n  return result",
    "Converts a sequence of IDs back to a string"
  ],
  [
    "def split_sentences_latin(text, min_len=10):\n    \"\"\"Split Long sentences into list of short ones\n\n    Args:\n        str: Input sentences.\n\n    Returns:\n        List[str]: list of output sentences.\n    \"\"\"\n    # deal with dirty sentences\n    text = re.sub('[]', '.', text)\n    text = re.sub('[]', ',', text)\n    text = re.sub('[]', '\"', text)\n    text = re.sub('[]', \"'\", text)\n    text = re.sub(r\"[\\<\\>\\(\\)\\[\\]\\\"\\\\]+\", \"\", text)\n    text = re.sub('[\\n\\t ]+', ' ', text)\n    text = re.sub('([,.!?;])', r'\\1 $#!', text)\n    # split\n    sentences = [s.strip() for s in text.split('$#!')]\n    if len(sentences[-1]) == 0: del sentences[-1]\n\n    new_sentences = []\n    new_sent = []\n    count_len = 0\n    for ind, sent in enumerate(sentences):\n        # print(sent)\n        new_sent.append(sent)\n        count_len += len(sent.split(\" \"))\n        if count_len > min_len or ind == len(sentences) - 1:\n            count_len = 0\n            new_sentences.append(' '.join(new_sent))\n            new_sent = []\n    return merge_short_sentences_latin(new_sentences)",
    "Split Long sentences into list of short ones\n\nArgs:\n    str: Input sentences.\n\nReturns:\n    List[str]: list of output sentences."
  ],
  [
    "def merge_short_sentences_latin(sens):\n    \"\"\"Avoid short sentences by merging them with the following sentence.\n\n    Args:\n        List[str]: list of input sentences.\n\n    Returns:\n        List[str]: list of output sentences.\n    \"\"\"\n    sens_out = []\n    for s in sens:\n        # If the previous sentence is too short, merge them with\n        # the current sentence.\n        if len(sens_out) > 0 and len(sens_out[-1].split(\" \")) <= 2:\n            sens_out[-1] = sens_out[-1] + \" \" + s\n        else:\n            sens_out.append(s)\n    try:\n        if len(sens_out[-1].split(\" \")) <= 2:\n            sens_out[-2] = sens_out[-2] + \" \" + sens_out[-1]\n            sens_out.pop(-1)\n    except:\n        pass\n    return sens_out",
    "Avoid short sentences by merging them with the following sentence.\n\nArgs:\n    List[str]: list of input sentences.\n\nReturns:\n    List[str]: list of output sentences."
  ],
  [
    "def merge_short_sentences_zh(sens):\n    # return sens\n    \"\"\"Avoid short sentences by merging them with the following sentence.\n\n    Args:\n        List[str]: list of input sentences.\n\n    Returns:\n        List[str]: list of output sentences.\n    \"\"\"\n    sens_out = []\n    for s in sens:\n        # If the previous sentense is too short, merge them with\n        # the current sentence.\n        if len(sens_out) > 0 and len(sens_out[-1]) <= 2:\n            sens_out[-1] = sens_out[-1] + \" \" + s\n        else:\n            sens_out.append(s)\n    try:\n        if len(sens_out[-1]) <= 2:\n            sens_out[-2] = sens_out[-2] + \" \" + sens_out[-1]\n            sens_out.pop(-1)\n    except:\n        pass\n    return sens_out",
    "Avoid short sentences by merging them with the following sentence.\n\nArgs:\n    List[str]: list of input sentences.\n\nReturns:\n    List[str]: list of output sentences."
  ],
  [
    "def default_X_scheduler(num_X):\n    \"\"\"\n    Returns the config for a default multi-step LR scheduler such as \"1x\", \"3x\",\n    commonly referred to in papers, where every 1x has the total length of 1440k\n    training images (~12 COCO epochs). LR is decayed twice at the end of training\n    following the strategy defined in \"Rethinking ImageNet Pretraining\", Sec 4.\n\n    Args:\n        num_X: a positive real number\n\n    Returns:\n        DictConfig: configs that define the multiplier for LR during training\n    \"\"\"\n    # total number of iterations assuming 16 batch size, using 1440000/16=90000\n    total_steps_16bs = num_X * 90000\n\n    if num_X <= 2:\n        scheduler = L(MultiStepParamScheduler)(\n            values=[1.0, 0.1, 0.01],\n            # note that scheduler is scale-invariant. This is equivalent to\n            # milestones=[6, 8, 9]\n            milestones=[60000, 80000, 90000],\n        )\n    else:\n        scheduler = L(MultiStepParamScheduler)(\n            values=[1.0, 0.1, 0.01],\n            milestones=[total_steps_16bs - 60000, total_steps_16bs - 20000, total_steps_16bs],\n        )\n    return L(WarmupParamScheduler)(\n        scheduler=scheduler,\n        warmup_length=1000 / total_steps_16bs,\n        warmup_method=\"linear\",\n        warmup_factor=0.001,\n    )",
    "Returns the config for a default multi-step LR scheduler such as \"1x\", \"3x\",\ncommonly referred to in papers, where every 1x has the total length of 1440k\ntraining images (~12 COCO epochs). LR is decayed twice at the end of training\nfollowing the strategy defined in \"Rethinking ImageNet Pretraining\", Sec 4.\n\nArgs:\n    num_X: a positive real number\n\nReturns:\n    DictConfig: configs that define the multiplier for LR during training"
  ],
  [
    "def gpt_35_api_stream(messages: list):\n    \"\"\" ()\n\n    Args:\n        messages (list): \n    \"\"\"\n    stream = client.chat.completions.create(\n        model='gpt-3.5-turbo',\n        messages=messages,\n        stream=True,\n    )\n    for chunk in stream:\n        if chunk.choices[0].delta.content is not None:\n            print(chunk.choices[0].delta.content, end=\"\")",
    " ()\n\nArgs:\n    messages (list): "
  ],
  [
    "def encode_prompt(prompt_instructions):\n    \n    prompt = open(\"./prompt.txt\").read() + \"\\n\"\n\n    for idx, task_dict in enumerate(prompt_instructions):\n        (instruction, input, output) = task_dict[\"instruction\"], task_dict[\"input\"], task_dict[\"output\"]\n        instruction = re.sub(r\"\\s+\", \" \", instruction).strip().rstrip(\":\")\n        input = \"<noinput>\" if input.lower() == \"\" else input\n        prompt += f\"###\\n\"\n        prompt += f\"{idx + 1}. Instruction: {instruction}\\n\"\n        prompt += f\"{idx + 1}. Input:\\n{input}\\n\"\n        prompt += f\"{idx + 1}. Output:\\n{output}\\n\"\n    prompt += f\"###\\n\"\n    prompt += f\"{idx + 2}. Instruction:\"\n    return prompt",
    "Encode multiple prompt instructions into a single string."
  ],
  [
    "def smart_tokenizer_and_embedding_resize(\n    special_tokens_dict: Dict,\n    tokenizer: transformers.PreTrainedTokenizer,\n    model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg",
    "Resize tokenizer and embedding.\n\nNote: This is the unoptimized version that may make your embedding size not be divisible by 64."
  ],
  [
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer, data_args) -> Dict:\n    \n    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=data_args.data_path)\n    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)",
    "Make dataset and collator for supervised fine-tuning."
  ],
  [
    "def openai_completion(\n    prompts: Union[str, Sequence[str], Sequence[dict[str, str]], dict[str, str]],\n    decoding_args: OpenAIDecodingArguments,\n    model_name=\"text-davinci-003\",\n    sleep_time=2,\n    batch_size=1,\n    max_instances=sys.maxsize,\n    max_batches=sys.maxsize,\n    return_text=False,\n    **decoding_kwargs,\n) -> Union[Union[StrOrOpenAIObject], Sequence[StrOrOpenAIObject], Sequence[Sequence[StrOrOpenAIObject]],]:\n    \"\"\"Decode with OpenAI API.\n\n    Args:\n        prompts: A string or a list of strings to complete. If it is a chat model the strings should be formatted\n            as explained here: https://github.com/openai/openai-python/blob/main/chatml.md. If it is a chat model\n            it can also be a dictionary (or list thereof) as explained here:\n            https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n        decoding_args: Decoding arguments.\n        model_name: Model name. Can be either in the format of \"org/model\" or just \"model\".\n        sleep_time: Time to sleep once the rate-limit is hit.\n        batch_size: Number of prompts to send in a single request. Only for non chat model.\n        max_instances: Maximum number of prompts to decode.\n        max_batches: Maximum number of batches to decode. This argument will be deprecated in the future.\n        return_text: If True, return text instead of full completion object (which contains things like logprob).\n        decoding_kwargs: Additional decoding arguments. Pass in `best_of` and `logit_bias` if you need them.\n\n    Returns:\n        A completion or a list of completions.\n        Depending on return_text, return_openai_object, and decoding_args.n, the completion type can be one of\n            - a string (if return_text is True)\n            - an openai_object.OpenAIObject object (if return_text is False)\n            - a list of objects of the above types (if decoding_args.n > 1)\n    \"\"\"\n    is_single_prompt = isinstance(prompts, (str, dict))\n    if is_single_prompt:\n        prompts = [prompts]\n\n    if max_batches < sys.maxsize:\n        logging.warning(\n            \"`max_batches` will be deprecated in the future, please use `max_instances` instead.\"\n            \"Setting `max_instances` to `max_batches * batch_size` for now.\"\n        )\n        max_instances = max_batches * batch_size\n\n    prompts = prompts[:max_instances]\n    num_prompts = len(prompts)\n    prompt_batches = [\n        prompts[batch_id * batch_size : (batch_id + 1) * batch_size]\n        for batch_id in range(int(math.ceil(num_prompts / batch_size)))\n    ]\n\n    completions = []\n    for batch_id, prompt_batch in tqdm.tqdm(\n        enumerate(prompt_batches),\n        desc=\"prompt_batches\",\n        total=len(prompt_batches),\n    ):\n        batch_decoding_args = copy.deepcopy(decoding_args)  # cloning the decoding_args\n\n        while True:\n            try:\n                shared_kwargs = dict(\n                    model=model_name,\n                    **batch_decoding_args.__dict__,\n                    **decoding_kwargs,\n                )\n                completion_batch = openai.Completion.create(prompt=prompt_batch, **shared_kwargs)\n                choices = completion_batch.choices\n\n                for choice in choices:\n                    choice[\"total_tokens\"] = completion_batch.usage.total_tokens\n                completions.extend(choices)\n                break\n            except openai.error.OpenAIError as e:\n                logging.warning(f\"OpenAIError: {e}.\")\n                if \"Please reduce your prompt\" in str(e):\n                    batch_decoding_args.max_tokens = int(batch_decoding_args.max_tokens * 0.8)\n                    logging.warning(f\"Reducing target length to {batch_decoding_args.max_tokens}, Retrying...\")\n                else:\n                    logging.warning(\"Hit request rate limit; retrying...\")\n                    time.sleep(sleep_time)  # Annoying rate limit on requests.\n\n    if return_text:\n        completions = [completion.text for completion in completions]\n    if decoding_args.n > 1:\n        # make completions a nested list, where each entry is a consecutive decoding_args.n of original entries.\n        completions = [completions[i : i + decoding_args.n] for i in range(0, len(completions), decoding_args.n)]\n    if is_single_prompt:\n        # Return non-tuple if only 1 input and 1 generation.\n        (completions,) = completions\n    return completions",
    "Decode with OpenAI API.\n\nArgs:\n    prompts: A string or a list of strings to complete. If it is a chat model the strings should be formatted\n        as explained here: https://github.com/openai/openai-python/blob/main/chatml.md. If it is a chat model\n        it can also be a dictionary (or list thereof) as explained here:\n        https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n    decoding_args: Decoding arguments.\n    model_name: Model name. Can be either in the format of \"org/model\" or just \"model\".\n    sleep_time: Time to sleep once the rate-limit is hit.\n    batch_size: Number of prompts to send in a single request. Only for non chat model.\n    max_instances: Maximum number of prompts to decode.\n    max_batches: Maximum number of batches to decode. This argument will be deprecated in the future.\n    return_text: If True, return text instead of full completion object (which contains things like logprob).\n    decoding_kwargs: Additional decoding arguments. Pass in `best_of` and `logit_bias` if you need them.\n\nReturns:\n    A completion or a list of completions.\n    Depending on return_text, return_openai_object, and decoding_args.n, the completion type can be one of\n        - a string (if return_text is True)\n        - an openai_object.OpenAIObject object (if return_text is False)\n        - a list of objects of the above types (if decoding_args.n > 1)"
  ],
  [
    "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n    \"\"\"Dump a str or dictionary to a file in json format.\n\n    Args:\n        obj: An object to be written.\n        f: A string path to the location on disk.\n        mode: Mode for opening the file.\n        indent: Indent for storing json dictionaries.\n        default: A function to handle non-serializable entries; defaults to `str`.\n    \"\"\"\n    f = _make_w_io_base(f, mode)\n    if isinstance(obj, (dict, list)):\n        json.dump(obj, f, indent=indent, default=default)\n    elif isinstance(obj, str):\n        f.write(obj)\n    else:\n        raise ValueError(f\"Unexpected type: {type(obj)}\")\n    f.close()",
    "Dump a str or dictionary to a file in json format.\n\nArgs:\n    obj: An object to be written.\n    f: A string path to the location on disk.\n    mode: Mode for opening the file.\n    indent: Indent for storing json dictionaries.\n    default: A function to handle non-serializable entries; defaults to `str`."
  ],
  [
    "def jload(f, mode=\"r\"):\n    \n    f = _make_r_io_base(f, mode)\n    jdict = json.load(f)\n    f.close()\n    return jdict",
    "Load a .json file into a dictionary."
  ],
  [
    "def make_diff(\n    path_raw: str, path_tuned: str, path_diff: str, device=\"cpu\",  # \"cuda\" or \"cpu\"\n):\n    \"\"\"Make the weight diff.\n\n    This function is given to present full transparency of how the weight diff was created.\n\n    Run:\n        python weight_diff.py make_diff --path_raw <your_path_raw> --path_tuned <your_path_tuned> --path_diff <your_path_diff>\n    \"\"\"\n    model_tuned: transformers.PreTrainedModel = transformers.AutoModelForCausalLM.from_pretrained(\n        path_tuned,\n        device_map={\"\": torch.device(device)},\n        torch_dtype=torch.float32,\n        low_cpu_mem_usage=True,\n    )\n    model_raw: transformers.PreTrainedModel = transformers.AutoModelForCausalLM.from_pretrained(\n        path_raw,\n        device_map={\"\": torch.device(device)},\n        torch_dtype=torch.float32,\n        low_cpu_mem_usage=True,\n    )\n\n    tokenizer_tuned: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\n        path_tuned\n    )\n    tokenizer_raw: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\n        path_raw\n    )\n    if tokenizer_raw.pad_token is None:\n        smart_tokenizer_and_embedding_resize(\n            special_tokens_dict=dict(pad_token=\"[PAD]\"),\n            model=model_raw,\n            tokenizer=tokenizer_raw,\n        )\n\n    state_dict_tuned = model_tuned.state_dict()\n    state_dict_raw = model_raw.state_dict()\n    for key in tqdm.tqdm(state_dict_tuned):\n        state_dict_tuned[key].add_(-state_dict_raw[key])\n\n    model_tuned.save_pretrained(path_diff)\n    tokenizer_tuned.save_pretrained(path_diff)",
    "Make the weight diff.\n\nThis function is given to present full transparency of how the weight diff was created.\n\nRun:\n    python weight_diff.py make_diff --path_raw <your_path_raw> --path_tuned <your_path_tuned> --path_diff <your_path_diff>"
  ],
  [
    "def recover(\n    path_raw,\n    path_diff,\n    path_tuned: Optional[str] = None,\n    device=\"cpu\",\n    test_inference=True,\n    check_integrity_naively=True,\n):\n    \"\"\"Recover the original weights from the released weight diff.\n\n    This function is given for you to run.\n\n    Things to do before running this:\n        1. Convert Meta's released weights into huggingface format. Follow this guide:\n            https://huggingface.co/docs/transformers/main/model_doc/llama\n        2. Make sure you cloned the released weight diff into your local machine. The weight diff is located at:\n            https://huggingface.co/tatsu-lab/alpaca-7b/tree/main\n        3. Run this function with the correct paths. E.g.,\n            python weight_diff.py recover --path_raw <path_to_step_1_dir> --path_diff <path_to_step_2_dir>\n\n    Additional notes:\n        - If things run too slowly, and you have an 80G GPU lying around, let GPU go brrr by setting `--device \"cuda\"`.\n        - If you want to save the recovered weights, set `--path_tuned <your_path_tuned>`.\n            Next time you can load the recovered weights directly from `<your_path_tuned>`.\n    \"\"\"\n    model_raw: transformers.PreTrainedModel = transformers.AutoModelForCausalLM.from_pretrained(\n        path_raw,\n        device_map={\"\": torch.device(device)},\n        torch_dtype=torch.float32,\n        low_cpu_mem_usage=True,\n    )\n    model_recovered: transformers.PreTrainedModel = transformers.AutoModelForCausalLM.from_pretrained(\n        path_diff,\n        device_map={\"\": torch.device(device)},\n        torch_dtype=torch.float32,\n        low_cpu_mem_usage=True,\n    )\n\n    tokenizer_raw: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\n        path_raw\n    )\n    if tokenizer_raw.pad_token is None:\n        smart_tokenizer_and_embedding_resize(\n            special_tokens_dict=dict(pad_token=\"[PAD]\"),\n            model=model_raw,\n            tokenizer=tokenizer_raw,\n        )\n    tokenizer_recovered: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(\n        path_diff\n    )\n\n    state_dict_recovered = model_recovered.state_dict()\n    state_dict_raw = model_raw.state_dict()\n    for key in tqdm.tqdm(state_dict_recovered):\n        state_dict_recovered[key].add_(state_dict_raw[key])\n\n    if check_integrity_naively:\n        # This is not a rigorous, cryptographically strong integrity check :)\n        allsum = sum(state_dict_recovered[key].sum() for key in state_dict_recovered)\n        assert torch.allclose(\n            allsum, torch.full_like(allsum, fill_value=50637.1836), atol=1e-2, rtol=0\n        ), \"Naive integrity check failed. This could imply that some of the checkpoint files are corrupted.\"\n\n    if path_tuned is not None:\n        model_recovered.save_pretrained(path_tuned)\n        tokenizer_recovered.save_pretrained(path_tuned)\n\n    if test_inference:\n        input_text = (\n            \"Below is an instruction that describes a task. \"\n            \"Write a response that appropriately completes the request.\\r\\n\\r\\n\"\n            \"### Instruction:\\r\\nList three technologies that make life easier.\\r\\n\\r\\n### Response:\"\n        )\n        inputs = tokenizer_recovered(input_text, return_tensors=\"pt\")\n        out = model_recovered.generate(inputs=inputs.input_ids, max_new_tokens=100)\n        output_text = tokenizer_recovered.batch_decode(out, skip_special_tokens=True)[0]\n        output_text = output_text[len(input_text) :]\n        print(f\"Input: {input_text}\\nCompletion: {output_text}\")\n\n    return model_recovered, tokenizer_recovered",
    "Recover the original weights from the released weight diff.\n\nThis function is given for you to run.\n\nThings to do before running this:\n    1. Convert Meta's released weights into huggingface format. Follow this guide:\n        https://huggingface.co/docs/transformers/main/model_doc/llama\n    2. Make sure you cloned the released weight diff into your local machine. The weight diff is located at:\n        https://huggingface.co/tatsu-lab/alpaca-7b/tree/main\n    3. Run this function with the correct paths. E.g.,\n        python weight_diff.py recover --path_raw <path_to_step_1_dir> --path_diff <path_to_step_2_dir>\n\nAdditional notes:\n    - If things run too slowly, and you have an 80G GPU lying around, let GPU go brrr by setting `--device \"cuda\"`.\n    - If you want to save the recovered weights, set `--path_tuned <your_path_tuned>`.\n        Next time you can load the recovered weights directly from `<your_path_tuned>`."
  ],
  [
    "def main(\n    ckpt_dir: str,\n    tokenizer_path: str,\n    temperature: float = 0.6,\n    top_p: float = 0.9,\n    max_seq_len: int = 512,\n    max_batch_size: int = 4,\n    max_gen_len: Optional[int] = None,\n):\n    \"\"\"\n    Examples to run with the models finetuned for chat. Prompts correspond of chat\n    turns between the user and assistant with the final one always being the user.\n\n    An optional system prompt at the beginning to control how the model should respond\n    is also supported.\n\n    The context window of llama3 models is 8192 tokens, so `max_seq_len` needs to be <= 8192.\n\n    `max_gen_len` is optional because finetuned models are able to stop generations naturally.\n    \"\"\"\n    generator = Llama.build(\n        ckpt_dir=ckpt_dir,\n        tokenizer_path=tokenizer_path,\n        max_seq_len=max_seq_len,\n        max_batch_size=max_batch_size,\n    )\n\n    dialogs: List[Dialog] = [\n        [{\"role\": \"user\", \"content\": \"what is the recipe of mayonnaise?\"}],\n        [\n            {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n            {\n                \"role\": \"assistant\",\n                \"content\": \"\"\"\\\nParis, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"\"\",\n            },\n            {\"role\": \"user\", \"content\": \"What is so great about #1?\"},\n        ],\n        [\n            {\"role\": \"system\", \"content\": \"Always answer with Haiku\"},\n            {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"},\n        ],\n        [\n            {\n                \"role\": \"system\",\n                \"content\": \"Always answer with emojis\",\n            },\n            {\"role\": \"user\", \"content\": \"How to go from Beijing to NY?\"},\n        ],\n    ]\n    results = generator.chat_completion(\n        dialogs,\n        max_gen_len=max_gen_len,\n        temperature=temperature,\n        top_p=top_p,\n    )\n\n    for dialog, result in zip(dialogs, results):\n        for msg in dialog:\n            print(f\"{msg['role'].capitalize()}: {msg['content']}\\n\")\n        print(\n            f\"> {result['generation']['role'].capitalize()}: {result['generation']['content']}\"\n        )\n        print(\"\\n==================================\\n\")",
    "Examples to run with the models finetuned for chat. Prompts correspond of chat\nturns between the user and assistant with the final one always being the user.\n\nAn optional system prompt at the beginning to control how the model should respond\nis also supported.\n\nThe context window of llama3 models is 8192 tokens, so `max_seq_len` needs to be <= 8192.\n\n`max_gen_len` is optional because finetuned models are able to stop generations naturally."
  ],
  [
    "def main(\n    ckpt_dir: str,\n    tokenizer_path: str,\n    temperature: float = 0.6,\n    top_p: float = 0.9,\n    max_seq_len: int = 128,\n    max_gen_len: int = 64,\n    max_batch_size: int = 4,\n):\n    \"\"\"\n    Examples to run with the pre-trained models (no fine-tuning). Prompts are\n    usually in the form of an incomplete text prefix that the model can then try to complete.\n\n    The context window of llama3 models is 8192 tokens, so `max_seq_len` needs to be <= 8192.\n    `max_gen_len` is needed because pre-trained models usually do not stop completions naturally.\n    \"\"\"\n    generator = Llama.build(\n        ckpt_dir=ckpt_dir,\n        tokenizer_path=tokenizer_path,\n        max_seq_len=max_seq_len,\n        max_batch_size=max_batch_size,\n    )\n\n    prompts: List[str] = [\n        # For these prompts, the expected answer is the natural continuation of the prompt\n        \"I believe the meaning of life is\",\n        \"Simply put, the theory of relativity states that \",\n        \"\"\"A brief message congratulating the team on the launch:\n\n        Hi everyone,\n\n        I just \"\"\",\n        # Few shot prompt (providing a few examples before asking model to complete more);\n        \"\"\"Translate English to French:\n\n        sea otter => loutre de mer\n        peppermint => menthe poivre\n        plush girafe => girafe peluche\n        cheese =>\"\"\",\n    ]\n    results = generator.text_completion(\n        prompts,\n        max_gen_len=max_gen_len,\n        temperature=temperature,\n        top_p=top_p,\n    )\n    for prompt, result in zip(prompts, results):\n        print(prompt)\n        print(f\"> {result['generation']}\")\n        print(\"\\n==================================\\n\")",
    "Examples to run with the pre-trained models (no fine-tuning). Prompts are\nusually in the form of an incomplete text prefix that the model can then try to complete.\n\nThe context window of llama3 models is 8192 tokens, so `max_seq_len` needs to be <= 8192.\n`max_gen_len` is needed because pre-trained models usually do not stop completions naturally."
  ],
  [
    "def sample_top_p(probs, p):\n    \"\"\"\n    Perform top-p (nucleus) sampling on a probability distribution.\n\n    Args:\n        probs (torch.Tensor): Probability distribution tensor.\n        p (float): Probability threshold for top-p sampling.\n\n    Returns:\n        torch.Tensor: Sampled token indices.\n\n    Note:\n        Top-p sampling selects the smallest set of tokens whose cumulative probability mass\n        exceeds the threshold p. The distribution is renormalized based on the selected tokens.\n    \"\"\"\n    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n    probs_sum = torch.cumsum(probs_sort, dim=-1)\n    mask = probs_sum - probs_sort > p\n    probs_sort[mask] = 0.0\n    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n    next_token = torch.multinomial(probs_sort, num_samples=1)\n    next_token = torch.gather(probs_idx, -1, next_token)\n    return next_token",
    "Perform top-p (nucleus) sampling on a probability distribution.\n\nArgs:\n    probs (torch.Tensor): Probability distribution tensor.\n    p (float): Probability threshold for top-p sampling.\n\nReturns:\n    torch.Tensor: Sampled token indices.\n\nNote:\n    Top-p sampling selects the smallest set of tokens whose cumulative probability mass\n    exceeds the threshold p. The distribution is renormalized based on the selected tokens."
  ],
  [
    "def build(\n        ckpt_dir: str,\n        tokenizer_path: str,\n        max_seq_len: int,\n        max_batch_size: int,\n        model_parallel_size: Optional[int] = None,\n        seed: int = 1,\n    ) -> \"Llama\":\n        \"\"\"\n        Build a Llama instance by initializing and loading a model checkpoint.\n\n        Args:\n            ckpt_dir (str): Path to the directory containing checkpoint files.\n            tokenizer_path (str): Path to the tokenizer file.\n            max_seq_len (int): Maximum sequence length for input text.\n            max_batch_size (int): Maximum batch size for inference.\n            model_parallel_size (Optional[int], optional): Number of model parallel processes.\n                If not provided, it's determined from the environment. Defaults to None.\n\n        Returns:\n            Llama: An instance of the Llama class with the loaded model and tokenizer.\n\n        Raises:\n            AssertionError: If there are no checkpoint files in the specified directory,\n                or if the model parallel size does not match the number of checkpoint files.\n\n        Note:\n            This method initializes the distributed process group, sets the device to CUDA,\n            and loads the pre-trained model and tokenizer.\n        \"\"\"\n        assert 1 <= max_seq_len <= 8192, f\"max_seq_len must be between 1 and 8192, got {max_seq_len}.\"\n        assert os.path.isdir(ckpt_dir), f\"Checkpoint directory '{ckpt_dir}' does not exist.\"\n        assert os.path.isfile(tokenizer_path), f\"Tokenizer file '{tokenizer_path}' does not exist.\"\n        \n        if not torch.distributed.is_initialized():\n            torch.distributed.init_process_group(\"nccl\")\n        if not model_parallel_is_initialized():\n            if model_parallel_size is None:\n                model_parallel_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n            initialize_model_parallel(model_parallel_size)\n\n        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n        torch.cuda.set_device(local_rank)\n\n        # seed must be the same in all processes\n        torch.manual_seed(seed)\n\n        if local_rank > 0:\n            sys.stdout = open(os.devnull, \"w\")\n\n        start_time = time.time()\n        checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n        assert len(checkpoints) > 0, f\"no checkpoint files found in {ckpt_dir}\"\n        assert model_parallel_size == len(\n            checkpoints\n        ), f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}\"\n        ckpt_path = checkpoints[get_model_parallel_rank()]\n        checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n        with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n            params = json.loads(f.read())\n\n        model_args: ModelArgs = ModelArgs(\n            max_seq_len=max_seq_len,\n            max_batch_size=max_batch_size,\n            **params,\n        )\n        tokenizer = Tokenizer(model_path=tokenizer_path)\n        assert model_args.vocab_size == tokenizer.n_words\n        if torch.cuda.is_bf16_supported():\n            torch.set_default_tensor_type(torch.cuda.BFloat16Tensor)\n        else:\n            torch.set_default_tensor_type(torch.cuda.HalfTensor)\n        model = Transformer(model_args)\n        model.load_state_dict(checkpoint, strict=False)\n        print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n\n        return Llama(model, tokenizer)",
    "Build a Llama instance by initializing and loading a model checkpoint.\n\nArgs:\n    ckpt_dir (str): Path to the directory containing checkpoint files.\n    tokenizer_path (str): Path to the tokenizer file.\n    max_seq_len (int): Maximum sequence length for input text.\n    max_batch_size (int): Maximum batch size for inference.\n    model_parallel_size (Optional[int], optional): Number of model parallel processes.\n        If not provided, it's determined from the environment. Defaults to None.\n\nReturns:\n    Llama: An instance of the Llama class with the loaded model and tokenizer.\n\nRaises:\n    AssertionError: If there are no checkpoint files in the specified directory,\n        or if the model parallel size does not match the number of checkpoint files.\n\nNote:\n    This method initializes the distributed process group, sets the device to CUDA,\n    and loads the pre-trained model and tokenizer."
  ],
  [
    "def generate(\n        self,\n        prompt_tokens: List[List[int]],\n        max_gen_len: int,\n        temperature: float = 0.6,\n        top_p: float = 0.9,\n        logprobs: bool = False,\n        echo: bool = False,\n    ) -> Tuple[List[List[int]], Optional[List[List[float]]]]:\n        \"\"\"\n        Generate text sequences based on provided prompts using the language generation model.\n\n        Args:\n            prompt_tokens (List[List[int]]): List of tokenized prompts, where each prompt is represented as a list of integers.\n            max_gen_len (int): Maximum length of the generated text sequence.\n            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n            echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n\n        Returns:\n            Tuple[List[List[int]], Optional[List[List[float]]]]: A tuple containing generated token sequences and, if logprobs is True, corresponding token log probabilities.\n\n        Note:\n            This method uses the provided prompts as a basis for generating text. It employs nucleus sampling to produce text with controlled randomness.\n            If logprobs is True, token log probabilities are computed for each generated token.\n\n        \"\"\"\n        params = self.model.params\n        bsz = len(prompt_tokens)\n        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n\n        min_prompt_len = min(len(t) for t in prompt_tokens)\n        max_prompt_len = max(len(t) for t in prompt_tokens)\n        assert max_prompt_len <= params.max_seq_len\n        total_len = min(params.max_seq_len, max_gen_len + max_prompt_len)\n\n        pad_id = self.tokenizer.pad_id\n        tokens = torch.full((bsz, total_len), pad_id, dtype=torch.long, device=\"cuda\")\n        for k, t in enumerate(prompt_tokens):\n            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=\"cuda\")\n        if logprobs:\n            token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n\n        prev_pos = 0\n        eos_reached = torch.tensor([False] * bsz, device=\"cuda\")\n        input_text_mask = tokens != pad_id\n        if min_prompt_len == total_len:\n            logits = self.model.forward(tokens, prev_pos)\n            token_logprobs = -F.cross_entropy(\n                input=logits.transpose(1, 2),\n                target=tokens,\n                reduction=\"none\",\n                ignore_index=pad_id,\n            )\n\n        stop_tokens = torch.tensor(list(self.tokenizer.stop_tokens))\n\n        for cur_pos in range(min_prompt_len, total_len):\n            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n            if temperature > 0:\n                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n                next_token = sample_top_p(probs, top_p)\n            else:\n                next_token = torch.argmax(logits[:, -1], dim=-1)\n\n            next_token = next_token.reshape(-1)\n            # only replace token if prompt has already been generated\n            next_token = torch.where(\n                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n            )\n            tokens[:, cur_pos] = next_token\n            if logprobs:\n                token_logprobs[:, prev_pos + 1 : cur_pos + 1] = -F.cross_entropy(\n                    input=logits.transpose(1, 2),\n                    target=tokens[:, prev_pos + 1 : cur_pos + 1],\n                    reduction=\"none\",\n                    ignore_index=pad_id,\n                )\n            eos_reached |= (~input_text_mask[:, cur_pos]) & (\n                torch.isin(next_token, stop_tokens)\n            )\n            prev_pos = cur_pos\n            if all(eos_reached):\n                break\n\n        if logprobs:\n            token_logprobs = token_logprobs.tolist()\n        out_tokens, out_logprobs = [], []\n        for i, toks in enumerate(tokens.tolist()):\n            # cut to max gen len\n            start = 0 if echo else len(prompt_tokens[i])\n            toks = toks[start : len(prompt_tokens[i]) + max_gen_len]\n            probs = None\n            if logprobs:\n                probs = token_logprobs[i][start : len(prompt_tokens[i]) + max_gen_len]\n            # cut to after eos tok if any\n            for stop_token in self.tokenizer.stop_tokens:\n                try:\n                    eos_idx = toks.index(stop_token)\n                    toks = toks[:eos_idx]\n                    probs = probs[:eos_idx] if logprobs else None\n                except ValueError:\n                    pass\n            out_tokens.append(toks)\n            out_logprobs.append(probs)\n        return (out_tokens, out_logprobs if logprobs else None)",
    "Generate text sequences based on provided prompts using the language generation model.\n\nArgs:\n    prompt_tokens (List[List[int]]): List of tokenized prompts, where each prompt is represented as a list of integers.\n    max_gen_len (int): Maximum length of the generated text sequence.\n    temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n    top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n    logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n    echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n\nReturns:\n    Tuple[List[List[int]], Optional[List[List[float]]]]: A tuple containing generated token sequences and, if logprobs is True, corresponding token log probabilities.\n\nNote:\n    This method uses the provided prompts as a basis for generating text. It employs nucleus sampling to produce text with controlled randomness.\n    If logprobs is True, token log probabilities are computed for each generated token."
  ],
  [
    "def text_completion(\n        self,\n        prompts: List[str],\n        temperature: float = 0.6,\n        top_p: float = 0.9,\n        max_gen_len: Optional[int] = None,\n        logprobs: bool = False,\n        echo: bool = False,\n    ) -> List[CompletionPrediction]:\n        \"\"\"\n        Perform text completion for a list of prompts using the language generation model.\n\n        Args:\n            prompts (List[str]): List of text prompts for completion.\n            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n            max_gen_len (Optional[int], optional): Maximum length of the generated completion sequence.\n                If not provided, it's set to the model's maximum sequence length minus 1.\n            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n            echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n\n        Returns:\n            List[CompletionPrediction]: List of completion predictions, each containing the generated text completion.\n\n        Note:\n            This method generates text completions for the provided prompts, employing nucleus sampling to introduce controlled randomness.\n            If logprobs is True, token log probabilities are computed for each generated token.\n\n        \"\"\"\n        if max_gen_len is None:\n            max_gen_len = self.model.params.max_seq_len - 1\n        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n        generation_tokens, generation_logprobs = self.generate(\n            prompt_tokens=prompt_tokens,\n            max_gen_len=max_gen_len,\n            temperature=temperature,\n            top_p=top_p,\n            logprobs=logprobs,\n            echo=echo,\n        )\n        if logprobs:\n            return [\n                {\n                    \"generation\": self.tokenizer.decode(t),\n                    \"tokens\": [self.tokenizer.decode([x]) for x in t],\n                    \"logprobs\": logprobs_i,\n                }\n                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n            ]\n        return [{\"generation\": self.tokenizer.decode(t)} for t in generation_tokens]",
    "Perform text completion for a list of prompts using the language generation model.\n\nArgs:\n    prompts (List[str]): List of text prompts for completion.\n    temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n    top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n    max_gen_len (Optional[int], optional): Maximum length of the generated completion sequence.\n        If not provided, it's set to the model's maximum sequence length minus 1.\n    logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n    echo (bool, optional): Flag indicating whether to include prompt tokens in the generated output. Defaults to False.\n\nReturns:\n    List[CompletionPrediction]: List of completion predictions, each containing the generated text completion.\n\nNote:\n    This method generates text completions for the provided prompts, employing nucleus sampling to introduce controlled randomness.\n    If logprobs is True, token log probabilities are computed for each generated token."
  ],
  [
    "def chat_completion(\n        self,\n        dialogs: List[Dialog],\n        temperature: float = 0.6,\n        top_p: float = 0.9,\n        max_gen_len: Optional[int] = None,\n        logprobs: bool = False,\n    ) -> List[ChatPrediction]:\n        \"\"\"\n        Generate assistant responses for a list of conversational dialogs using the language generation model.\n\n        Args:\n            dialogs (List[Dialog]): List of conversational dialogs, where each dialog is a list of messages.\n            temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n            top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n            max_gen_len (Optional[int], optional): Maximum length of the generated response sequence.\n                If not provided, it's set to the model's maximum sequence length minus 1.\n            logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n\n        Returns:\n            List[ChatPrediction]: List of chat predictions, each containing the assistant's generated response.\n\n        Note:\n            This method generates assistant responses for the provided conversational dialogs.\n            It employs nucleus sampling to introduce controlled randomness in text generation.\n            If logprobs is True, token log probabilities are computed for each generated token.\n        \"\"\"\n        if max_gen_len is None:\n            max_gen_len = self.model.params.max_seq_len - 1\n\n        prompt_tokens = [\n            self.formatter.encode_dialog_prompt(dialog) for dialog in dialogs\n        ]\n        generation_tokens, generation_logprobs = self.generate(\n            prompt_tokens=prompt_tokens,\n            max_gen_len=max_gen_len,\n            temperature=temperature,\n            top_p=top_p,\n            logprobs=logprobs,\n        )\n        if logprobs:\n            return [\n                {\n                    \"generation\": {\n                        \"role\": \"assistant\",\n                        \"content\": self.tokenizer.decode(t),\n                    },\n                    \"tokens\": [self.tokenizer.decode([x]) for x in t],\n                    \"logprobs\": logprobs_i,\n                }\n                for t, logprobs_i in zip(generation_tokens, generation_logprobs)\n            ]\n        return [\n            {\n                \"generation\": {\n                    \"role\": \"assistant\",\n                    \"content\": self.tokenizer.decode(t),\n                },\n            }\n            for t in generation_tokens\n        ]",
    "Generate assistant responses for a list of conversational dialogs using the language generation model.\n\nArgs:\n    dialogs (List[Dialog]): List of conversational dialogs, where each dialog is a list of messages.\n    temperature (float, optional): Temperature value for controlling randomness in sampling. Defaults to 0.6.\n    top_p (float, optional): Top-p probability threshold for nucleus sampling. Defaults to 0.9.\n    max_gen_len (Optional[int], optional): Maximum length of the generated response sequence.\n        If not provided, it's set to the model's maximum sequence length minus 1.\n    logprobs (bool, optional): Flag indicating whether to compute token log probabilities. Defaults to False.\n\nReturns:\n    List[ChatPrediction]: List of chat predictions, each containing the assistant's generated response.\n\nNote:\n    This method generates assistant responses for the provided conversational dialogs.\n    It employs nucleus sampling to introduce controlled randomness in text generation.\n    If logprobs is True, token log probabilities are computed for each generated token."
  ],
  [
    "class Tokenizer:\n    \"\"\"\n    Tokenizing and encoding/decoding text using the Tiktoken tokenizer.\n    \"\"\"\n\n    special_tokens: Dict[str, int]\n\n    num_reserved_special_tokens = 256\n\n    pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n\n    def __init__(self, model_path: str):\n        \"\"\"\n        Initializes the Tokenizer with a Tiktoken model.\n\n        Args:\n            model_path (str): The path to the Tiktoken model file.\n        \"\"\"\n        assert os.path.isfile(model_path), model_path\n\n        mergeable_ranks = load_tiktoken_bpe(model_path)\n        num_base_tokens = len(mergeable_ranks)\n        special_tokens = [\n            \"<|begin_of_text|>\",\n            \"<|end_of_text|>\",\n            \"<|reserved_special_token_0|>\",\n            \"<|reserved_special_token_1|>\",\n            \"<|reserved_special_token_2|>\",\n            \"<|reserved_special_token_3|>\",\n            \"<|start_header_id|>\",\n            \"<|end_header_id|>\",\n            \"<|reserved_special_token_4|>\",\n            \"<|eot_id|>\",  # end of turn\n        ] + [\n            f\"<|reserved_special_token_{i}|>\"\n            for i in range(5, self.num_reserved_special_tokens - 5)\n        ]\n        self.special_tokens = {\n            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n        }\n        self.model = tiktoken.Encoding(\n            name=Path(model_path).name,\n            pat_str=self.pat_str,\n            mergeable_ranks=mergeable_ranks,\n            special_tokens=self.special_tokens,\n        )\n        logger.info(f\"Reloaded tiktoken model from {model_path}\")\n\n        self.n_words: int = self.model.n_vocab\n        # BOS / EOS token IDs\n        self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n        self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n        self.pad_id: int = -1\n        self.stop_tokens = {\n            self.special_tokens[\"<|end_of_text|>\"],\n            self.special_tokens[\"<|eot_id|>\"],\n        }\n        logger.info(\n            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n        )\n\n    def encode(\n        self,\n        s: str,\n        *,\n        bos: bool,\n        eos: bool,\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n    ) -> List[int]:\n        \"\"\"\n        Encodes a string into a list of token IDs.\n\n        Args:\n            s (str): The input string to be encoded.\n            bos (bool): Whether to prepend the beginning-of-sequence token.\n            eos (bool): Whether to append the end-of-sequence token.\n            allowed_tokens (\"all\"|set[str]): allowed special tokens in string\n            disallowed_tokens (\"all\"|set[str]): special tokens that raise an error when in string\n\n        Returns:\n            list[int]: A list of token IDs.\n\n        By default, setting disallowed_special=() encodes a string by ignoring\n        special tokens. Specifically:\n        - Setting `disallowed_special` to () will cause all text corresponding\n          to special tokens to be encoded as natural text (insteading of raising\n          an error).\n        - Setting `allowed_special` to \"all\" will treat all text corresponding\n          to special tokens to be encoded as special tokens.\n        \"\"\"\n        assert type(s) is str\n\n        # The tiktoken tokenizer can handle <=400k chars without\n        # pyo3_runtime.PanicException.\n        TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n\n        # https://github.com/openai/tiktoken/issues/195\n        # Here we iterate over subsequences and split if we exceed the limit\n        # of max consecutive non-whitespace or whitespace characters.\n        MAX_NO_WHITESPACES_CHARS = 25_000\n\n        substrs = (\n            substr\n            for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n            for substr in self._split_whitespaces_or_nonwhitespaces(\n                s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n            )\n        )\n        t: List[int] = []\n        for substr in substrs:\n            t.extend(\n                self.model.encode(\n                    substr,\n                    allowed_special=allowed_special,\n                    disallowed_special=disallowed_special,\n                )\n            )\n        if bos:\n            t.insert(0, self.bos_id)\n        if eos:\n            t.append(self.eos_id)\n        return t\n\n    def decode(self, t: Sequence[int]) -> str:\n        \"\"\"\n        Decodes a list of token IDs into a string.\n\n        Args:\n            t (List[int]): The list of token IDs to be decoded.\n\n        Returns:\n            str: The decoded string.\n        \"\"\"\n        # Typecast is safe here. Tiktoken doesn't do anything list-related with the sequence.\n        return self.model.decode(cast(List[int], t))\n\n    @staticmethod\n    def _split_whitespaces_or_nonwhitespaces(\n        s: str, max_consecutive_slice_len: int\n    ) -> Iterator[str]:\n        \"\"\"\n        Splits the string `s` so that each substring contains no more than `max_consecutive_slice_len`\n        consecutive whitespaces or consecutive non-whitespaces.\n        \"\"\"\n        current_slice_len = 0\n        current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n        slice_start = 0\n\n        for i in range(len(s)):\n            is_now_space = s[i].isspace()\n\n            if current_slice_is_space ^ is_now_space:\n                current_slice_len = 1\n                current_slice_is_space = is_now_space\n            else:\n                current_slice_len += 1\n                if current_slice_len > max_consecutive_slice_len:\n                    yield s[slice_start:i]\n                    slice_start = i\n                    current_slice_len = 1\n        yield s[slice_start:]",
    "Tokenizing and encoding/decoding text using the Tiktoken tokenizer."
  ],
  [
    "def __init__(self, model_path: str):\n        \"\"\"\n        Initializes the Tokenizer with a Tiktoken model.\n\n        Args:\n            model_path (str): The path to the Tiktoken model file.\n        \"\"\"\n        assert os.path.isfile(model_path), model_path\n\n        mergeable_ranks = load_tiktoken_bpe(model_path)\n        num_base_tokens = len(mergeable_ranks)\n        special_tokens = [\n            \"<|begin_of_text|>\",\n            \"<|end_of_text|>\",\n            \"<|reserved_special_token_0|>\",\n            \"<|reserved_special_token_1|>\",\n            \"<|reserved_special_token_2|>\",\n            \"<|reserved_special_token_3|>\",\n            \"<|start_header_id|>\",\n            \"<|end_header_id|>\",\n            \"<|reserved_special_token_4|>\",\n            \"<|eot_id|>\",  # end of turn\n        ] + [\n            f\"<|reserved_special_token_{i}|>\"\n            for i in range(5, self.num_reserved_special_tokens - 5)\n        ]\n        self.special_tokens = {\n            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n        }\n        self.model = tiktoken.Encoding(\n            name=Path(model_path).name,\n            pat_str=self.pat_str,\n            mergeable_ranks=mergeable_ranks,\n            special_tokens=self.special_tokens,\n        )\n        logger.info(f\"Reloaded tiktoken model from {model_path}\")\n\n        self.n_words: int = self.model.n_vocab\n        # BOS / EOS token IDs\n        self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n        self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n        self.pad_id: int = -1\n        self.stop_tokens = {\n            self.special_tokens[\"<|end_of_text|>\"],\n            self.special_tokens[\"<|eot_id|>\"],\n        }\n        logger.info(\n            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n        )",
    "Initializes the Tokenizer with a Tiktoken model.\n\nArgs:\n    model_path (str): The path to the Tiktoken model file."
  ],
  [
    "def encode(\n        self,\n        s: str,\n        *,\n        bos: bool,\n        eos: bool,\n        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\n        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n    ) -> List[int]:\n        \"\"\"\n        Encodes a string into a list of token IDs.\n\n        Args:\n            s (str): The input string to be encoded.\n            bos (bool): Whether to prepend the beginning-of-sequence token.\n            eos (bool): Whether to append the end-of-sequence token.\n            allowed_tokens (\"all\"|set[str]): allowed special tokens in string\n            disallowed_tokens (\"all\"|set[str]): special tokens that raise an error when in string\n\n        Returns:\n            list[int]: A list of token IDs.\n\n        By default, setting disallowed_special=() encodes a string by ignoring\n        special tokens. Specifically:\n        - Setting `disallowed_special` to () will cause all text corresponding\n          to special tokens to be encoded as natural text (insteading of raising\n          an error).\n        - Setting `allowed_special` to \"all\" will treat all text corresponding\n          to special tokens to be encoded as special tokens.\n        \"\"\"\n        assert type(s) is str\n\n        # The tiktoken tokenizer can handle <=400k chars without\n        # pyo3_runtime.PanicException.\n        TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n\n        # https://github.com/openai/tiktoken/issues/195\n        # Here we iterate over subsequences and split if we exceed the limit\n        # of max consecutive non-whitespace or whitespace characters.\n        MAX_NO_WHITESPACES_CHARS = 25_000\n\n        substrs = (\n            substr\n            for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n            for substr in self._split_whitespaces_or_nonwhitespaces(\n                s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n            )\n        )\n        t: List[int] = []\n        for substr in substrs:\n            t.extend(\n                self.model.encode(\n                    substr,\n                    allowed_special=allowed_special,\n                    disallowed_special=disallowed_special,\n                )\n            )\n        if bos:\n            t.insert(0, self.bos_id)\n        if eos:\n            t.append(self.eos_id)\n        return t",
    "Encodes a string into a list of token IDs.\n\nArgs:\n    s (str): The input string to be encoded.\n    bos (bool): Whether to prepend the beginning-of-sequence token.\n    eos (bool): Whether to append the end-of-sequence token.\n    allowed_tokens (\"all\"|set[str]): allowed special tokens in string\n    disallowed_tokens (\"all\"|set[str]): special tokens that raise an error when in string\n\nReturns:\n    list[int]: A list of token IDs.\n\nBy default, setting disallowed_special=() encodes a string by ignoring\nspecial tokens. Specifically:\n- Setting `disallowed_special` to () will cause all text corresponding\n  to special tokens to be encoded as natural text (insteading of raising\n  an error).\n- Setting `allowed_special` to \"all\" will treat all text corresponding\n  to special tokens to be encoded as special tokens."
  ],
  [
    "def decode(self, t: Sequence[int]) -> str:\n        \"\"\"\n        Decodes a list of token IDs into a string.\n\n        Args:\n            t (List[int]): The list of token IDs to be decoded.\n\n        Returns:\n            str: The decoded string.\n        \"\"\"\n        # Typecast is safe here. Tiktoken doesn't do anything list-related with the sequence.\n        return self.model.decode(cast(List[int], t))",
    "Decodes a list of token IDs into a string.\n\nArgs:\n    t (List[int]): The list of token IDs to be decoded.\n\nReturns:\n    str: The decoded string."
  ],
  [
    "def _split_whitespaces_or_nonwhitespaces(\n        s: str, max_consecutive_slice_len: int\n    ) -> Iterator[str]:\n        \"\"\"\n        Splits the string `s` so that each substring contains no more than `max_consecutive_slice_len`\n        consecutive whitespaces or consecutive non-whitespaces.\n        \"\"\"\n        current_slice_len = 0\n        current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n        slice_start = 0\n\n        for i in range(len(s)):\n            is_now_space = s[i].isspace()\n\n            if current_slice_is_space ^ is_now_space:\n                current_slice_len = 1\n                current_slice_is_space = is_now_space\n            else:\n                current_slice_len += 1\n                if current_slice_len > max_consecutive_slice_len:\n                    yield s[slice_start:i]\n                    slice_start = i\n                    current_slice_len = 1\n        yield s[slice_start:]",
    "Splits the string `s` so that each substring contains no more than `max_consecutive_slice_len`\nconsecutive whitespaces or consecutive non-whitespaces."
  ],
  [
    "class ConfigValidator:\n    \n\n    EMAIL_REGEX = re.compile(r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\")\n    REQUIRED_CONFIG_KEYS = {\n        \"remote\": bool,\n        \"experience_level\": dict,\n        \"job_types\": dict,\n        \"date\": dict,\n        \"positions\": list,\n        \"locations\": list,\n        \"location_blacklist\": list,\n        \"distance\": int,\n        \"company_blacklist\": list,\n        \"title_blacklist\": list,\n    }\n    EXPERIENCE_LEVELS = [\n        \"internship\",\n        \"entry\",\n        \"associate\",\n        \"mid_senior_level\",\n        \"director\",\n        \"executive\",\n    ]\n    JOB_TYPES = [\n        \"full_time\",\n        \"contract\",\n        \"part_time\",\n        \"temporary\",\n        \"internship\",\n        \"other\",\n        \"volunteer\",\n    ]\n    DATE_FILTERS = [\"all_time\", \"month\", \"week\", \"24_hours\"]\n    APPROVED_DISTANCES = {0, 5, 10, 25, 50, 100}\n\n    @staticmethod\n    def validate_email(email: str) -> bool:\n        \"\"\"Validate the format of an email address.\"\"\"\n        return bool(ConfigValidator.EMAIL_REGEX.match(email))\n\n    @staticmethod\n    def load_yaml(yaml_path: Path) -> dict:\n        \"\"\"Load and parse a YAML file.\"\"\"\n        try:\n            with open(yaml_path, \"r\") as stream:\n                return yaml.safe_load(stream)\n        except yaml.YAMLError as exc:\n            raise ConfigError(f\"Error reading YAML file {yaml_path}: {exc}\")\n        except FileNotFoundError:\n            raise ConfigError(f\"YAML file not found: {yaml_path}\")\n\n    @classmethod\n    def validate_config(cls, config_yaml_path: Path) -> dict:\n        \"\"\"Validate the main configuration YAML file.\"\"\"\n        parameters = cls.load_yaml(config_yaml_path)\n        # Check for required keys and their types\n        for key, expected_type in cls.REQUIRED_CONFIG_KEYS.items():\n            if key not in parameters:\n                if key in [\"company_blacklist\", \"title_blacklist\", \"location_blacklist\"]:\n                    parameters[key] = []\n                else:\n                    raise ConfigError(f\"Missing required key '{key}' in {config_yaml_path}\")\n            elif not isinstance(parameters[key], expected_type):\n                if key in [\"company_blacklist\", \"title_blacklist\", \"location_blacklist\"] and parameters[key] is None:\n                    parameters[key] = []\n                else:\n                    raise ConfigError(\n                        f\"Invalid type for key '{key}' in {config_yaml_path}. Expected {expected_type.__name__}.\"\n                    )\n        cls._validate_experience_levels(parameters[\"experience_level\"], config_yaml_path)\n        cls._validate_job_types(parameters[\"job_types\"], config_yaml_path)\n        cls._validate_date_filters(parameters[\"date\"], config_yaml_path)\n        cls._validate_list_of_strings(parameters, [\"positions\", \"locations\"], config_yaml_path)\n        cls._validate_distance(parameters[\"distance\"], config_yaml_path)\n        cls._validate_blacklists(parameters, config_yaml_path)\n        return parameters\n\n    @classmethod\n    def _validate_experience_levels(cls, experience_levels: dict, config_path: Path):\n        \"\"\"Ensure experience levels are booleans.\"\"\"\n        for level in cls.EXPERIENCE_LEVELS:\n            if not isinstance(experience_levels.get(level), bool):\n                raise ConfigError(\n                    f\"Experience level '{level}' must be a boolean in {config_path}\"\n                )\n\n    @classmethod\n    def _validate_job_types(cls, job_types: dict, config_path: Path):\n        \"\"\"Ensure job types are booleans.\"\"\"\n        for job_type in cls.JOB_TYPES:\n            if not isinstance(job_types.get(job_type), bool):\n                raise ConfigError(\n                    f\"Job type '{job_type}' must be a boolean in {config_path}\"\n                )\n\n    @classmethod\n    def _validate_date_filters(cls, date_filters: dict, config_path: Path):\n        \"\"\"Ensure date filters are booleans.\"\"\"\n        for date_filter in cls.DATE_FILTERS:\n            if not isinstance(date_filters.get(date_filter), bool):\n                raise ConfigError(\n                    f\"Date filter '{date_filter}' must be a boolean in {config_path}\"\n                )\n\n    @classmethod\n    def _validate_list_of_strings(cls, parameters: dict, keys: list, config_path: Path):\n        \"\"\"Ensure specified keys are lists of strings.\"\"\"\n        for key in keys:\n            if not all(isinstance(item, str) for item in parameters[key]):\n                raise ConfigError(\n                    f\"'{key}' must be a list of strings in {config_path}\"\n                )\n\n    @classmethod\n    def _validate_distance(cls, distance: int, config_path: Path):\n        \"\"\"Validate the distance value.\"\"\"\n        if distance not in cls.APPROVED_DISTANCES:\n            raise ConfigError(\n                f\"Invalid distance value '{distance}' in {config_path}. Must be one of: {cls.APPROVED_DISTANCES}\"\n            )\n\n    @classmethod\n    def _validate_blacklists(cls, parameters: dict, config_path: Path):\n        \"\"\"Ensure blacklists are lists.\"\"\"\n        for blacklist in [\"company_blacklist\", \"title_blacklist\", \"location_blacklist\"]:\n            if not isinstance(parameters.get(blacklist), list):\n                raise ConfigError(\n                    f\"'{blacklist}' must be a list in {config_path}\"\n                )\n            if parameters[blacklist] is None:\n                parameters[blacklist] = []\n\n    @staticmethod\n    def validate_secrets(secrets_yaml_path: Path) -> str:\n        \"\"\"Validate the secrets YAML file and retrieve the LLM API key.\"\"\"\n        secrets = ConfigValidator.load_yaml(secrets_yaml_path)\n        mandatory_secrets = [\"llm_api_key\"]\n\n        for secret in mandatory_secrets:\n            if secret not in secrets:\n                raise ConfigError(f\"Missing secret '{secret}' in {secrets_yaml_path}\")\n\n            if not secrets[secret]:\n                raise ConfigError(f\"Secret '{secret}' cannot be empty in {secrets_yaml_path}\")\n\n        return secrets[\"llm_api_key\"]",
    "Validates configuration and secrets YAML files."
  ],
  [
    "class FileManager:\n    \n\n    REQUIRED_FILES = [SECRETS_YAML, WORK_PREFERENCES_YAML, PLAIN_TEXT_RESUME_YAML]\n\n    @staticmethod\n    def validate_data_folder(app_data_folder: Path) -> Tuple[Path, Path, Path, Path]:\n        \"\"\"Validate the existence of the data folder and required files.\"\"\"\n        if not app_data_folder.is_dir():\n            raise FileNotFoundError(f\"Data folder not found: {app_data_folder}\")\n\n        missing_files = [file for file in FileManager.REQUIRED_FILES if not (app_data_folder / file).exists()]\n        if missing_files:\n            raise FileNotFoundError(f\"Missing files in data folder: {', '.join(missing_files)}\")\n\n        output_folder = app_data_folder / \"output\"\n        output_folder.mkdir(exist_ok=True)\n\n        return (\n            app_data_folder / SECRETS_YAML,\n            app_data_folder / WORK_PREFERENCES_YAML,\n            app_data_folder / PLAIN_TEXT_RESUME_YAML,\n            output_folder,\n        )\n\n    @staticmethod\n    def get_uploads(plain_text_resume_file: Path) -> Dict[str, Path]:\n        \"\"\"Convert resume file paths to a dictionary.\"\"\"\n        if not plain_text_resume_file.exists():\n            raise FileNotFoundError(f\"Plain text resume file not found: {plain_text_resume_file}\")\n\n        uploads = {\"plainTextResume\": plain_text_resume_file}\n\n        return uploads",
    "Handles file system operations and validations."
  ],
  [
    "def handle_inquiries(selected_actions: List[str], parameters: dict, llm_api_key: str):\n    \"\"\"\n    Decide which function to call based on the selected user actions.\n\n    :param selected_actions: List of actions selected by the user.\n    :param parameters: Configuration parameters dictionary.\n    :param llm_api_key: API key for the language model.\n    \"\"\"\n    try:\n        if selected_actions:\n            if \"Generate Resume\" == selected_actions:\n                logger.info(\"Crafting a standout professional resume...\")\n                create_resume_pdf(parameters, llm_api_key)\n                \n            if \"Generate Resume Tailored for Job Description\" == selected_actions:\n                logger.info(\"Customizing your resume to enhance your job application...\")\n                create_resume_pdf_job_tailored(parameters, llm_api_key)\n                \n            if \"Generate Tailored Cover Letter for Job Description\" == selected_actions:\n                logger.info(\"Designing a personalized cover letter to enhance your job application...\")\n                create_cover_letter(parameters, llm_api_key)\n\n        else:\n            logger.warning(\"No actions selected. Nothing to execute.\")\n    except Exception as e:\n        logger.exception(f\"An error occurred while handling inquiries: {e}\")\n        raise",
    "Decide which function to call based on the selected user actions.\n\n:param selected_actions: List of actions selected by the user.\n:param parameters: Configuration parameters dictionary.\n:param llm_api_key: API key for the language model."
  ],
  [
    "def prompt_user_action() -> str:\n    \"\"\"\n    Use inquirer to ask the user which action they want to perform.\n\n    :return: Selected action.\n    \"\"\"\n    try:\n        questions = [\n            inquirer.List(\n                'action',\n                message=\"Select the action you want to perform:\",\n                choices=[\n                    \"Generate Resume\",\n                    \"Generate Resume Tailored for Job Description\",\n                    \"Generate Tailored Cover Letter for Job Description\",\n                ],\n            ),\n        ]\n        answer = inquirer.prompt(questions)\n        if answer is None:\n            print(\"No answer provided. The user may have interrupted.\")\n            return \"\"\n        return answer.get('action', \"\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return \"\"",
    "Use inquirer to ask the user which action they want to perform.\n\n:return: Selected action."
  ],
  [
    "def main():\n    \n    try:\n        # Define and validate the data folder\n        data_folder = Path(\"data_folder\")\n        secrets_file, config_file, plain_text_resume_file, output_folder = FileManager.validate_data_folder(data_folder)\n\n        # Validate configuration and secrets\n        config = ConfigValidator.validate_config(config_file)\n        llm_api_key = ConfigValidator.validate_secrets(secrets_file)\n\n        # Prepare parameters\n        config[\"uploads\"] = FileManager.get_uploads(plain_text_resume_file)\n        config[\"outputFileDirectory\"] = output_folder\n\n        # Interactive prompt for user to select actions\n        selected_actions = prompt_user_action()\n\n        # Handle selected actions and execute them\n        handle_inquiries(selected_actions, config, llm_api_key)\n\n    except ConfigError as ce:\n        logger.error(f\"Configuration error: {ce}\")\n        logger.error(\n            \"Refer to the configuration guide for troubleshooting: \"\n            \"https://github.com/feder-cr/Auto_Jobs_Applier_AIHawk?tab=readme-ov-file#configuration\"\n        )\n    except FileNotFoundError as fnf:\n        logger.error(f\"File not found: {fnf}\")\n        logger.error(\"Ensure all required files are present in the data folder.\")\n    except RuntimeError as re:\n        logger.error(f\"Runtime error: {re}\")\n        logger.debug(traceback.format_exc())\n    except Exception as e:\n        logger.exception(f\"An unexpected error occurred: {e}\")",
    "Main entry point for the AIHawk Job Application Bot."
  ],
  [
    "def validate_email(email: str) -> bool:\n        \n        return bool(ConfigValidator.EMAIL_REGEX.match(email))",
    "Validate the format of an email address."
  ],
  [
    "def load_yaml(yaml_path: Path) -> dict:\n        \n        try:\n            with open(yaml_path, \"r\") as stream:\n                return yaml.safe_load(stream)\n        except yaml.YAMLError as exc:\n            raise ConfigError(f\"Error reading YAML file {yaml_path}: {exc}\")\n        except FileNotFoundError:\n            raise ConfigError(f\"YAML file not found: {yaml_path}\")",
    "Load and parse a YAML file."
  ],
  [
    "def validate_config(cls, config_yaml_path: Path) -> dict:\n        \n        parameters = cls.load_yaml(config_yaml_path)\n        # Check for required keys and their types\n        for key, expected_type in cls.REQUIRED_CONFIG_KEYS.items():\n            if key not in parameters:\n                if key in [\"company_blacklist\", \"title_blacklist\", \"location_blacklist\"]:\n                    parameters[key] = []\n                else:\n                    raise ConfigError(f\"Missing required key '{key}' in {config_yaml_path}\")\n            elif not isinstance(parameters[key], expected_type):\n                if key in [\"company_blacklist\", \"title_blacklist\", \"location_blacklist\"] and parameters[key] is None:\n                    parameters[key] = []\n                else:\n                    raise ConfigError(\n                        f\"Invalid type for key '{key}' in {config_yaml_path}. Expected {expected_type.__name__}.\"\n                    )\n        cls._validate_experience_levels(parameters[\"experience_level\"], config_yaml_path)\n        cls._validate_job_types(parameters[\"job_types\"], config_yaml_path)\n        cls._validate_date_filters(parameters[\"date\"], config_yaml_path)\n        cls._validate_list_of_strings(parameters, [\"positions\", \"locations\"], config_yaml_path)\n        cls._validate_distance(parameters[\"distance\"], config_yaml_path)\n        cls._validate_blacklists(parameters, config_yaml_path)\n        return parameters",
    "Validate the main configuration YAML file."
  ],
  [
    "def _validate_list_of_strings(cls, parameters: dict, keys: list, config_path: Path):\n        \n        for key in keys:\n            if not all(isinstance(item, str) for item in parameters[key]):\n                raise ConfigError(\n                    f\"'{key}' must be a list of strings in {config_path}\"\n                )",
    "Ensure specified keys are lists of strings."
  ],
  [
    "def validate_secrets(secrets_yaml_path: Path) -> str:\n        \n        secrets = ConfigValidator.load_yaml(secrets_yaml_path)\n        mandatory_secrets = [\"llm_api_key\"]\n\n        for secret in mandatory_secrets:\n            if secret not in secrets:\n                raise ConfigError(f\"Missing secret '{secret}' in {secrets_yaml_path}\")\n\n            if not secrets[secret]:\n                raise ConfigError(f\"Secret '{secret}' cannot be empty in {secrets_yaml_path}\")\n\n        return secrets[\"llm_api_key\"]",
    "Validate the secrets YAML file and retrieve the LLM API key."
  ],
  [
    "def validate_data_folder(app_data_folder: Path) -> Tuple[Path, Path, Path, Path]:\n        \n        if not app_data_folder.is_dir():\n            raise FileNotFoundError(f\"Data folder not found: {app_data_folder}\")\n\n        missing_files = [file for file in FileManager.REQUIRED_FILES if not (app_data_folder / file).exists()]\n        if missing_files:\n            raise FileNotFoundError(f\"Missing files in data folder: {', '.join(missing_files)}\")\n\n        output_folder = app_data_folder / \"output\"\n        output_folder.mkdir(exist_ok=True)\n\n        return (\n            app_data_folder / SECRETS_YAML,\n            app_data_folder / WORK_PREFERENCES_YAML,\n            app_data_folder / PLAIN_TEXT_RESUME_YAML,\n            output_folder,\n        )",
    "Validate the existence of the data folder and required files."
  ],
  [
    "def get_uploads(plain_text_resume_file: Path) -> Dict[str, Path]:\n        \n        if not plain_text_resume_file.exists():\n            raise FileNotFoundError(f\"Plain text resume file not found: {plain_text_resume_file}\")\n\n        uploads = {\"plainTextResume\": plain_text_resume_file}\n\n        return uploads",
    "Convert resume file paths to a dictionary."
  ],
  [
    "def formatted_job_information(self):\n        \"\"\"\n        Formats the job information as a markdown string.\n        \"\"\"\n        logger.debug(f\"Formatting job information for job: {self.role} at {self.company}\")\n        job_information = f\"\"\"\n        # Job Description\n        ## Job Information \n        - Position: {self.role}\n        - At: {self.company}\n        - Location: {self.location}\n        - Recruiter Profile: {self.recruiter_link or 'Not available'}\n        \n        ## Description\n        {self.description or 'No description provided.'}\n        \"\"\"\n        formatted_information = job_information.strip()\n        logger.debug(f\"Formatted job information: {formatted_information}\")\n        return formatted_information",
    "Formats the job information as a markdown string."
  ],
  [
    "def _preprocess_template_string(template: str) -> str:\n        \"\"\"\n        Preprocess the template string by removing leading whitespace and indentation.\n        Args:\n            template (str): The template string to preprocess.\n        Returns:\n            str: The preprocessed template string.\n        \"\"\"\n        return textwrap.dedent(template)",
    "Preprocess the template string by removing leading whitespace and indentation.\nArgs:\n    template (str): The template string to preprocess.\nReturns:\n    str: The preprocessed template string."
  ],
  [
    "def set_resume(self, resume) -> None:\n        \"\"\"\n        Set the resume text to be used for generating the cover letter.\n        Args:\n            resume (str): The plain text resume to be used.\n        \"\"\"\n        self.resume = resume",
    "Set the resume text to be used for generating the cover letter.\nArgs:\n    resume (str): The plain text resume to be used."
  ],
  [
    "def set_job_description_from_text(self, job_description_text) -> None:\n        \"\"\"\n        Set the job description text to be used for generating the cover letter.\n        Args:\n            job_description_text (str): The plain text job description to be used.\n        \"\"\"\n        logger.debug(\"Starting job description summarization...\")\n        prompt = ChatPromptTemplate.from_template(self.strings.summarize_prompt_template)\n        chain = prompt | self.llm_cheap | StrOutputParser()\n        output = chain.invoke({\"text\": job_description_text})\n        self.job_description = output\n        logger.debug(f\"Job description summarization complete: {self.job_description}\")",
    "Set the job description text to be used for generating the cover letter.\nArgs:\n    job_description_text (str): The plain text job description to be used."
  ],
  [
    "def generate_cover_letter(self) -> str:\n        \"\"\"\n        Generate the cover letter based on the job description and resume.\n        Returns:\n            str: The generated cover letter\n        \"\"\"\n        logger.debug(\"Starting cover letter generation...\")\n        prompt_template = self._preprocess_template_string(self.strings.cover_letter_template)\n        logger.debug(f\"Cover letter template after preprocessing: {prompt_template}\")\n\n        prompt = ChatPromptTemplate.from_template(prompt_template)\n        logger.debug(f\"Prompt created: {prompt}\")\n\n        chain = prompt | self.llm_cheap | StrOutputParser()\n        logger.debug(f\"Chain created: {chain}\")\n\n        input_data = {\n            \"job_description\": self.job_description,\n            \"resume\": self.resume\n        }\n        logger.debug(f\"Input data: {input_data}\")\n\n        output = chain.invoke(input_data)\n        logger.debug(f\"Cover letter generation result: {output}\")\n\n        logger.debug(\"Cover letter generation completed\")\n        return output",
    "Generate the cover letter based on the job description and resume.\nReturns:\n    str: The generated cover letter"
  ],
  [
    "def _preprocess_template_string(template: str) -> str:\n        \"\"\"\n        Preprocess the template string by removing leading whitespace and indentation.\n        Args:\n            template (str): The template string to preprocess.\n        Returns:\n            str: The preprocessed template string.\n        \"\"\"\n        return textwrap.dedent(template)",
    "Preprocess the template string by removing leading whitespace and indentation.\nArgs:\n    template (str): The template string to preprocess.\nReturns:\n    str: The preprocessed template string."
  ],
  [
    "def set_resume(self, resume) -> None:\n        \"\"\"\n        Set the resume object to be used for generating the resume.\n        Args:\n            resume (Resume): The resume object to be used.\n        \"\"\"\n        self.resume = resume",
    "Set the resume object to be used for generating the resume.\nArgs:\n    resume (Resume): The resume object to be used."
  ],
  [
    "def generate_header(self, data = None) -> str:\n        \"\"\"\n        Generate the header section of the resume.\n        Args:\n            data (dict): The personal information to use for generating the header.\n        Returns:\n            str: The generated header section.\n        \"\"\"\n        header_prompt_template = self._preprocess_template_string(\n            self.strings.prompt_header\n        )\n        prompt = ChatPromptTemplate.from_template(header_prompt_template)\n        chain = prompt | self.llm_cheap | StrOutputParser()\n        input_data = {\n            \"personal_information\": self.resume.personal_information\n        } if data is None else data\n        output = chain.invoke(input_data)\n        return output",
    "Generate the header section of the resume.\nArgs:\n    data (dict): The personal information to use for generating the header.\nReturns:\n    str: The generated header section."
  ],
  [
    "def generate_education_section(self, data = None) -> str:\n        \"\"\"\n        Generate the education section of the resume.\n        Args:\n            data (dict): The education details to use for generating the education section.\n        Returns:\n            str: The generated education section.\n        \"\"\"\n        logger.debug(\"Starting education section generation\")\n\n        education_prompt_template = self._preprocess_template_string(self.strings.prompt_education)\n        logger.debug(f\"Education template: {education_prompt_template}\")\n\n        prompt = ChatPromptTemplate.from_template(education_prompt_template)\n        logger.debug(f\"Prompt: {prompt}\")\n        \n        chain = prompt | self.llm_cheap | StrOutputParser()\n        logger.debug(f\"Chain created: {chain}\")\n        \n        input_data = {\n            \"education_details\": self.resume.education_details\n        } if data is None else data\n        output = chain.invoke(input_data)\n        logger.debug(f\"Chain invocation result: {output}\")\n\n        logger.debug(\"Education section generation completed\")\n        return output",
    "Generate the education section of the resume.\nArgs:\n    data (dict): The education details to use for generating the education section.\nReturns:\n    str: The generated education section."
  ],
  [
    "def generate_work_experience_section(self, data = None) -> str:\n        \"\"\"\n        Generate the work experience section of the resume.\n        Args:\n            data (dict): The work experience details to use for generating the work experience section.\n        Returns:\n            str: The generated work experience section.\n        \"\"\"\n        logger.debug(\"Starting work experience section generation\")\n\n        work_experience_prompt_template = self._preprocess_template_string(self.strings.prompt_working_experience)\n        logger.debug(f\"Work experience template: {work_experience_prompt_template}\")\n\n        prompt = ChatPromptTemplate.from_template(work_experience_prompt_template)\n        logger.debug(f\"Prompt: {prompt}\")\n        \n        chain = prompt | self.llm_cheap | StrOutputParser()\n        logger.debug(f\"Chain created: {chain}\")\n        \n        input_data = {\n            \"experience_details\": self.resume.experience_details\n        } if data is None else data\n        output = chain.invoke(input_data)\n        logger.debug(f\"Chain invocation result: {output}\")\n\n        logger.debug(\"Work experience section generation completed\")\n        return output",
    "Generate the work experience section of the resume.\nArgs:\n    data (dict): The work experience details to use for generating the work experience section.\nReturns:\n    str: The generated work experience section."
  ],
  [
    "def generate_projects_section(self, data = None) -> str:\n        \"\"\"\n        Generate the side projects section of the resume.\n        Args:\n            data (dict): The side projects to use for generating the side projects section.\n        Returns:\n            str: The generated side projects section.\n        \"\"\"\n        logger.debug(\"Starting side projects section generation\")\n\n        projects_prompt_template = self._preprocess_template_string(self.strings.prompt_projects)\n        logger.debug(f\"Side projects template: {projects_prompt_template}\")\n\n        prompt = ChatPromptTemplate.from_template(projects_prompt_template)\n        logger.debug(f\"Prompt: {prompt}\")\n        \n        chain = prompt | self.llm_cheap | StrOutputParser()\n        logger.debug(f\"Chain created: {chain}\")\n        \n        input_data = {\n            \"projects\": self.resume.projects\n        } if data is None else data\n        output = chain.invoke(input_data)\n        logger.debug(f\"Chain invocation result: {output}\")\n\n        logger.debug(\"Side projects section generation completed\")\n        return output",
    "Generate the side projects section of the resume.\nArgs:\n    data (dict): The side projects to use for generating the side projects section.\nReturns:\n    str: The generated side projects section."
  ],
  [
    "def generate_achievements_section(self, data = None) -> str:\n        \"\"\"\n        Generate the achievements section of the resume.\n        Args:\n            data (dict): The achievements to use for generating the achievements section.\n        Returns:\n            str: The generated achievements section.\n        \"\"\"\n        logger.debug(\"Starting achievements section generation\")\n\n        achievements_prompt_template = self._preprocess_template_string(self.strings.prompt_achievements)\n        logger.debug(f\"Achievements template: {achievements_prompt_template}\")\n\n        prompt = ChatPromptTemplate.from_template(achievements_prompt_template)\n        logger.debug(f\"Prompt: {prompt}\")\n\n        chain = prompt | self.llm_cheap | StrOutputParser()\n        logger.debug(f\"Chain created: {chain}\")\n\n        input_data = {\n            \"achievements\": self.resume.achievements,\n            \"certifications\": self.resume.certifications,\n        } if data is None else data\n        logger.debug(f\"Input data for the chain: {input_data}\")\n\n        output = chain.invoke(input_data)\n        logger.debug(f\"Chain invocation result: {output}\")\n\n        logger.debug(\"Achievements section generation completed\")\n        return output",
    "Generate the achievements section of the resume.\nArgs:\n    data (dict): The achievements to use for generating the achievements section.\nReturns:\n    str: The generated achievements section."
  ],
  [
    "def generate_certifications_section(self, data = None) -> str:\n        \"\"\"\n        Generate the certifications section of the resume.\n        Returns:\n            str: The generated certifications section.\n        \"\"\"\n        logger.debug(\"Starting Certifications section generation\")\n\n        certifications_prompt_template = self._preprocess_template_string(self.strings.prompt_certifications)\n        logger.debug(f\"Certifications template: {certifications_prompt_template}\")\n\n        prompt = ChatPromptTemplate.from_template(certifications_prompt_template)\n        logger.debug(f\"Prompt: {prompt}\")\n\n        chain = prompt | self.llm_cheap | StrOutputParser()\n        logger.debug(f\"Chain created: {chain}\")\n\n        input_data = {\n            \"certifications\": self.resume.certifications\n        } if data is None else data\n        logger.debug(f\"Input data for the chain: {input_data}\")\n\n        output = chain.invoke(input_data)\n        logger.debug(f\"Chain invocation result: {output}\")\n\n        logger.debug(\"Certifications section generation completed\")\n        return output",
    "Generate the certifications section of the resume.\nReturns:\n    str: The generated certifications section."
  ],
  [
    "def generate_additional_skills_section(self, data = None) -> str:\n        \"\"\"\n        Generate the additional skills section of the resume.\n        Returns:\n            str: The generated additional skills section.\n        \"\"\"\n        additional_skills_prompt_template = self._preprocess_template_string(self.strings.prompt_additional_skills)\n        \n        skills = set()\n        if self.resume.experience_details:\n            for exp in self.resume.experience_details:\n                if exp.skills_acquired:\n                    skills.update(exp.skills_acquired)\n\n        if self.resume.education_details:\n            for edu in self.resume.education_details:\n                if edu.exam:\n                    for exam in edu.exam:\n                        skills.update(exam.keys())\n        prompt = ChatPromptTemplate.from_template(additional_skills_prompt_template)\n        chain = prompt | self.llm_cheap | StrOutputParser()\n        input_data = {\n            \"languages\": self.resume.languages,\n            \"interests\": self.resume.interests,\n            \"skills\": skills,\n        } if data is None else data\n        output = chain.invoke(input_data)\n        \n        return output",
    "Generate the additional skills section of the resume.\nReturns:\n    str: The generated additional skills section."
  ],
  [
    "def generate_html_resume(self) -> str:\n        \"\"\"\n        Generate the full HTML resume based on the resume object.\n        Returns:\n            str: The generated HTML resume.\n        \"\"\"\n        def header_fn():\n            if self.resume.personal_information:\n                return self.generate_header()\n            return \"\"\n\n        def education_fn():\n            if self.resume.education_details:\n                return self.generate_education_section()\n            return \"\"\n\n        def work_experience_fn():\n            if self.resume.experience_details:\n                return self.generate_work_experience_section()\n            return \"\"\n\n        def projects_fn():\n            if self.resume.projects:\n                return self.generate_projects_section()\n            return \"\"\n\n        def achievements_fn():\n            if self.resume.achievements:\n                return self.generate_achievements_section()\n            return \"\"\n        \n        def certifications_fn():\n            if self.resume.certifications:\n                return self.generate_certifications_section()\n            return \"\"\n\n        def additional_skills_fn():\n            if (self.resume.experience_details or self.resume.education_details or\n                self.resume.languages or self.resume.interests):\n                return self.generate_additional_skills_section()\n            return \"\"\n\n        # Create a dictionary to map the function names to their respective callables\n        functions = {\n            \"header\": header_fn,\n            \"education\": education_fn,\n            \"work_experience\": work_experience_fn,\n            \"projects\": projects_fn,\n            \"achievements\": achievements_fn,\n            \"certifications\": certifications_fn,\n            \"additional_skills\": additional_skills_fn,\n        }\n\n        # Use ThreadPoolExecutor to run the functions in parallel\n        with ThreadPoolExecutor() as executor:\n            future_to_section = {executor.submit(fn): section for section, fn in functions.items()}\n            results = {}\n            for future in as_completed(future_to_section):\n                section = future_to_section[future]\n                try:\n                    result = future.result()\n                    if result:\n                        results[section] = result\n                except Exception as exc:\n                    logger.error(f'{section} raised an exception: {exc}')\n        full_resume = \"<body>\\n\"\n        full_resume += f\"  {results.get('header', '')}\\n\"\n        full_resume += \"  <main>\\n\"\n        full_resume += f\"    {results.get('education', '')}\\n\"\n        full_resume += f\"    {results.get('work_experience', '')}\\n\"\n        full_resume += f\"    {results.get('projects', '')}\\n\"\n        full_resume += f\"    {results.get('achievements', '')}\\n\"\n        full_resume += f\"    {results.get('certifications', '')}\\n\"\n        full_resume += f\"    {results.get('additional_skills', '')}\\n\"\n        full_resume += \"  </main>\\n\"\n        full_resume += \"</body>\"\n        return full_resume",
    "Generate the full HTML resume based on the resume object.\nReturns:\n    str: The generated HTML resume."
  ],
  [
    "def set_job_description_from_text(self, job_description_text) -> None:\n        \"\"\"\n        Set the job description text to be used for generating the resume.\n        Args:\n            job_description_text (str): The plain text job description to be used.\n        \"\"\"\n        prompt = ChatPromptTemplate.from_template(self.strings.summarize_prompt_template)\n        chain = prompt | self.llm_cheap | StrOutputParser()\n        output = chain.invoke({\"text\": job_description_text})\n        self.job_description = output",
    "Set the job description text to be used for generating the resume.\nArgs:\n    job_description_text (str): The plain text job description to be used."
  ],
  [
    "def generate_header(self) -> str:\n        \"\"\"\n        Generate the header section of the resume.\n        Returns:\n            str: The generated header section.\n        \"\"\"\n        return super().generate_header(data={\n            \"personal_information\": self.resume.personal_information,\n            \"job_description\": self.job_description\n        })",
    "Generate the header section of the resume.\nReturns:\n    str: The generated header section."
  ],
  [
    "def generate_education_section(self) -> str:\n        \"\"\"\n        Generate the education section of the resume.\n        Returns:\n            str: The generated education section.\n        \"\"\"\n        return super().generate_education_section(data={\n            \"education_details\": self.resume.education_details,\n            \"job_description\": self.job_description\n        })",
    "Generate the education section of the resume.\nReturns:\n    str: The generated education section."
  ],
  [
    "def generate_work_experience_section(self) -> str:\n        \"\"\"\n        Generate the work experience section of the resume.\n        Returns:\n            str: The generated work experience section.\n        \"\"\"\n        return super().generate_work_experience_section(data={\n            \"experience_details\": self.resume.experience_details,\n            \"job_description\": self.job_description\n        })",
    "Generate the work experience section of the resume.\nReturns:\n    str: The generated work experience section."
  ],
  [
    "def generate_projects_section(self) -> str:\n        \"\"\"\n        Generate the side projects section of the resume.\n        Returns:\n            str: The generated side projects section.\n        \"\"\"\n        return super().generate_projects_section(data={\n            \"projects\": self.resume.projects,\n            \"job_description\": self.job_description\n        })",
    "Generate the side projects section of the resume.\nReturns:\n    str: The generated side projects section."
  ],
  [
    "def generate_achievements_section(self) -> str:\n        \"\"\"\n        Generate the achievements section of the resume.\n        Returns:\n            str: The generated achievements section.\n        \"\"\"\n        return super().generate_achievements_section(data={\n            \"achievements\": self.resume.achievements,\n            \"job_description\": self.job_description\n        })",
    "Generate the achievements section of the resume.\nReturns:\n    str: The generated achievements section."
  ],
  [
    "def generate_certifications_section(self) -> str:\n        \"\"\"\n        Generate the certifications section of the resume.\n        Returns:\n            str: The generated certifications section.\n        \"\"\"\n        return super().generate_certifications_section(data={\n            \"certifications\": self.resume.certifications,\n            \"job_description\": self.job_description\n        })",
    "Generate the certifications section of the resume.\nReturns:\n    str: The generated certifications section."
  ],
  [
    "def generate_additional_skills_section(self) -> str:\n        \"\"\"\n        Generate the additional skills section of the resume.\n        Returns:\n            str: The generated additional skills section.\n        \"\"\"\n        additional_skills_prompt_template = self._preprocess_template_string(\n            self.strings.prompt_additional_skills\n        )\n        skills = set()\n        if self.resume.experience_details:\n            for exp in self.resume.experience_details:\n                if exp.skills_acquired:\n                    skills.update(exp.skills_acquired)\n\n        if self.resume.education_details:\n            for edu in self.resume.education_details:\n                if edu.exam:\n                    for exam in edu.exam:\n                        skills.update(exam.keys())\n        prompt = ChatPromptTemplate.from_template(additional_skills_prompt_template)\n        chain = prompt | self.llm_cheap | StrOutputParser()\n        output = chain.invoke({\n            \"languages\": self.resume.languages,\n            \"interests\": self.resume.interests,\n            \"skills\": skills,\n            \"job_description\": self.job_description\n        })\n        return output",
    "Generate the additional skills section of the resume.\nReturns:\n    str: The generated additional skills section."
  ],
  [
    "def _preprocess_template_string(template: str) -> str:\n        \"\"\"\n        Preprocess the template string by removing leading whitespaces and indentation.\n        Args:\n            template (str): The template string to preprocess.\n        Returns:\n            str: The preprocessed template string.\n        \"\"\"\n        return textwrap.dedent(template)",
    "Preprocess the template string by removing leading whitespaces and indentation.\nArgs:\n    template (str): The template string to preprocess.\nReturns:\n    str: The preprocessed template string."
  ],
  [
    "def set_body_html(self, body_html):\n        \"\"\"\n        Retrieves the job description from HTML, processes it, and initializes the vectorstore.\n        Args:\n            body_html (str): The HTML content to process.\n        \"\"\"\n\n        # Save the HTML content to a temporary file\n        with tempfile.NamedTemporaryFile(delete=False, suffix=\".html\", mode=\"w\", encoding=\"utf-8\") as temp_file:\n            temp_file.write(body_html)\n            temp_file_path = temp_file.name \n        try:\n            loader = TextLoader(temp_file_path, encoding=\"utf-8\", autodetect_encoding=True)\n            document = loader.load()\n            logger.debug(\"Document successfully loaded.\")\n        except Exception as e:\n            logger.error(f\"Error during document loading: {e}\")\n            raise\n        finally:\n            os.remove(temp_file_path)\n            logger.debug(f\"Temporary file removed: {temp_file_path}\")\n        \n        # Split the text into chunks\n        text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=50)\n        all_splits = text_splitter.split_documents(document)\n        logger.debug(f\"Text split into {len(all_splits)} fragments.\")\n        \n        # Create the vectorstore using FAISS\n        try:\n            self.vectorstore = FAISS.from_documents(documents=all_splits, embedding=self.llm_embeddings)\n            logger.debug(\"Vectorstore successfully initialized.\")\n        except Exception as e:\n            logger.error(f\"Error during vectorstore creation: {e}\")\n            raise",
    "Retrieves the job description from HTML, processes it, and initializes the vectorstore.\nArgs:\n    body_html (str): The HTML content to process."
  ],
  [
    "def _retrieve_context(self, query: str, top_k: int = 3) -> str:\n        \"\"\"\n        Retrieves the most relevant text fragments using the retriever.\n        Args:\n            query (str): The search query.\n            top_k (int): Number of fragments to retrieve.\n        Returns:\n            str: Concatenated text fragments.\n        \"\"\"\n        if not self.vectorstore:\n            raise ValueError(\"Vectorstore not initialized. Run extract_job_description first.\")\n        \n        retriever = self.vectorstore.as_retriever()\n        retrieved_docs = retriever.get_relevant_documents(query)[:top_k]\n        context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n        logger.debug(f\"Context retrieved for query '{query}': {context[:200]}...\")  # Log the first 200 characters\n        return context",
    "Retrieves the most relevant text fragments using the retriever.\nArgs:\n    query (str): The search query.\n    top_k (int): Number of fragments to retrieve.\nReturns:\n    str: Concatenated text fragments."
  ],
  [
    "def _extract_information(self, question: str, retrieval_query: str) -> str:\n        \"\"\"\n        Generic method to extract specific information using the retriever and LLM.\n        Args:\n            question (str): The question to ask the LLM for extraction.\n            retrieval_query (str): The query to use for retrieving relevant context.\n        Returns:\n            str: The extracted information.\n        \"\"\"\n        context = self._retrieve_context(retrieval_query)\n        \n        prompt = ChatPromptTemplate.from_template(\n            template=\"\"\"\n            You are an expert in extracting specific information from job descriptions. \n            Carefully read the job description context below and provide a clear and concise answer to the question.\n\n            Context: {context}\n\n            Question: {question}\n            Answer:\n            \"\"\"\n        )\n        \n        formatted_prompt = prompt.format(context=context, question=question)\n        logger.debug(f\"Formatted prompt for extraction: {formatted_prompt[:200]}...\")  # Log the first 200 characters\n        \n        try:\n            chain = prompt | self.llm | StrOutputParser()\n            result = chain.invoke({\"context\": context, \"question\": question})\n            extracted_info = result.strip()\n            logger.debug(f\"Extracted information: {extracted_info}\")\n            return extracted_info\n        except Exception as e:  \n            logger.error(f\"Error during information extraction: {e}\")\n            return \"\"",
    "Generic method to extract specific information using the retriever and LLM.\nArgs:\n    question (str): The question to ask the LLM for extraction.\n    retrieval_query (str): The query to use for retrieving relevant context.\nReturns:\n    str: The extracted information."
  ],
  [
    "def extract_job_description(self) -> str:\n        \"\"\"\n        Extracts the company name from the job description.\n        Returns:\n            str: The extracted job description.\n        \"\"\"\n        question = \"What is the job description of the company?\"\n        retrieval_query = \"Job description\"\n        logger.debug(\"Starting job description extraction.\")\n        return self._extract_information(question, retrieval_query)",
    "Extracts the company name from the job description.\nReturns:\n    str: The extracted job description."
  ],
  [
    "def extract_company_name(self) -> str:\n        \"\"\"\n        Extracts the company name from the job description.\n        Returns:\n            str: The extracted company name.\n        \"\"\"\n        question = \"What is the company's name?\"\n        retrieval_query = \"Company name\"\n        logger.debug(\"Starting company name extraction.\")\n        return self._extract_information(question, retrieval_query)",
    "Extracts the company name from the job description.\nReturns:\n    str: The extracted company name."
  ],
  [
    "def extract_role(self) -> str:\n        \"\"\"\n        Extracts the sought role/title from the job description.\n        Returns:\n            str: The extracted role/title.\n        \"\"\"\n        question = \"What is the role or title sought in this job description?\"\n        retrieval_query = \"Job title\"\n        logger.debug(\"Starting role/title extraction.\")\n        return self._extract_information(question, retrieval_query)",
    "Extracts the sought role/title from the job description.\nReturns:\n    str: The extracted role/title."
  ],
  [
    "def extract_location(self) -> str:\n        \"\"\"\n        Extracts the location from the job description.\n        Returns:\n            str: The extracted location.\n        \"\"\"\n        question = \"What is the location mentioned in this job description?\"\n        retrieval_query = \"Location\"\n        logger.debug(\"Starting location extraction.\")\n        return self._extract_information(question, retrieval_query)",
    "Extracts the location from the job description.\nReturns:\n    str: The extracted location."
  ],
  [
    "def extract_recruiter_email(self) -> str:\n        \"\"\"\n        Extracts the recruiter's email from the job description.\n        Returns:\n            str: The extracted recruiter's email.\n        \"\"\"\n        question = \"What is the recruiter's email address in this job description?\"\n        retrieval_query = \"Recruiter email\"\n        logger.debug(\"Starting recruiter email extraction.\")\n        email = self._extract_information(question, retrieval_query)\n        \n        # Validate the extracted email using regex\n        email_regex = r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+'\n        if re.match(email_regex, email):\n            logger.debug(\"Valid recruiter's email.\")\n            return email\n        else:\n            logger.warning(\"Invalid or not found recruiter's email.\")\n            return \"\"",
    "Extracts the recruiter's email from the job description.\nReturns:\n    str: The extracted recruiter's email."
  ],
  [
    "def __init__(self, api_key, style_manager, resume_generator, resume_object, output_path):\n        \"\"\"\n        Initialize the FacadeManager with the given API key, style manager, resume generator, resume object, and log path.\n        Args:\n            api_key (str): The OpenAI API key to be used for generating text.\n            style_manager (StyleManager): The StyleManager instance to manage the styles.\n            resume_generator (ResumeGenerator): The ResumeGenerator instance to generate resumes and cover letters.\n            resume_object (str): The resume object to be used for generating resumes and cover letters.\n            output_path (str): The path to the log file.\n        \"\"\"\n        lib_directory = Path(__file__).resolve().parent\n        global_config.STRINGS_MODULE_RESUME_PATH = lib_directory / \"resume_prompt/strings_feder-cr.py\"\n        global_config.STRINGS_MODULE_RESUME_JOB_DESCRIPTION_PATH = lib_directory / \"resume_job_description_prompt/strings_feder-cr.py\"\n        global_config.STRINGS_MODULE_COVER_LETTER_JOB_DESCRIPTION_PATH = lib_directory / \"cover_letter_prompt/strings_feder-cr.py\"\n        global_config.STRINGS_MODULE_NAME = \"strings_feder_cr\"\n        global_config.STYLES_DIRECTORY = lib_directory / \"resume_style\"\n        global_config.LOG_OUTPUT_FILE_PATH = output_path\n        global_config.API_KEY = api_key\n        self.style_manager = style_manager\n        self.resume_generator = resume_generator\n        self.resume_generator.set_resume_object(resume_object)\n        self.selected_style = None  # Property to store the selected style",
    "Initialize the FacadeManager with the given API key, style manager, resume generator, resume object, and log path.\nArgs:\n    api_key (str): The OpenAI API key to be used for generating text.\n    style_manager (StyleManager): The StyleManager instance to manage the styles.\n    resume_generator (ResumeGenerator): The ResumeGenerator instance to generate resumes and cover letters.\n    resume_object (str): The resume object to be used for generating resumes and cover letters.\n    output_path (str): The path to the log file."
  ],
  [
    "def prompt_user(self, choices: list[str], message: str) -> str:\n        \"\"\"\n        Prompt the user with the given message and choices.\n        Args:\n            choices (list[str]): The list of choices to present to the user.\n            message (str): The message to display to the user.\n        Returns:\n            str: The choice selected by the user.\n        \"\"\"\n        questions = [\n            inquirer.List('selection', message=message, choices=choices),\n        ]\n        return inquirer.prompt(questions)['selection']",
    "Prompt the user with the given message and choices.\nArgs:\n    choices (list[str]): The list of choices to present to the user.\n    message (str): The message to display to the user.\nReturns:\n    str: The choice selected by the user."
  ],
  [
    "def prompt_for_text(self, message: str) -> str:\n        \"\"\"\n        Prompt the user to enter text with the given message.\n        Args:\n            message (str): The message to display to the user.\n        Returns:\n            str: The text entered by the user.\n        \"\"\"\n        questions = [\n            inquirer.Text('text', message=message),\n        ]\n        return inquirer.prompt(questions)['text']",
    "Prompt the user to enter text with the given message.\nArgs:\n    message (str): The message to display to the user.\nReturns:\n    str: The text entered by the user."
  ],
  [
    "def create_resume_pdf_job_tailored(self) -> tuple[bytes, str]:\n        \"\"\"\n        Create a resume PDF using the selected style and the given job description text.\n        Args:\n            job_url (str): The job URL to generate the hash for.\n            job_description_text (str): The job description text to include in the resume.\n        Returns:\n            tuple: A tuple containing the PDF content as bytes and the unique filename.\n        \"\"\"\n        style_path = self.style_manager.get_style_path()\n        if style_path is None:\n            raise ValueError(\"You must choose a style before generating the PDF.\")\n\n\n        html_resume = self.resume_generator.create_resume_job_description_text(style_path, self.job.description)\n\n        # Generate a unique name using the job URL hash\n        suggested_name = hashlib.md5(self.job.link.encode()).hexdigest()[:10]\n        \n        result = HTML_to_PDF(html_resume, self.driver)\n        self.driver.quit()\n        return result, suggested_name",
    "Create a resume PDF using the selected style and the given job description text.\nArgs:\n    job_url (str): The job URL to generate the hash for.\n    job_description_text (str): The job description text to include in the resume.\nReturns:\n    tuple: A tuple containing the PDF content as bytes and the unique filename."
  ],
  [
    "def create_resume_pdf(self) -> tuple[bytes, str]:\n        \"\"\"\n        Create a resume PDF using the selected style and the given job description text.\n        Args:\n            job_url (str): The job URL to generate the hash for.\n            job_description_text (str): The job description text to include in the resume.\n        Returns:\n            tuple: A tuple containing the PDF content as bytes and the unique filename.\n        \"\"\"\n        style_path = self.style_manager.get_style_path()\n        if style_path is None:\n            raise ValueError(\"You must choose a style before generating the PDF.\")\n        \n        html_resume = self.resume_generator.create_resume(style_path)\n        result = HTML_to_PDF(html_resume, self.driver)\n        self.driver.quit()\n        return result",
    "Create a resume PDF using the selected style and the given job description text.\nArgs:\n    job_url (str): The job URL to generate the hash for.\n    job_description_text (str): The job description text to include in the resume.\nReturns:\n    tuple: A tuple containing the PDF content as bytes and the unique filename."
  ],
  [
    "def create_cover_letter(self) -> tuple[bytes, str]:\n        \"\"\"\n        Create a cover letter based on the given job description text and job URL.\n        Args:\n            job_url (str): The job URL to generate the hash for.\n            job_description_text (str): The job description text to include in the cover letter.\n        Returns:\n            tuple: A tuple containing the PDF content as bytes and the unique filename.\n        \"\"\"\n        style_path = self.style_manager.get_style_path()\n        if style_path is None:\n            raise ValueError(\"You must choose a style before generating the PDF.\")\n        \n        \n        cover_letter_html = self.resume_generator.create_cover_letter_job_description(style_path, self.job.description)\n\n        # Generate a unique name using the job URL hash\n        suggested_name = hashlib.md5(self.job.link.encode()).hexdigest()[:10]\n\n        \n        result = HTML_to_PDF(cover_letter_html, self.driver)\n        self.driver.quit()\n        return result, suggested_name",
    "Create a cover letter based on the given job description text and job URL.\nArgs:\n    job_url (str): The job URL to generate the hash for.\n    job_description_text (str): The job description text to include in the cover letter.\nReturns:\n    tuple: A tuple containing the PDF content as bytes and the unique filename."
  ],
  [
    "def generate_random_hex(length: int = 17) -> str:\n    \"\"\"Generate a random hex string\n\n    Args:\n        length (int, optional): Length of the hex string. Defaults to 17.\n\n    Returns:\n        str: Random hex string\n    \"\"\"\n    return secrets.token_hex(length)",
    "Generate a random hex string\n\nArgs:\n    length (int, optional): Length of the hex string. Defaults to 17.\n\nReturns:\n    str: Random hex string"
  ],
  [
    "def random_int(min: int, max: int) -> int:\n    \"\"\"Generate a random integer\n\n    Args:\n        min (int): Minimum value\n        max (int): Maximum value\n\n    Returns:\n        int: Random integer\n    \"\"\"\n    return secrets.randbelow(max - min) + min",
    "Generate a random integer\n\nArgs:\n    min (int): Minimum value\n    max (int): Maximum value\n\nReturns:\n    int: Random integer"
  ],
  [
    "def logger(is_timed: bool) -> function:\n    \"\"\"Logger decorator\n\n    Args:\n        is_timed (bool): Whether to include function running time in exit log\n\n    Returns:\n        _type_: decorated function\n    \"\"\"\n\n    def decorator(func: function) -> function:\n        wraps(func)\n\n        def wrapper(*args, **kwargs):\n            log.debug(\n                \"Entering %s with args %s and kwargs %s\",\n                func.__name__,\n                args,\n                kwargs,\n            )\n            start = time.time()\n            out = func(*args, **kwargs)\n            end = time.time()\n            if is_timed:\n                log.debug(\n                    \"Exiting %s with return value %s. Took %s seconds.\",\n                    func.__name__,\n                    out,\n                    end - start,\n                )\n            else:\n                log.debug(\"Exiting %s with return value %s\", func.__name__, out)\n            return out\n\n        return wrapper\n\n    return decorator",
    "Logger decorator\n\nArgs:\n    is_timed (bool): Whether to include function running time in exit log\n\nReturns:\n    _type_: decorated function"
  ],
  [
    "def get_arkose_token(\n    download_images: bool = True,\n    solver: function = captcha_solver,\n    captcha_supported: bool = True,\n) -> str:\n    \"\"\"\n    The solver function should take in a list of images in base64 and a dict of challenge details\n    and return the index of the image that matches the challenge details\n\n    Challenge details:\n        game_type: str - Audio or Image\n        instructions: str - Instructions for the captcha\n        URLs: list[str] - URLs of the images or audio files\n    \"\"\"\n    if captcha_supported:\n        resp = requests.get(\n            (CAPTCHA_URL + \"start?download_images=true\")\n            if download_images\n            else CAPTCHA_URL + \"start\",\n        )\n        resp_json: dict = resp.json()\n        if resp.status_code == 200:\n            return resp_json.get(\"token\")\n        if resp.status_code != 511:\n            raise Exception(resp_json.get(\"error\", \"Unknown error\"))\n\n        if resp_json.get(\"status\") != \"captcha\":\n            raise Exception(\"unknown error\")\n\n        challenge_details: dict = resp_json.get(\"session\", {}).get(\"concise_challenge\")\n        if not challenge_details:\n            raise Exception(\"missing details\")\n\n        images: list[str] = resp_json.get(\"images\")\n\n        index = solver(images, challenge_details)\n\n        resp = requests.post(\n            CAPTCHA_URL + \"verify\",\n            json={\"session\": resp_json.get(\"session\"), \"index\": index},\n        )\n        if resp.status_code != 200:\n            raise Exception(\"Failed to verify captcha\")\n        return resp_json.get(\"token\")\n    working_endpoints: list[str] = []\n    # Check uptime for different endpoints via gatus\n    resp2: list[dict] = requests.get(\n        \"https://stats.churchless.tech/api/v1/endpoints/statuses?page=1\",\n    ).json()\n    for endpoint in resp2:\n        # print(endpoint.get(\"name\"))\n        if endpoint.get(\"group\") != \"Arkose Labs\":\n            continue\n        # Check the last 5 results\n        results: list[dict] = endpoint.get(\"results\", [])[-5:-1]\n        # print(results)\n        if not results:\n            print(f\"Endpoint {endpoint.get('name')} has no results\")\n            continue\n        # Check if all the results are up\n        if all(result.get(\"success\") is True for result in results):\n            working_endpoints.append(endpoint.get(\"name\"))\n    if not working_endpoints:\n        print(\"No working endpoints found. Please solve the captcha manually.\")\n        return get_arkose_token(download_images=True, captcha_supported=True)\n    # Choose a random endpoint\n    endpoint = random.choice(working_endpoints)\n    resp: requests.Response = requests.get(endpoint)\n    if resp.status_code != 200:\n        if resp.status_code != 511:\n            raise Exception(\"Failed to get captcha token\")\n        print(\"Captcha required. Please solve the captcha manually.\")\n        return get_arkose_token(download_images=True, captcha_supported=True)\n    try:\n        return resp.json().get(\"token\")\n    except Exception:\n        return resp.text",
    "The solver function should take in a list of images in base64 and a dict of challenge details\nand return the index of the image that matches the challenge details\n\nChallenge details:\n    game_type: str - Audio or Image\n    instructions: str - Instructions for the captcha\n    URLs: list[str] - URLs of the images or audio files"
  ],
  [
    "def configure() -> dict:\n    \"\"\"\n    Looks for a config file in the following locations:\n    \"\"\"\n    config_files: list[Path] = [Path(\"config.json\")]\n    if xdg_config_home := getenv(\"XDG_CONFIG_HOME\"):\n        config_files.append(Path(xdg_config_home, \"revChatGPT/config.json\"))\n    if user_home := getenv(\"HOME\"):\n        config_files.append(Path(user_home, \".config/revChatGPT/config.json\"))\n    if windows_home := getenv(\"HOMEPATH\"):\n        config_files.append(Path(f\"{windows_home}/.config/revChatGPT/config.json\"))\n    if config_file := next((f for f in config_files if f.exists()), None):\n        with open(config_file, encoding=\"utf-8\") as f:\n            config = json.load(f)\n    else:\n        print(\"No config file found.\")\n        raise FileNotFoundError(\"No config file found.\")\n    return config",
    "Looks for a config file in the following locations:"
  ],
  [
    "def main(config: dict) -> NoReturn:\n    \"\"\"\n    Main function for the chatGPT program.\n    \"\"\"\n    chatbot = Chatbot(\n        config,\n        conversation_id=config.get(\"conversation_id\"),\n        parent_id=config.get(\"parent_id\"),\n    )\n\n    def handle_commands(command: str) -> bool:\n        if command == \"!help\":\n            print(\n                \"\"\"\n            !help - Show this message\n            !reset - Forget the current conversation\n            !config - Show the current configuration\n            !plugins - Show the current plugins\n            !switch x - Switch to plugin x. Need to reset the conversation to ativate the plugin.\n            !rollback x - Rollback the conversation (x being the number of messages to rollback)\n            !setconversation - Changes the conversation\n            !share - Creates a share link to the current conversation\n            !exit - Exit this program\n            \"\"\",\n            )\n        elif command == \"!reset\":\n            chatbot.reset_chat()\n            print(\"Chat session successfully reset.\")\n        elif command == \"!config\":\n            print(json.dumps(chatbot.config, indent=4))\n        elif command.startswith(\"!rollback\"):\n            try:\n                rollback = int(command.split(\" \")[1])\n            except IndexError:\n                logging.exception(\n                    \"No number specified, rolling back 1 message\",\n                    stack_info=True,\n                )\n                rollback = 1\n            chatbot.rollback_conversation(rollback)\n            print(f\"Rolled back {rollback} messages.\")\n        elif command.startswith(\"!setconversation\"):\n            try:\n                chatbot.conversation_id = chatbot.config[\n                    \"conversation_id\"\n                ] = command.split(\" \")[1]\n                print(\"Conversation has been changed\")\n            except IndexError:\n                log.exception(\n                    \"Please include conversation UUID in command\",\n                    stack_info=True,\n                )\n                print(\"Please include conversation UUID in command\")\n        elif command.startswith(\"!continue\"):\n            print()\n            print(f\"{bcolors.OKGREEN + bcolors.BOLD}Chatbot: {bcolors.ENDC}\")\n            prev_text = \"\"\n            for data in chatbot.continue_write():\n                message = data[\"message\"][len(prev_text) :]\n                print(message, end=\"\", flush=True)\n                prev_text = data[\"message\"]\n            print(bcolors.ENDC)\n            print()\n        elif command.startswith(\"!share\"):\n            print(f\"Conversation shared at {chatbot.share_conversation()}\")\n        elif command == \"!exit\":\n            if isinstance(chatbot.session, httpx.AsyncClient):\n                chatbot.session.aclose()\n            sys.exit()\n        else:\n            return False\n        return True\n\n    session = create_session()\n    completer = create_completer(\n        [\n            \"!help\",\n            \"!reset\",\n            \"!config\",\n            \"!rollback\",\n            \"!exit\",\n            \"!setconversation\",\n            \"!continue\",\n            \"!plugins\",\n            \"!switch\",\n            \"!share\",\n        ],\n    )\n    print()\n    try:\n        result = {}\n        while True:\n            print(f\"{bcolors.OKBLUE + bcolors.BOLD}You: {bcolors.ENDC}\")\n\n            prompt = get_input(session=session, completer=completer)\n            if prompt.startswith(\"!\") and handle_commands(prompt):\n                continue\n\n            print()\n            print(f\"{bcolors.OKGREEN + bcolors.BOLD}Chatbot: {bcolors.ENDC}\")\n            if chatbot.config.get(\"model\") == \"gpt-4-browsing\":\n                print(\"Browsing takes a while, please wait...\")\n            with Live(Markdown(\"\"), auto_refresh=False) as live:\n                for data in chatbot.ask(prompt=prompt, auto_continue=True):\n                    if data[\"recipient\"] != \"all\":\n                        continue\n                    result = data\n                    message = data[\"message\"]\n                    live.update(Markdown(message), refresh=True)\n            print()\n\n            if result.get(\"citations\", False):\n                print(\n                    f\"{bcolors.WARNING + bcolors.BOLD}Citations: {bcolors.ENDC}\",\n                )\n                for citation in result[\"citations\"]:\n                    print(\n                        f'{citation[\"metadata\"][\"title\"]}: {citation[\"metadata\"][\"url\"]}',\n                    )\n                print()\n\n    except (KeyboardInterrupt, EOFError):\n        sys.exit()\n    except Exception as exc:\n        error = t.CLIError(\"command line program unknown error\")\n        raise error from exc",
    "Main function for the chatGPT program."
  ],
  [
    "def __init__(\n        self,\n        config: dict[str, str],\n        conversation_id: str | None = None,\n        parent_id: str | None = None,\n        lazy_loading: bool = True,\n        base_url: str | None = None,\n        captcha_solver: function = captcha_solver,\n        captcha_download_images: bool = True,\n    ) -> None:\n        \"\"\"Initialize a chatbot\n\n        Args:\n            config (dict[str, str]): Login and proxy info. Example:\n                {\n                    \"access_token\": \"<access_token>\"\n                    \"proxy\": \"<proxy_url_string>\",\n                    \"model\": \"<model_name>\",\n                    \"plugin\": \"<plugin_id>\",\n                }\n                More details on these are available at https://github.com/acheong08/ChatGPT#configuration\n            conversation_id (str | None, optional): Id of the conversation to continue on. Defaults to None.\n            parent_id (str | None, optional): Id of the previous response message to continue on. Defaults to None.\n            lazy_loading (bool, optional): Whether to load only the active conversation. Defaults to True.\n            base_url (str | None, optional): Base URL of the ChatGPT server. Defaults to None.\n            captcha_solver (function, optional): Function to solve captcha. Defaults to captcha_solver.\n            captcha_download_images (bool, optional): Whether to download captcha images. Defaults to True.\n\n        Raises:\n            Exception: _description_\n        \"\"\"\n        user_home = getenv(\"HOME\") or getenv(\"USERPROFILE\")\n        if user_home is None:\n            user_home = Path().cwd()\n            self.cache_path = Path(Path().cwd(), \".chatgpt_cache.json\")\n        else:\n            # mkdir ~/.config/revChatGPT\n            if not Path(user_home, \".config\").exists():\n                Path(user_home, \".config\").mkdir()\n            if not Path(user_home, \".config\", \"revChatGPT\").exists():\n                Path(user_home, \".config\", \"revChatGPT\").mkdir()\n            self.cache_path = Path(user_home, \".config\", \"revChatGPT\", \"cache.json\")\n\n        self.config = config\n        self.session = requests.Session()\n        if \"email\" in config and \"password\" in config:\n            try:\n                cached_access_token = self.__get_cached_access_token(\n                    self.config.get(\"email\", None),\n                )\n            except t.Error as error:\n                if error.code == 5:\n                    raise\n                cached_access_token = None\n            if cached_access_token is not None:\n                self.config[\"access_token\"] = cached_access_token\n\n        if \"proxy\" in config:\n            if not isinstance(config[\"proxy\"], str):\n                error = TypeError(\"Proxy must be a string!\")\n                raise error\n            proxies = {\n                \"http\": config[\"proxy\"],\n                \"https\": config[\"proxy\"],\n            }\n            if isinstance(self.session, AsyncClient):\n                proxies = {\n                    \"http://\": config[\"proxy\"],\n                    \"https://\": config[\"proxy\"],\n                }\n                self.session = AsyncClient(proxies=proxies)  # type: ignore\n            else:\n                self.session.proxies.update(proxies)\n\n        self.conversation_id = conversation_id or config.get(\"conversation_id\")\n        self.parent_id = parent_id or config.get(\"parent_id\")\n        self.conversation_mapping = {}\n        self.conversation_id_prev_queue = []\n        self.parent_id_prev_queue = []\n        self.lazy_loading = lazy_loading\n        self.base_url = base_url or BASE_URL\n        self.disable_history = config.get(\"disable_history\", False)\n\n        self.__check_credentials()\n\n        if self.config.get(\"plugin_ids\", []):\n            for plugin in self.config.get(\"plugin_ids\"):\n                self.install_plugin(plugin)\n        if self.config.get(\"unverified_plugin_domains\", []):\n            for domain in self.config.get(\"unverified_plugin_domains\"):\n                if self.config.get(\"plugin_ids\"):\n                    self.config[\"plugin_ids\"].append(\n                        self.get_unverified_plugin(domain, install=True).get(\"id\"),\n                    )\n                else:\n                    self.config[\"plugin_ids\"] = [\n                        self.get_unverified_plugin(domain, install=True).get(\"id\"),\n                    ]\n        # Get PUID cookie\n        try:\n            auth = Authenticator(\"blah\", \"blah\")\n            auth.access_token = self.config[\"access_token\"]\n            puid = auth.get_puid()\n            self.session.headers.update({\"PUID\": puid})\n            print(\"Setting PUID (You are a Plus user!): \" + puid)\n        except:\n            pass\n        self.captcha_solver = captcha_solver\n        self.captcha_download_images = captcha_download_images",
    "Initialize a chatbot\n\nArgs:\n    config (dict[str, str]): Login and proxy info. Example:\n        {\n            \"access_token\": \"<access_token>\"\n            \"proxy\": \"<proxy_url_string>\",\n            \"model\": \"<model_name>\",\n            \"plugin\": \"<plugin_id>\",\n        }\n        More details on these are available at https://github.com/acheong08/ChatGPT#configuration\n    conversation_id (str | None, optional): Id of the conversation to continue on. Defaults to None.\n    parent_id (str | None, optional): Id of the previous response message to continue on. Defaults to None.\n    lazy_loading (bool, optional): Whether to load only the active conversation. Defaults to True.\n    base_url (str | None, optional): Base URL of the ChatGPT server. Defaults to None.\n    captcha_solver (function, optional): Function to solve captcha. Defaults to captcha_solver.\n    captcha_download_images (bool, optional): Whether to download captcha images. Defaults to True.\n\nRaises:\n    Exception: _description_"
  ],
  [
    "def __check_credentials(self) -> None:\n        \"\"\"Check login info and perform login\n\n        Any one of the following is sufficient for login. Multiple login info can be provided at the same time and they will be used in the order listed below.\n            - access_token\n            - email + password\n\n        Raises:\n            Exception: _description_\n            AuthError: _description_\n        \"\"\"\n        if \"access_token\" in self.config:\n            self.set_access_token(self.config[\"access_token\"])\n        elif \"email\" not in self.config or \"password\" not in self.config:\n            error = t.AuthenticationError(\"Insufficient login details provided!\")\n            raise error\n        if \"access_token\" not in self.config:\n            try:\n                self.login()\n            except Exception as error:\n                print(error)\n                raise error",
    "Check login info and perform login\n\nAny one of the following is sufficient for login. Multiple login info can be provided at the same time and they will be used in the order listed below.\n    - access_token\n    - email + password\n\nRaises:\n    Exception: _description_\n    AuthError: _description_"
  ],
  [
    "def set_access_token(self, access_token: str) -> None:\n        \"\"\"Set access token in request header and self.config, then cache it to file.\n\n        Args:\n            access_token (str): access_token\n        \"\"\"\n        self.session.headers.clear()\n        self.session.headers.update(\n            {\n                \"Accept\": \"text/event-stream\",\n                \"Authorization\": f\"Bearer {access_token}\",\n                \"Content-Type\": \"application/json\",\n                \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\",\n            },\n        )\n\n        self.config[\"access_token\"] = access_token\n\n        email = self.config.get(\"email\", None)\n        if email is not None:\n            self.__cache_access_token(email, access_token)",
    "Set access token in request header and self.config, then cache it to file.\n\nArgs:\n    access_token (str): access_token"
  ],
  [
    "def __get_cached_access_token(self, email: str | None) -> str | None:\n        \"\"\"Read access token from cache\n\n        Args:\n            email (str | None): email of the account to get access token\n\n        Raises:\n            Error: _description_\n            Error: _description_\n            Error: _description_\n\n        Returns:\n            str | None: access token string or None if not found\n        \"\"\"\n        email = email or \"default\"\n        cache = self.__read_cache()\n        access_token = cache.get(\"access_tokens\", {}).get(email, None)\n\n        # Parse access_token as JWT\n        if access_token is not None:\n            try:\n                # Split access_token into 3 parts\n                s_access_token = access_token.split(\".\")\n                # Add padding to the middle part\n                s_access_token[1] += \"=\" * ((4 - len(s_access_token[1]) % 4) % 4)\n                d_access_token = base64.b64decode(s_access_token[1])\n                d_access_token = json.loads(d_access_token)\n            except binascii.Error:\n                del cache[\"access_tokens\"][email]\n                self.__write_cache(cache)\n                error = t.Error(\n                    source=\"__get_cached_access_token\",\n                    message=\"Invalid access token\",\n                    code=t.ErrorType.INVALID_ACCESS_TOKEN_ERROR,\n                )\n                del cache[\"access_tokens\"][email]\n                raise error from None\n            except json.JSONDecodeError:\n                del cache[\"access_tokens\"][email]\n                self.__write_cache(cache)\n                error = t.Error(\n                    source=\"__get_cached_access_token\",\n                    message=\"Invalid access token\",\n                    code=t.ErrorType.INVALID_ACCESS_TOKEN_ERROR,\n                )\n                raise error from None\n            except IndexError:\n                del cache[\"access_tokens\"][email]\n                self.__write_cache(cache)\n                error = t.Error(\n                    source=\"__get_cached_access_token\",\n                    message=\"Invalid access token\",\n                    code=t.ErrorType.INVALID_ACCESS_TOKEN_ERROR,\n                )\n                raise error from None\n\n            exp = d_access_token.get(\"exp\", None)\n            if exp is not None and exp < time.time():\n                error = t.Error(\n                    source=\"__get_cached_access_token\",\n                    message=\"Access token expired\",\n                    code=t.ErrorType.EXPIRED_ACCESS_TOKEN_ERROR,\n                )\n                raise error\n\n        return access_token",
    "Read access token from cache\n\nArgs:\n    email (str | None): email of the account to get access token\n\nRaises:\n    Error: _description_\n    Error: _description_\n    Error: _description_\n\nReturns:\n    str | None: access token string or None if not found"
  ],
  [
    "def __cache_access_token(self, email: str, access_token: str) -> None:\n        \"\"\"Write an access token to cache\n\n        Args:\n            email (str): account email\n            access_token (str): account access token\n        \"\"\"\n        email = email or \"default\"\n        cache = self.__read_cache()\n        if \"access_tokens\" not in cache:\n            cache[\"access_tokens\"] = {}\n        cache[\"access_tokens\"][email] = access_token\n        self.__write_cache(cache)",
    "Write an access token to cache\n\nArgs:\n    email (str): account email\n    access_token (str): account access token"
  ],
  [
    "def __write_cache(self, info: dict) -> None:\n        \"\"\"Write cache info to file\n\n        Args:\n            info (dict): cache info, current format\n            {\n                \"access_tokens\":{\"someone@example.com\": 'this account's access token', }\n            }\n        \"\"\"\n        dirname = self.cache_path.home() or Path(\".\")\n        dirname.mkdir(parents=True, exist_ok=True)\n        json.dump(info, open(self.cache_path, \"w\", encoding=\"utf-8\"), indent=4)",
    "Write cache info to file\n\nArgs:\n    info (dict): cache info, current format\n    {\n        \"access_tokens\":{\"someone@example.com\": 'this account's access token', }\n    }"
  ],
  [
    "def login(self) -> None:\n        \n        if not self.config.get(\"email\") and not self.config.get(\"password\"):\n            log.error(\"Insufficient login details provided!\")\n            error = t.AuthenticationError(\"Insufficient login details provided!\")\n            raise error\n        auth = Authenticator(\n            email_address=self.config.get(\"email\"),\n            password=self.config.get(\"password\"),\n            proxy=self.config.get(\"proxy\"),\n        )\n        log.debug(\"Using authenticator to get access token\")\n\n        self.set_access_token(auth.get_access_token())",
    "Login to OpenAI by email and password"
  ],
  [
    "def post_messages(\n        self,\n        messages: list[dict],\n        conversation_id: str | None = None,\n        parent_id: str | None = None,\n        plugin_ids: list = None,\n        model: str | None = None,\n        auto_continue: bool = False,\n        timeout: float = 360,\n        **kwargs,\n    ) -> Generator[dict, None, None]:\n        \"\"\"Ask a question to the chatbot\n        Args:\n            messages (list[dict]): The messages to send\n            conversation_id (str | None, optional): UUID for the conversation to continue on. Defaults to None.\n            parent_id (str | None, optional): UUID for the message to continue on. Defaults to None.\n            model (str | None, optional): The model to use. Defaults to None.\n            auto_continue (bool, optional): Whether to continue the conversation automatically. Defaults to False.\n            timeout (float, optional): Timeout for getting the full response, unit is second. Defaults to 360.\n\n        Yields: Generator[dict, None, None] - The response from the chatbot\n            dict: {\n                \"message\": str,\n                \"conversation_id\": str,\n                \"parent_id\": str,\n                \"model\": str,\n                \"finish_details\": str, # \"max_tokens\" or \"stop\"\n                \"end_turn\": bool,\n                \"recipient\": str,\n                \"citations\": list[dict],\n            }\n        \"\"\"\n        if plugin_ids is None:\n            plugin_ids = []\n        if parent_id and not conversation_id:\n            raise t.Error(\n                source=\"User\",\n                message=\"conversation_id must be set once parent_id is set\",\n                code=t.ErrorType.USER_ERROR,\n            )\n\n        if conversation_id and conversation_id != self.conversation_id:\n            self.parent_id = None\n        conversation_id = conversation_id or self.conversation_id\n        parent_id = parent_id or self.parent_id or \"\"\n        if not conversation_id and not parent_id:\n            parent_id = str(uuid.uuid4())\n\n        if conversation_id and not parent_id:\n            if conversation_id not in self.conversation_mapping:\n                if self.lazy_loading:\n                    log.debug(\n                        \"Conversation ID %s not found in conversation mapping, try to get conversation history for the given ID\",\n                        conversation_id,\n                    )\n                    try:\n                        history = self.get_msg_history(conversation_id)\n                        self.conversation_mapping[conversation_id] = history[\n                            \"current_node\"\n                        ]\n                    except requests.exceptions.HTTPError:\n                        print(\"Conversation unavailable\")\n                else:\n                    self.__map_conversations()\n            if conversation_id in self.conversation_mapping:\n                parent_id = self.conversation_mapping[conversation_id]\n            else:\n                print(\n                    \"Warning: Invalid conversation_id provided, treat as a new conversation\",\n                )\n                conversation_id = None\n                parent_id = str(uuid.uuid4())\n        model = model or self.config.get(\"model\") or \"text-davinci-002-render-sha\"\n        data = {\n            \"action\": \"next\",\n            \"messages\": messages,\n            \"conversation_id\": conversation_id,\n            \"parent_message_id\": parent_id,\n            \"model\": model,\n            \"history_and_training_disabled\": self.disable_history,\n        }\n        plugin_ids = self.config.get(\"plugin_ids\", []) or plugin_ids\n        if len(plugin_ids) > 0 and not conversation_id:\n            data[\"plugin_ids\"] = plugin_ids\n\n        yield from self.__send_request(\n            data,\n            timeout=timeout,\n            auto_continue=auto_continue,\n        )",
    "Ask a question to the chatbot\nArgs:\n    messages (list[dict]): The messages to send\n    conversation_id (str | None, optional): UUID for the conversation to continue on. Defaults to None.\n    parent_id (str | None, optional): UUID for the message to continue on. Defaults to None.\n    model (str | None, optional): The model to use. Defaults to None.\n    auto_continue (bool, optional): Whether to continue the conversation automatically. Defaults to False.\n    timeout (float, optional): Timeout for getting the full response, unit is second. Defaults to 360.\n\nYields: Generator[dict, None, None] - The response from the chatbot\n    dict: {\n        \"message\": str,\n        \"conversation_id\": str,\n        \"parent_id\": str,\n        \"model\": str,\n        \"finish_details\": str, # \"max_tokens\" or \"stop\"\n        \"end_turn\": bool,\n        \"recipient\": str,\n        \"citations\": list[dict],\n    }"
  ],
  [
    "def ask(\n        self,\n        prompt: str,\n        conversation_id: str | None = None,\n        parent_id: str = \"\",\n        model: str = \"\",\n        plugin_ids: list = None,\n        auto_continue: bool = False,\n        timeout: float = 360,\n        **kwargs,\n    ) -> Generator[dict, None, None]:\n        \"\"\"Ask a question to the chatbot\n        Args:\n            prompt (str): The question\n            conversation_id (str, optional): UUID for the conversation to continue on. Defaults to None.\n            parent_id (str, optional): UUID for the message to continue on. Defaults to \"\".\n            model (str, optional): The model to use. Defaults to \"\".\n            auto_continue (bool, optional): Whether to continue the conversation automatically. Defaults to False.\n            timeout (float, optional): Timeout for getting the full response, unit is second. Defaults to 360.\n\n        Yields: The response from the chatbot\n            dict: {\n                \"message\": str,\n                \"conversation_id\": str,\n                \"parent_id\": str,\n                \"model\": str,\n                \"finish_details\": str, # \"max_tokens\" or \"stop\"\n                \"end_turn\": bool,\n                \"recipient\": str,\n            }\n        \"\"\"\n        if plugin_ids is None:\n            plugin_ids = []\n        messages = [\n            {\n                \"id\": str(uuid.uuid4()),\n                \"role\": \"user\",\n                \"author\": {\"role\": \"user\"},\n                \"content\": {\"content_type\": \"text\", \"parts\": [prompt]},\n            },\n        ]\n\n        yield from self.post_messages(\n            messages,\n            conversation_id=conversation_id,\n            parent_id=parent_id,\n            plugin_ids=plugin_ids,\n            model=model,\n            auto_continue=auto_continue,\n            timeout=timeout,\n        )",
    "Ask a question to the chatbot\nArgs:\n    prompt (str): The question\n    conversation_id (str, optional): UUID for the conversation to continue on. Defaults to None.\n    parent_id (str, optional): UUID for the message to continue on. Defaults to \"\".\n    model (str, optional): The model to use. Defaults to \"\".\n    auto_continue (bool, optional): Whether to continue the conversation automatically. Defaults to False.\n    timeout (float, optional): Timeout for getting the full response, unit is second. Defaults to 360.\n\nYields: The response from the chatbot\n    dict: {\n        \"message\": str,\n        \"conversation_id\": str,\n        \"parent_id\": str,\n        \"model\": str,\n        \"finish_details\": str, # \"max_tokens\" or \"stop\"\n        \"end_turn\": bool,\n        \"recipient\": str,\n    }"
  ],
  [
    "def continue_write(\n        self,\n        conversation_id: str | None = None,\n        parent_id: str = \"\",\n        model: str = \"\",\n        auto_continue: bool = False,\n        timeout: float = 360,\n    ) -> Generator[dict, None, None]:\n        \"\"\"let the chatbot continue to write.\n        Args:\n            conversation_id (str | None, optional): UUID for the conversation to continue on. Defaults to None.\n            parent_id (str, optional): UUID for the message to continue on. Defaults to None.\n            model (str, optional): The model to use. Defaults to None.\n            auto_continue (bool, optional): Whether to continue the conversation automatically. Defaults to False.\n            timeout (float, optional): Timeout for getting the full response, unit is second. Defaults to 360.\n\n        Yields:\n            dict: {\n                \"message\": str,\n                \"conversation_id\": str,\n                \"parent_id\": str,\n                \"model\": str,\n                \"finish_details\": str, # \"max_tokens\" or \"stop\"\n                \"end_turn\": bool,\n                \"recipient\": str,\n            }\n        \"\"\"\n        if parent_id and not conversation_id:\n            raise t.Error(\n                source=\"User\",\n                message=\"conversation_id must be set once parent_id is set\",\n                code=t.ErrorType.USER_ERROR,\n            )\n\n        if conversation_id and conversation_id != self.conversation_id:\n            self.parent_id = None\n        conversation_id = conversation_id or self.conversation_id\n        parent_id = parent_id or self.parent_id or \"\"\n        if not conversation_id and not parent_id:\n            parent_id = str(uuid.uuid4())\n\n        if conversation_id and not parent_id:\n            if conversation_id not in self.conversation_mapping:\n                if self.lazy_loading:\n                    log.debug(\n                        \"Conversation ID %s not found in conversation mapping, try to get conversation history for the given ID\",\n                        conversation_id,\n                    )\n                    with contextlib.suppress(Exception):\n                        history = self.get_msg_history(conversation_id)\n                        self.conversation_mapping[conversation_id] = history[\n                            \"current_node\"\n                        ]\n                else:\n                    log.debug(\n                        f\"Conversation ID {conversation_id} not found in conversation mapping, mapping conversations\",\n                    )\n                    self.__map_conversations()\n            if conversation_id in self.conversation_mapping:\n                parent_id = self.conversation_mapping[conversation_id]\n            else:  # invalid conversation_id provided, treat as a new conversation\n                conversation_id = None\n                parent_id = str(uuid.uuid4())\n        model = model or self.config.get(\"model\") or \"text-davinci-002-render-sha\"\n        data = {\n            \"action\": \"continue\",\n            \"conversation_id\": conversation_id,\n            \"parent_message_id\": parent_id,\n            \"model\": model\n            or self.config.get(\"model\")\n            or (\n                \"text-davinci-002-render-paid\"\n                if self.config.get(\"paid\")\n                else \"text-davinci-002-render-sha\"\n            ),\n            \"history_and_training_disabled\": self.disable_history,\n        }\n        yield from self.__send_request(\n            data,\n            timeout=timeout,\n            auto_continue=auto_continue,\n        )",
    "let the chatbot continue to write.\nArgs:\n    conversation_id (str | None, optional): UUID for the conversation to continue on. Defaults to None.\n    parent_id (str, optional): UUID for the message to continue on. Defaults to None.\n    model (str, optional): The model to use. Defaults to None.\n    auto_continue (bool, optional): Whether to continue the conversation automatically. Defaults to False.\n    timeout (float, optional): Timeout for getting the full response, unit is second. Defaults to 360.\n\nYields:\n    dict: {\n        \"message\": str,\n        \"conversation_id\": str,\n        \"parent_id\": str,\n        \"model\": str,\n        \"finish_details\": str, # \"max_tokens\" or \"stop\"\n        \"end_turn\": bool,\n        \"recipient\": str,\n    }"
  ],
  [
    "def __check_response(self, response: requests.Response) -> None:\n        \"\"\"Make sure response is success\n\n        Args:\n            response (_type_): _description_\n\n        Raises:\n            Error: _description_\n        \"\"\"\n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError as ex:\n            error = t.Error(\n                source=\"OpenAI\",\n                message=response.text,\n                code=response.status_code,\n            )\n            raise error from ex",
    "Make sure response is success\n\nArgs:\n    response (_type_): _description_\n\nRaises:\n    Error: _description_"
  ],
  [
    "def get_conversations(\n        self,\n        offset: int = 0,\n        limit: int = 20,\n        encoding: str | None = None,\n    ) -> list:\n        \"\"\"\n        Get conversations\n        :param offset: Integer\n        :param limit: Integer\n        \"\"\"\n        url = f\"{self.base_url}conversations?offset={offset}&limit={limit}\"\n        response = self.session.get(url)\n        self.__check_response(response)\n        if encoding is not None:\n            response.encoding = encoding\n        data = json.loads(response.text)\n        return data[\"items\"]",
    "Get conversations\n:param offset: Integer\n:param limit: Integer"
  ],
  [
    "def get_msg_history(self, convo_id: str, encoding: str | None = None) -> list:\n        \"\"\"\n        Get message history\n        :param id: UUID of conversation\n        :param encoding: String\n        \"\"\"\n        url = f\"{self.base_url}conversation/{convo_id}\"\n        response = self.session.get(url)\n        self.__check_response(response)\n        if encoding is not None:\n            response.encoding = encoding\n        return response.json()",
    "Get message history\n:param id: UUID of conversation\n:param encoding: String"
  ],
  [
    "def share_conversation(\n        self,\n        title: str = None,\n        convo_id: str = None,\n        node_id: str = None,\n        anonymous: bool = True,\n    ) -> str:\n        \"\"\"\n        Creates a share link to a conversation\n        :param convo_id: UUID of conversation\n        :param node_id: UUID of node\n        :param anonymous: Boolean\n        :param title: String\n\n        Returns:\n            str: A URL to the shared link\n        \"\"\"\n        convo_id = convo_id or self.conversation_id\n        node_id = node_id or self.parent_id\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"origin\": \"https://chat.openai.com\",\n            \"referer\": f\"https://chat.openai.com/c/{convo_id}\",\n        }\n        # First create the share\n        payload = {\n            \"conversation_id\": convo_id,\n            \"current_node_id\": node_id,\n            \"is_anonymous\": anonymous,\n        }\n        url = f\"{self.base_url}share/create\"\n        response = self.session.post(url, data=json.dumps(payload), headers=headers)\n        self.__check_response(response)\n        share_url = response.json().get(\"share_url\")\n        # Then patch the share to make public\n        share_id = response.json().get(\"share_id\")\n        url = f\"{self.base_url}share/{share_id}\"\n        payload = {\n            \"share_id\": share_id,\n            \"highlighted_message_id\": node_id,\n            \"title\": title or response.json().get(\"title\", \"New chat\"),\n            \"is_public\": True,\n            \"is_visible\": True,\n            \"is_anonymous\": True,\n        }\n        response = self.session.patch(url, data=json.dumps(payload), headers=headers)\n        self.__check_response(response)\n        return share_url",
    "Creates a share link to a conversation\n:param convo_id: UUID of conversation\n:param node_id: UUID of node\n:param anonymous: Boolean\n:param title: String\n\nReturns:\n    str: A URL to the shared link"
  ],
  [
    "def gen_title(self, convo_id: str, message_id: str) -> str:\n        \"\"\"\n        Generate title for conversation\n        :param id: UUID of conversation\n        :param message_id: UUID of message\n        \"\"\"\n        response = self.session.post(\n            f\"{self.base_url}conversation/gen_title/{convo_id}\",\n            data=json.dumps(\n                {\"message_id\": message_id, \"model\": \"text-davinci-002-render\"},\n            ),\n        )\n        self.__check_response(response)\n        return response.json().get(\"title\", \"Error generating title\")",
    "Generate title for conversation\n:param id: UUID of conversation\n:param message_id: UUID of message"
  ],
  [
    "def change_title(self, convo_id: str, title: str) -> None:\n        \"\"\"\n        Change title of conversation\n        :param id: UUID of conversation\n        :param title: String\n        \"\"\"\n        url = f\"{self.base_url}conversation/{convo_id}\"\n        response = self.session.patch(url, data=json.dumps({\"title\": title}))\n        self.__check_response(response)",
    "Change title of conversation\n:param id: UUID of conversation\n:param title: String"
  ],
  [
    "def delete_conversation(self, convo_id: str) -> None:\n        \"\"\"\n        Delete conversation\n        :param id: UUID of conversation\n        \"\"\"\n        url = f\"{self.base_url}conversation/{convo_id}\"\n        response = self.session.patch(url, data='{\"is_visible\": false}')\n        self.__check_response(response)",
    "Delete conversation\n:param id: UUID of conversation"
  ],
  [
    "def reset_chat(self) -> None:\n        \"\"\"\n        Reset the conversation ID and parent ID.\n\n        :return: None\n        \"\"\"\n        self.conversation_id = None\n        self.parent_id = str(uuid.uuid4())",
    "Reset the conversation ID and parent ID.\n\n:return: None"
  ],
  [
    "def rollback_conversation(self, num: int = 1) -> None:\n        \"\"\"\n        Rollback the conversation.\n        :param num: Integer. The number of messages to rollback\n        :return: None\n        \"\"\"\n        for _ in range(num):\n            self.conversation_id = self.conversation_id_prev_queue.pop()\n            self.parent_id = self.parent_id_prev_queue.pop()",
    "Rollback the conversation.\n:param num: Integer. The number of messages to rollback\n:return: None"
  ],
  [
    "def get_plugins(\n        self,\n        offset: int = 0,\n        limit: int = 250,\n        status: str = \"approved\",\n    ) -> dict[str, str]:\n        \"\"\"\n        Get plugins\n        :param offset: Integer. Offset (Only supports 0)\n        :param limit: Integer. Limit (Only below 250)\n        :param status: String. Status of plugin (approved)\n        \"\"\"\n        url = f\"{self.base_url}aip/p?offset={offset}&limit={limit}&statuses={status}\"\n        response = self.session.get(url)\n        self.__check_response(response)\n        # Parse as JSON\n        return json.loads(response.text)",
    "Get plugins\n:param offset: Integer. Offset (Only supports 0)\n:param limit: Integer. Limit (Only below 250)\n:param status: String. Status of plugin (approved)"
  ],
  [
    "def install_plugin(self, plugin_id: str) -> None:\n        \"\"\"\n        Install plugin by ID\n        :param plugin_id: String. ID of plugin\n        \"\"\"\n        url = f\"{self.base_url}aip/p/{plugin_id}/user-settings\"\n        payload = {\"is_installed\": True}\n        response = self.session.patch(url, data=json.dumps(payload))\n        self.__check_response(response)",
    "Install plugin by ID\n:param plugin_id: String. ID of plugin"
  ],
  [
    "def get_unverified_plugin(self, domain: str, install: bool = True) -> dict:\n        \"\"\"\n        Get unverified plugin by domain\n        :param domain: String. Domain of plugin\n        :param install: Boolean. Install plugin if found\n        \"\"\"\n        url = f\"{self.base_url}aip/p/domain?domain={domain}\"\n        response = self.session.get(url)\n        self.__check_response(response)\n        if install:\n            self.install_plugin(response.json().get(\"id\"))\n        return response.json()",
    "Get unverified plugin by domain\n:param domain: String. Domain of plugin\n:param install: Boolean. Install plugin if found"
  ],
  [
    "def __init__(\n        self,\n        config: dict,\n        conversation_id: str | None = None,\n        parent_id: str | None = None,\n        base_url: str | None = None,\n        lazy_loading: bool = True,\n    ) -> None:\n        \"\"\"\n        Same as Chatbot class, but with async methods.\n        \"\"\"\n        super().__init__(\n            config=config,\n            conversation_id=conversation_id,\n            parent_id=parent_id,\n            base_url=base_url,\n            lazy_loading=lazy_loading,\n        )\n\n        # overwrite inherited normal session with async\n        self.session = AsyncClient(headers=self.session.headers)",
    "Same as Chatbot class, but with async methods."
  ],
  [
    "async def post_messages(\n        self,\n        messages: list[dict],\n        conversation_id: str | None = None,\n        parent_id: str | None = None,\n        plugin_ids: list = None,\n        model: str | None = None,\n        auto_continue: bool = False,\n        timeout: float = 360,\n        **kwargs,\n    ) -> AsyncGenerator[dict, None]:\n        \"\"\"Post messages to the chatbot\n\n        Args:\n            messages (list[dict]): the messages to post\n            conversation_id (str | None, optional): UUID for the conversation to continue on. Defaults to None.\n            parent_id (str | None, optional): UUID for the message to continue on. Defaults to None.\n            model (str | None, optional): The model to use. Defaults to None.\n            auto_continue (bool, optional): Whether to continue the conversation automatically. Defaults to False.\n            timeout (float, optional): Timeout for getting the full response, unit is second. Defaults to 360.\n\n        Yields:\n            AsyncGenerator[dict, None]: The response from the chatbot\n            {\n                \"message\": str,\n                \"conversation_id\": str,\n                \"parent_id\": str,\n                \"model\": str,\n                \"finish_details\": str,\n                \"end_turn\": bool,\n                \"recipient\": str,\n                \"citations\": list[dict],\n            }\n        \"\"\"\n        if plugin_ids is None:\n            plugin_ids = []\n        if parent_id and not conversation_id:\n            raise t.Error(\n                source=\"User\",\n                message=\"conversation_id must be set once parent_id is set\",\n                code=t.ErrorType.USER_ERROR,\n            )\n\n        if conversation_id and conversation_id != self.conversation_id:\n            self.parent_id = None\n        conversation_id = conversation_id or self.conversation_id\n        parent_id = parent_id or self.parent_id or \"\"\n        if not conversation_id and not parent_id:\n            parent_id = str(uuid.uuid4())\n\n        if conversation_id and not parent_id:\n            if conversation_id not in self.conversation_mapping:\n                if self.lazy_loading:\n                    log.debug(\n                        \"Conversation ID %s not found in conversation mapping, try to get conversation history for the given ID\",\n                        conversation_id,\n                    )\n                    try:\n                        history = await self.get_msg_history(conversation_id)\n                        self.conversation_mapping[conversation_id] = history[\n                            \"current_node\"\n                        ]\n                    except requests.exceptions.HTTPError:\n                        print(\"Conversation unavailable\")\n                else:\n                    await self.__map_conversations()\n            if conversation_id in self.conversation_mapping:\n                parent_id = self.conversation_mapping[conversation_id]\n            else:\n                print(\n                    \"Warning: Invalid conversation_id provided, treat as a new conversation\",\n                )\n                conversation_id = None\n                parent_id = str(uuid.uuid4())\n        model = model or self.config.get(\"model\") or \"text-davinci-002-render-sha\"\n        data = {\n            \"action\": \"next\",\n            \"messages\": messages,\n            \"conversation_id\": conversation_id,\n            \"parent_message_id\": parent_id,\n            \"model\": model,\n            \"history_and_training_disabled\": self.disable_history,\n        }\n        plugin_ids = self.config.get(\"plugin_ids\", []) or plugin_ids\n        if len(plugin_ids) > 0 and not conversation_id:\n            data[\"plugin_ids\"] = plugin_ids\n        async for msg in self.__send_request(\n            data,\n            timeout=timeout,\n            auto_continue=auto_continue,\n        ):\n            yield msg",
    "Post messages to the chatbot\n\nArgs:\n    messages (list[dict]): the messages to post\n    conversation_id (str | None, optional): UUID for the conversation to continue on. Defaults to None.\n    parent_id (str | None, optional): UUID for the message to continue on. Defaults to None.\n    model (str | None, optional): The model to use. Defaults to None.\n    auto_continue (bool, optional): Whether to continue the conversation automatically. Defaults to False.\n    timeout (float, optional): Timeout for getting the full response, unit is second. Defaults to 360.\n\nYields:\n    AsyncGenerator[dict, None]: The response from the chatbot\n    {\n        \"message\": str,\n        \"conversation_id\": str,\n        \"parent_id\": str,\n        \"model\": str,\n        \"finish_details\": str,\n        \"end_turn\": bool,\n        \"recipient\": str,\n        \"citations\": list[dict],\n    }"
  ],
  [
    "async def ask(\n        self,\n        prompt: str,\n        conversation_id: str | None = None,\n        parent_id: str = \"\",\n        model: str = \"\",\n        plugin_ids: list = None,\n        auto_continue: bool = False,\n        timeout: int = 360,\n        **kwargs,\n    ) -> AsyncGenerator[dict, None]:\n        \"\"\"Ask a question to the chatbot\n\n        Args:\n            prompt (str): The question to ask\n            conversation_id (str | None, optional): UUID for the conversation to continue on. Defaults to None.\n            parent_id (str, optional): UUID for the message to continue on. Defaults to \"\".\n            model (str, optional): The model to use. Defaults to \"\".\n            auto_continue (bool, optional): Whether to continue the conversation automatically. Defaults to False.\n            timeout (float, optional): Timeout for getting the full response, unit is second. Defaults to 360.\n\n        Yields:\n            AsyncGenerator[dict, None]: The response from the chatbot\n            {\n                \"message\": str,\n                \"conversation_id\": str,\n                \"parent_id\": str,\n                \"model\": str,\n                \"finish_details\": str,\n                \"end_turn\": bool,\n                \"recipient\": str,\n            }\n        \"\"\"\n\n        if plugin_ids is None:\n            plugin_ids = []\n        messages = [\n            {\n                \"id\": str(uuid.uuid4()),\n                \"author\": {\"role\": \"user\"},\n                \"content\": {\"content_type\": \"text\", \"parts\": [prompt]},\n            },\n        ]\n\n        async for msg in self.post_messages(\n            messages=messages,\n            conversation_id=conversation_id,\n            parent_id=parent_id,\n            plugin_ids=plugin_ids,\n            model=model,\n            auto_continue=auto_continue,\n            timeout=timeout,\n        ):\n            yield msg",
    "Ask a question to the chatbot\n\nArgs:\n    prompt (str): The question to ask\n    conversation_id (str | None, optional): UUID for the conversation to continue on. Defaults to None.\n    parent_id (str, optional): UUID for the message to continue on. Defaults to \"\".\n    model (str, optional): The model to use. Defaults to \"\".\n    auto_continue (bool, optional): Whether to continue the conversation automatically. Defaults to False.\n    timeout (float, optional): Timeout for getting the full response, unit is second. Defaults to 360.\n\nYields:\n    AsyncGenerator[dict, None]: The response from the chatbot\n    {\n        \"message\": str,\n        \"conversation_id\": str,\n        \"parent_id\": str,\n        \"model\": str,\n        \"finish_details\": str,\n        \"end_turn\": bool,\n        \"recipient\": str,\n    }"
  ],
  [
    "async def continue_write(\n        self,\n        conversation_id: str | None = None,\n        parent_id: str = \"\",\n        model: str = \"\",\n        auto_continue: bool = False,\n        timeout: float = 360,\n    ) -> AsyncGenerator[dict, None]:\n        \"\"\"let the chatbot continue to write\n        Args:\n            conversation_id (str | None, optional): UUID for the conversation to continue on. Defaults to None.\n            parent_id (str, optional): UUID for the message to continue on. Defaults to None.\n            model (str, optional): Model to use. Defaults to None.\n            auto_continue (bool, optional): Whether to continue writing automatically. Defaults to False.\n            timeout (float, optional): Timeout for getting the full response, unit is second. Defaults to 360.\n\n\n        Yields:\n            AsyncGenerator[dict, None]: The response from the chatbot\n            {\n                \"message\": str,\n                \"conversation_id\": str,\n                \"parent_id\": str,\n                \"model\": str,\n                \"finish_details\": str,\n                \"end_turn\": bool,\n                \"recipient\": str,\n            }\n        \"\"\"\n        if parent_id and not conversation_id:\n            error = t.Error(\n                source=\"User\",\n                message=\"conversation_id must be set once parent_id is set\",\n                code=t.ErrorType.SERVER_ERROR,\n            )\n            raise error\n        if conversation_id and conversation_id != self.conversation_id:\n            self.parent_id = None\n        conversation_id = conversation_id or self.conversation_id\n\n        parent_id = parent_id or self.parent_id or \"\"\n        if not conversation_id and not parent_id:\n            parent_id = str(uuid.uuid4())\n        if conversation_id and not parent_id:\n            if conversation_id not in self.conversation_mapping:\n                await self.__map_conversations()\n            if conversation_id in self.conversation_mapping:\n                parent_id = self.conversation_mapping[conversation_id]\n            else:  # invalid conversation_id provided, treat as a new conversation\n                conversation_id = None\n                parent_id = str(uuid.uuid4())\n        model = model or self.config.get(\"model\") or \"text-davinci-002-render-sha\"\n        data = {\n            \"action\": \"continue\",\n            \"conversation_id\": conversation_id,\n            \"parent_message_id\": parent_id,\n            \"model\": model\n            or self.config.get(\"model\")\n            or (\n                \"text-davinci-002-render-paid\"\n                if self.config.get(\"paid\")\n                else \"text-davinci-002-render-sha\"\n            ),\n            \"history_and_training_disabled\": self.disable_history,\n        }\n        async for msg in self.__send_request(\n            data=data,\n            auto_continue=auto_continue,\n            timeout=timeout,\n        ):\n            yield msg",
    "let the chatbot continue to write\nArgs:\n    conversation_id (str | None, optional): UUID for the conversation to continue on. Defaults to None.\n    parent_id (str, optional): UUID for the message to continue on. Defaults to None.\n    model (str, optional): Model to use. Defaults to None.\n    auto_continue (bool, optional): Whether to continue writing automatically. Defaults to False.\n    timeout (float, optional): Timeout for getting the full response, unit is second. Defaults to 360.\n\n\nYields:\n    AsyncGenerator[dict, None]: The response from the chatbot\n    {\n        \"message\": str,\n        \"conversation_id\": str,\n        \"parent_id\": str,\n        \"model\": str,\n        \"finish_details\": str,\n        \"end_turn\": bool,\n        \"recipient\": str,\n    }"
  ],
  [
    "async def get_conversations(self, offset: int = 0, limit: int = 20) -> list:\n        \"\"\"\n        Get conversations\n        :param offset: Integer\n        :param limit: Integer\n        \"\"\"\n        url = f\"{self.base_url}conversations?offset={offset}&limit={limit}\"\n        response = await self.session.get(url)\n        await self.__check_response(response)\n        data = json.loads(response.text)\n        return data[\"items\"]",
    "Get conversations\n:param offset: Integer\n:param limit: Integer"
  ],
  [
    "async def get_msg_history(\n        self,\n        convo_id: str,\n        encoding: str | None = \"utf-8\",\n    ) -> dict:\n        \"\"\"\n        Get message history\n        :param id: UUID of conversation\n        \"\"\"\n        url = f\"{self.base_url}conversation/{convo_id}\"\n        response = await self.session.get(url)\n        if encoding is not None:\n            response.encoding = encoding\n            await self.__check_response(response)\n            return json.loads(response.text)\n        return None",
    "Get message history\n:param id: UUID of conversation"
  ],
  [
    "async def share_conversation(\n        self,\n        title: str = None,\n        convo_id: str = None,\n        node_id: str = None,\n        anonymous: bool = True,\n    ) -> str:\n        \"\"\"\n        Creates a share link to a conversation\n        :param convo_id: UUID of conversation\n        :param node_id: UUID of node\n\n        Returns:\n            str: A URL to the shared link\n        \"\"\"\n        convo_id = convo_id or self.conversation_id\n        node_id = node_id or self.parent_id\n        # First create the share\n        payload = {\n            \"conversation_id\": convo_id,\n            \"current_node_id\": node_id,\n            \"is_anonymous\": anonymous,\n        }\n        url = f\"{self.base_url}share/create\"\n        response = await self.session.post(\n            url,\n            data=json.dumps(payload),\n        )\n        await self.__check_response(response)\n        share_url = response.json().get(\"share_url\")\n        # Then patch the share to make public\n        share_id = response.json().get(\"share_id\")\n        url = f\"{self.base_url}share/{share_id}\"\n        print(url)\n        payload = {\n            \"share_id\": share_id,\n            \"highlighted_message_id\": node_id,\n            \"title\": title or response.json().get(\"title\", \"New chat\"),\n            \"is_public\": True,\n            \"is_visible\": True,\n            \"is_anonymous\": True,\n        }\n        response = await self.session.patch(\n            url,\n            data=json.dumps(payload),\n        )\n        await self.__check_response(response)\n        return share_url",
    "Creates a share link to a conversation\n:param convo_id: UUID of conversation\n:param node_id: UUID of node\n\nReturns:\n    str: A URL to the shared link"
  ],
  [
    "async def change_title(self, convo_id: str, title: str) -> None:\n        \"\"\"\n        Change title of conversation\n        :param convo_id: UUID of conversation\n        :param title: String\n        \"\"\"\n        url = f\"{self.base_url}conversation/{convo_id}\"\n        response = await self.session.patch(url, data=f'{{\"title\": \"{title}\"}}')\n        await self.__check_response(response)",
    "Change title of conversation\n:param convo_id: UUID of conversation\n:param title: String"
  ],
  [
    "async def delete_conversation(self, convo_id: str) -> None:\n        \"\"\"\n        Delete conversation\n        :param convo_id: UUID of conversation\n        \"\"\"\n        url = f\"{self.base_url}conversation/{convo_id}\"\n        response = await self.session.patch(url, data='{\"is_visible\": false}')\n        await self.__check_response(response)",
    "Delete conversation\n:param convo_id: UUID of conversation"
  ],
  [
    "def __init__(\n        self,\n        api_key: str,\n        engine: str = os.environ.get(\"GPT_ENGINE\") or \"gpt-3.5-turbo\",\n        proxy: str = None,\n        timeout: float = None,\n        max_tokens: int = None,\n        temperature: float = 0.5,\n        top_p: float = 1.0,\n        presence_penalty: float = 0.0,\n        frequency_penalty: float = 0.0,\n        reply_count: int = 1,\n        truncate_limit: int = None,\n        system_prompt: str = \"You are ChatGPT, a large language model trained by OpenAI. Respond conversationally\",\n    ) -> None:\n        \"\"\"\n        Initialize Chatbot with API key (from https://platform.openai.com/account/api-keys)\n        \"\"\"\n        self.engine: str = engine\n        self.api_key: str = api_key\n        self.system_prompt: str = system_prompt\n        self.max_tokens: int = max_tokens or (\n            31000\n            if \"gpt-4-32k\" in engine\n            else 7000\n            if \"gpt-4\" in engine\n            else 15000\n            if \"gpt-3.5-turbo-16k\" in engine\n            else 4000\n        )\n        self.truncate_limit: int = truncate_limit or (\n            30500\n            if \"gpt-4-32k\" in engine\n            else 6500\n            if \"gpt-4\" in engine\n            else 14500\n            if \"gpt-3.5-turbo-16k\" in engine\n            else 3500\n        )\n        self.temperature: float = temperature\n        self.top_p: float = top_p\n        self.presence_penalty: float = presence_penalty\n        self.frequency_penalty: float = frequency_penalty\n        self.reply_count: int = reply_count\n        self.timeout: float = timeout\n        self.proxy = proxy\n        self.session = requests.Session()\n        self.session.proxies.update(\n            {\n                \"http\": proxy,\n                \"https\": proxy,\n            },\n        )\n        if proxy := (\n            proxy or os.environ.get(\"all_proxy\") or os.environ.get(\"ALL_PROXY\") or None\n        ):\n            if \"socks5h\" not in proxy:\n                self.aclient = httpx.AsyncClient(\n                    follow_redirects=True,\n                    proxies=proxy,\n                    timeout=timeout,\n                )\n        else:\n            self.aclient = httpx.AsyncClient(\n                follow_redirects=True,\n                proxies=proxy,\n                timeout=timeout,\n            )\n\n        self.conversation: dict[str, list[dict]] = {\n            \"default\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": system_prompt,\n                },\n            ],\n        }\n\n        if self.get_token_count(\"default\") > self.max_tokens:\n            raise t.ActionRefuseError(\"System prompt is too long\")",
    "Initialize Chatbot with API key (from https://platform.openai.com/account/api-keys)"
  ],
  [
    "def add_to_conversation(\n        self,\n        message: str,\n        role: str,\n        convo_id: str = \"default\",\n    ) -> None:\n        \"\"\"\n        Add a message to the conversation\n        \"\"\"\n        self.conversation[convo_id].append({\"role\": role, \"content\": message})",
    "Add a message to the conversation"
  ],
  [
    "def save(self, file: str, *keys: str) -> None:\n        \"\"\"\n        Save the Chatbot configuration to a JSON file\n        \"\"\"\n        with open(file, \"w\", encoding=\"utf-8\") as f:\n            data = {\n                key: self.__dict__[key]\n                for key in get_filtered_keys_from_object(self, *keys)\n            }\n            # saves session.proxies dict as session\n            # leave this here for compatibility\n            data[\"session\"] = data[\"proxy\"]\n            del data[\"aclient\"]\n            json.dump(\n                data,\n                f,\n                indent=2,\n            )",
    "Save the Chatbot configuration to a JSON file"
  ],
  [
    "def load(self, file: Path, *keys_: str) -> None:\n        \"\"\"\n        Load the Chatbot configuration from a JSON file\n        \"\"\"\n        with open(file, encoding=\"utf-8\") as f:\n            # load json, if session is in keys, load proxies\n            loaded_config = json.load(f)\n            keys = get_filtered_keys_from_object(self, *keys_)\n\n            if (\n                \"session\" in keys\n                and loaded_config[\"session\"]\n                or \"proxy\" in keys\n                and loaded_config[\"proxy\"]\n            ):\n                self.proxy = loaded_config.get(\"session\", loaded_config[\"proxy\"])\n                self.session = httpx.Client(\n                    follow_redirects=True,\n                    proxies=self.proxy,\n                    timeout=self.timeout,\n                    cookies=self.session.cookies,\n                    headers=self.session.headers,\n                )\n                self.aclient = httpx.AsyncClient(\n                    follow_redirects=True,\n                    proxies=self.proxy,\n                    timeout=self.timeout,\n                    cookies=self.session.cookies,\n                    headers=self.session.headers,\n                )\n            if \"session\" in keys:\n                keys.remove(\"session\")\n            if \"aclient\" in keys:\n                keys.remove(\"aclient\")\n            self.__dict__.update({key: loaded_config[key] for key in keys})",
    "Load the Chatbot configuration from a JSON file"
  ],
  [
    "class ChatbotError(Exception):\n    \"\"\"\n    Base class for all Chatbot errors in this Project\n    \"\"\"\n\n    def __init__(self, *args: object) -> None:\n        if SUPPORT_ADD_NOTES:\n            super().add_note(\n                \"Please check that the input is correct, or you can resolve this issue by filing an issue\",\n            )\n            super().add_note(\"Project URL: https://github.com/acheong08/ChatGPT\")\n        super().__init__(*args)",
    "Base class for all Chatbot errors in this Project"
  ],
  [
    "class ActionError(ChatbotError):\n    \"\"\"\n    Subclass of ChatbotError\n\n    An object that throws an error because the execution of an operation is blocked\n    \"\"\"\n\n    def __init__(self, *args: object) -> None:\n        if SUPPORT_ADD_NOTES:\n            super().add_note(\n                \"The current operation is not allowed, which may be intentional\",\n            )\n        super().__init__(*args)",
    "Subclass of ChatbotError\n\nAn object that throws an error because the execution of an operation is blocked"
  ],
  [
    "class ActionNotAllowedError(ActionError):\n    \"\"\"\n    Subclass of ActionError\n\n    An object that throws an error because the execution of an unalloyed operation is blocked\n    \"\"\"",
    "Subclass of ActionError\n\nAn object that throws an error because the execution of an unalloyed operation is blocked"
  ],
  [
    "class ActionRefuseError(ActionError):\n    \"\"\"\n    Subclass of ActionError\n\n    An object that throws an error because the execution of a refused operation is blocked.\n    \"\"\"",
    "Subclass of ActionError\n\nAn object that throws an error because the execution of a refused operation is blocked."
  ],
  [
    "class CLIError(ChatbotError):\n    \"\"\"\n    Subclass of ChatbotError\n\n    The error caused by a CLI program error\n    \"\"\"",
    "Subclass of ChatbotError\n\nThe error caused by a CLI program error"
  ],
  [
    "class ErrorType(Enum):\n    \"\"\"\n    Enumeration class for different types of errors.\n    \"\"\"\n\n    USER_ERROR = -1\n    UNKNOWN_ERROR = 0\n    SERVER_ERROR = 1\n    RATE_LIMIT_ERROR = 2\n    INVALID_REQUEST_ERROR = 3\n    EXPIRED_ACCESS_TOKEN_ERROR = 4\n    INVALID_ACCESS_TOKEN_ERROR = 5\n    PROHIBITED_CONCURRENT_QUERY_ERROR = 6\n    AUTHENTICATION_ERROR = 7\n    CLOUDFLARE_ERROR = 8",
    "Enumeration class for different types of errors."
  ],
  [
    "class Error(ChatbotError):\n    \"\"\"\n    Base class for exceptions in V1 module.\n    \"\"\"\n\n    def __init__(\n        self,\n        source: str,\n        message: str,\n        *args: object,\n        code: Union[ErrorType, int] = ErrorType.UNKNOWN_ERROR,\n    ) -> None:\n        self.source: str = source\n        self.message: str = message\n        self.code: ErrorType | int = code\n        super().__init__(*args)\n\n    def __str__(self) -> str:\n        return f\"{self.source}: {self.message} (code: {self.code})\"\n\n    def __repr__(self) -> str:\n        return f\"{self.source}: {self.message} (code: {self.code})\"",
    "Base class for exceptions in V1 module."
  ],
  [
    "class AuthenticationError(ChatbotError):\n    \"\"\"\n    Subclass of ChatbotError\n\n    The object of the error thrown by a validation failure or exception\n    \"\"\"\n\n    def __init__(self, *args: object) -> None:\n        if SUPPORT_ADD_NOTES:\n            super().add_note(\n                \"Please check if your key is correct, maybe it may not be valid\",\n            )\n        super().__init__(*args)",
    "Subclass of ChatbotError\n\nThe object of the error thrown by a validation failure or exception"
  ],
  [
    "class APIConnectionError(ChatbotError):\n    \"\"\"\n    Subclass of ChatbotError\n\n    An exception object thrown when an API connection fails or fails to connect due to network or\n    other miscellaneous reasons\n    \"\"\"\n\n    def __init__(self, *args: object) -> None:\n        if SUPPORT_ADD_NOTES:\n            super().add_note(\n                \"Please check if there is a problem with your network connection\",\n            )\n        super().__init__(*args)",
    "Subclass of ChatbotError\n\nAn exception object thrown when an API connection fails or fails to connect due to network or\nother miscellaneous reasons"
  ],
  [
    "class NotAllowRunning(ActionNotAllowedError):\n    \"\"\"\n    Subclass of ActionNotAllowedError\n\n    Direct startup is not allowed for some reason\n    \"\"\"",
    "Subclass of ActionNotAllowedError\n\nDirect startup is not allowed for some reason"
  ],
  [
    "class ResponseError(APIConnectionError):\n    \"\"\"\n    Subclass of APIConnectionError\n\n    Error objects caused by API request errors due to network or other miscellaneous reasons\n    \"\"\"",
    "Subclass of APIConnectionError\n\nError objects caused by API request errors due to network or other miscellaneous reasons"
  ],
  [
    "class OpenAIError(APIConnectionError):\n    \"\"\"\n    Subclass of APIConnectionError\n\n    Error objects caused by OpenAI's own server errors\n    \"\"\"",
    "Subclass of APIConnectionError\n\nError objects caused by OpenAI's own server errors"
  ],
  [
    "class RequestError(APIConnectionError):\n    \"\"\"\n    Subclass of APIConnectionError\n\n    There is a problem with the API response due to network or other miscellaneous reasons, or there\n    is no reply to the object that caused the error at all\n    \"\"\"",
    "Subclass of APIConnectionError\n\nThere is a problem with the API response due to network or other miscellaneous reasons, or there\nis no reply to the object that caused the error at all"
  ],
  [
    "def create_keybindings(key: str = \"c-@\") -> KeyBindings:\n    \"\"\"\n    Create keybindings for prompt_toolkit. Default key is ctrl+space.\n    For possible keybindings, see: https://python-prompt-toolkit.readthedocs.io/en/stable/pages/advanced_topics/key_bindings.html#list-of-special-keys\n    \"\"\"\n\n    @bindings.add(key)\n    def _(event: dict) -> None:\n        event.app.exit(result=event.app.current_buffer.text)\n\n    return bindings",
    "Create keybindings for prompt_toolkit. Default key is ctrl+space.\nFor possible keybindings, see: https://python-prompt-toolkit.readthedocs.io/en/stable/pages/advanced_topics/key_bindings.html#list-of-special-keys"
  ],
  [
    "def get_filtered_keys_from_object(obj: object, *keys: str) -> Set[str]:\n    \"\"\"\n    Get filtered list of object variable names.\n    :param keys: List of keys to include. If the first key is \"not\", the remaining keys will be removed from the class keys.\n    :return: List of class keys.\n    \"\"\"\n    class_keys = obj.__dict__.keys()\n    if not keys:\n        return set(class_keys)\n\n    # Remove the passed keys from the class keys.\n    if keys[0] == \"not\":\n        return {key for key in class_keys if key not in keys[1:]}\n    # Check if all passed keys are valid\n    if invalid_keys := set(keys) - class_keys:\n        raise ValueError(\n            f\"Invalid keys: {invalid_keys}\",\n        )\n    # Only return specified keys that are in class_keys\n    return {key for key in keys if key in class_keys}",
    "Get filtered list of object variable names.\n:param keys: List of keys to include. If the first key is \"not\", the remaining keys will be removed from the class keys.\n:return: List of class keys."
  ],
  [
    "def main() -> None:\n    \n    stream = client.images.generate(\n        model=\"gpt-image-1\",\n        prompt=\"A cute baby sea otter\",\n        n=1,\n        size=\"1024x1024\",\n        stream=True,\n        partial_images=3,\n    )\n\n    for event in stream:\n        if event.type == \"image_generation.partial_image\":\n            print(f\"  Partial image {event.partial_image_index + 1}/3 received\")\n            print(f\"   Size: {len(event.b64_json)} characters (base64)\")\n\n            # Save partial image to file\n            filename = f\"partial_{event.partial_image_index + 1}.png\"\n            image_data = base64.b64decode(event.b64_json)\n            with open(filename, \"wb\") as f:\n                f.write(image_data)\n            print(f\"    Saved to: {Path(filename).resolve()}\")\n\n        elif event.type == \"image_generation.completed\":\n            print(f\"\\n Final image completed!\")\n            print(f\"   Size: {len(event.b64_json)} characters (base64)\")\n\n            # Save final image to file\n            filename = \"final_image.png\"\n            image_data = base64.b64decode(event.b64_json)\n            with open(filename, \"wb\") as f:\n                f.write(image_data)\n            print(f\"    Saved to: {Path(filename).resolve()}\")\n\n        else:\n            print(f\" Unknown event: {event}\")  # type: ignore[unreachable]",
    "Example of OpenAI image streaming with partial images."
  ],
  [
    "async def main() -> None:\n    \"\"\"The following example demonstrates how to configure Azure OpenAI to use the Realtime API.\n    For an audio example, see push_to_talk_app.py and update the client and model parameter accordingly.\n\n    When prompted for user input, type a message and hit enter to send it to the model.\n    Enter \"q\" to quit the conversation.\n    \"\"\"\n\n    credential = DefaultAzureCredential()\n    client = AsyncAzureOpenAI(\n        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n        azure_ad_token_provider=get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\"),\n        api_version=\"2024-10-01-preview\",\n    )\n    async with client.beta.realtime.connect(\n        model=\"gpt-4o-realtime-preview\",  # deployment name for your model\n    ) as connection:\n        await connection.session.update(session={\"modalities\": [\"text\"]})  # type: ignore\n        while True:\n            user_input = input(\"Enter a message: \")\n            if user_input == \"q\":\n                break\n\n            await connection.conversation.item.create(\n                item={\n                    \"type\": \"message\",\n                    \"role\": \"user\",\n                    \"content\": [{\"type\": \"input_text\", \"text\": user_input}],\n                }\n            )\n            await connection.response.create()\n            async for event in connection:\n                if event.type == \"response.text.delta\":\n                    print(event.delta, flush=True, end=\"\")\n                elif event.type == \"response.text.done\":\n                    print()\n                elif event.type == \"response.done\":\n                    break\n\n    await credential.close()",
    "The following example demonstrates how to configure Azure OpenAI to use the Realtime API.\nFor an audio example, see push_to_talk_app.py and update the client and model parameter accordingly.\n\nWhen prompted for user input, type a message and hit enter to send it to the model.\nEnter \"q\" to quit the conversation."
  ],
  [
    "class SessionDisplay(Static):\n    \n\n    session_id = reactive(\"\")\n\n    @override\n    def render(self) -> str:\n        return f\"Session ID: {self.session_id}\" if self.session_id else \"Connecting...\"",
    "A widget that shows the current session ID."
  ],
  [
    "class AudioStatusIndicator(Static):\n    \n\n    is_recording = reactive(False)\n\n    @override\n    def render(self) -> str:\n        status = (\n            \" Recording... (Press K to stop)\" if self.is_recording else \" Press K to start recording (Q to quit)\"\n        )\n        return status",
    "A widget that shows the current audio recording status."
  ],
  [
    "def compose(self) -> ComposeResult:\n        \n        with Container():\n            yield SessionDisplay(id=\"session-display\")\n            yield AudioStatusIndicator(id=\"status-indicator\")\n            yield RichLog(id=\"bottom-pane\", wrap=True, highlight=True, markup=True)",
    "Create child widgets for the app."
  ],
  [
    "function withHeadingContext(relativeHeadingLevel, markdown) {\n  return markdown.replaceAll(/^(#+)/gm, (match, markdownHeadingTokens) => {\n    return \"#\".repeat(markdownHeadingTokens.length + relativeHeadingLevel);\n  }",
    "* Adjusts the headings in the given `markdown` to be in a given heading context.\n * Headings must start in a line.\n * Preceding whitespace or any other character will result in the heading not being recognized.\n *\n * @example `withHeadingContext(2, '# Heading') === '### Heading'`\n * @param {number} relativeHeadingLevel\n * @param {string} markdown"
  ],
  [
    "function request( url ) {\n\tconsole.log(\n\t\t'Requesting URL \"%s\"',\n\t\turl.length > 70\n\t\t\t? url.substring( 0, 67 ) + '...'\n\t\t\t: url\n\t);\n\tconst res = await phin.promisified( url );\n\tif ( res.statusCode !== 200 ) {\n\t\tthrow new Error(\n\t\t\t'HTTP response code ' + res.statusCode\n\t\t\t+ ' for URL: ' + url\n\t\t);\n\t}",
    "* Perform an HTTP request to a URL and return the request body."
  ],
  [
    "function copyAssetToBuild( filename, content = null, addSuffix = true ) {\n\tlet destFilename = filename;\n\tif ( addSuffix ) {\n\t\tdestFilename = destFilename\n\t\t\t.replace( /(\\.[^.]+)$/, '-' + assetCacheBuster + '$1' );\n\t}",
    "* Write a file to site/build/assets/ (from memory or from an existing file in\n * site/assets/) and include a cache buster in the new name.  Return the URL to\n * the asset file."
  ],
  [
    "function githubEditUrl( filename ) {\n\treturn (\n\t\t'https://github.com/remoteintech/remote-jobs/edit/main/'\n\t\t+ filename\n\t);\n}",
    "* Return a URL to edit a page on GitHub."
  ],
  [
    "function writePage( filename, pageContent ) {\n\tfilename = path.join( siteBuildPath, filename );\n\tif ( ! fs.existsSync( path.dirname( filename ) ) ) {\n\t\tfs.mkdirSync( path.dirname( filename ) );\n\t}",
    "* Write a page's contents to an HTML file."
  ],
  [
    "function buildSite() {\n\t// Load the HTML from the WP.com blog site\n\tconst $ = cheerio.load( await request( 'https://blog.remoteintech.company/' ) );\n\n\t// Load stylesheets from the WP.com blog site\n\tconst wpcomStylesheets = $( 'style, link[rel=stylesheet]' ).map( ( i, el ) => {\n\t\tconst $el = $( el );\n\t\tconst stylesheet = {\n\t\t\tid: $el.attr( 'id' ) || null,\n\t\t\tmedia: $el.attr( 'media' ) || null,\n\t\t}",
    "* The main function that prepares the static site."
  ],
  [
    "function error( filename, msg, ...params ) {\n\t\terrors.push( {\n\t\t\tfilename,\n\t\t\tmessage: util.format( msg, ...params ),\n\t\t}",
    "* The main exported function\n *\n * Start with a directory including a README.md and company-profiles/*.md\n * files, and validate and parse the content of the Markdown files."
  ],
  [
    "function getUrlInfo(url) {\n\t    const data = {}",
    "* Getting info about the url. It includes checking isEmail of isInternal\n\t * @param {*} url"
  ],
  [
    "function extractMainDomainFromUrl(url) {\n\t    try {\n\t        const domainRe = /(https?:\\/\\/)?(([\\w\\d-]+\\.)+[\\w\\d]{2,}",
    "* Extracting main domain from the url\n\t * @param {*} url"
  ],
  [
    "function stringToArgs(string) {\n  var args = [];\n\n  var parts = string.split(' ');\n  var length = parts.length;\n  var i = 0;\n  var open = false;\n  var grouped = '';\n  var lead = '';\n\n  for (; i < length; i++) {\n    lead = parts[i].substring(0, 1);\n    if (lead === '\"' || lead === '\\'') {\n      open = lead;\n      grouped = parts[i].substring(1);\n    }",
    "* Converts a string to command line args, in particular\n * groups together quoted values.\n * This is a utility function to allow calling nodemon as a required\n * library, but with the CLI args passed in (instead of an object).\n *\n * @param  {String} string\n * @return {Array}"
  ],
  [
    "function parse(argv) {\n  if (typeof argv === 'string') {\n    argv = argv.split(' ');\n  }",
    "* Parses the command line arguments `process.argv` and returns the\n * nodemon options, the user script and the executable script.\n *\n * @param  {Array<string> | string} argv full process arguments, including `node` leading arg\n * @return {Object} { options, script, args }"
  ],
  [
    "function nodemonOption(options, arg, eatNext) {\n  // line separation on purpose to help legibility\n  if (arg === '--help' || arg === '-h' || arg === '-?') {\n    var help = eatNext();\n    options.help = help ? help : true;\n  }",
    "* Given an argument (ie. from process.argv), sets nodemon\n * options and can eat up the argument value\n *\n * @param {import('../..').NodemonSettings} options object that will be updated\n * @param {String} arg current argument from argv\n * @param {Function} eatNext the callback to eat up the next argument in argv\n * @return {Boolean} false if argument was not a nodemon arg"
  ],
  [
    "function parseDelay(value) {\n  var millisPerSecond = 1000;\n  var millis = 0;\n\n  if (value.match(/^\\d*ms$/)) {\n    // Explicitly parse for milliseconds when using ms time specifier\n    millis = parseInt(value, 10);\n  }",
    "* Given an argument (ie. from nodemonOption()), will parse and return the\n * equivalent millisecond value or 0 if the argument cannot be parsed\n *\n * @param {String} value argument value given to the --delay option\n * @return {Number} millisecond equivalent of the argument"
  ],
  [
    "function command(settings) {\n  var options = settings.execOptions;\n  var executable = options.exec;\n  var args = [];\n\n  // after \"executable\" go the exec args (like --debug, etc)\n  if (options.execArgs) {\n    [].push.apply(args, options.execArgs);\n  }",
    "* command constructs the executable command to run in a shell including the\n * user script, the command arguments.\n *\n * @param  {Object} settings Object as:\n *                           { execOptions: {\n *                               exec: String,\n *                               [script: String],\n *                               [scriptPosition: Number],\n *                               [execArgs: Array<string>]\n *                             }\n *                           }\n * @return {Object}          an object with the node executable and the\n *                           arguments to the command"
  ],
  [
    "function execFromPackage() {\n  // doing a try/catch because we can't use the path.exist callback pattern\n  // or we could, but the code would get messy, so this will do exactly\n  // what we're after - if the file doesn't exist, it'll throw.\n  try {\n    // note: this isn't nodemon's package, it's the user's cwd package\n    var pkg = require(path.join(process.cwd(), 'package.json'));\n    if (pkg.main !== undefined) {\n      // no app found to run - so give them a tip and get the feck out\n      return { exec: null, script: pkg.main }",
    "* Reads the cwd/package.json file and looks to see if it can load a script\n * and possibly an exec first from package.main, then package.start.\n *\n * @return {Object} exec & script if found"
  ],
  [
    "function exec(nodemonOptions, execMap) {\n  if (!execMap) {\n    execMap = {}",
    "* Discovers all the options required to run the script\n * and if a custom exec has been passed in, then it will\n * also try to work out what extensions to monitor and\n * whether there's a special way of running that script.\n *\n * @param  {Object} nodemonOptions\n * @param  {Object} execMap\n * @return {Object} new and updated version of nodemonOptions"
  ],
  [
    "function load(settings, options, config, callback) {\n  config.loaded = [];\n  // first load the root nodemon.json\n  loadFile(options, config, utils.home, function (options) {\n    // then load the user's local configuration file\n    if (settings.configFile) {\n      options.configFile = path.resolve(settings.configFile);\n    }",
    "* Load the nodemon config, first reading the global root/nodemon.json, then\n * the local nodemon.json to the exec and then overwriting using any user\n * specified settings (i.e. from the cli)\n *\n * @param {Object} settings user defined settings\n * @param {Object} options global options\n * @param {Object} config the config object to be updated\n * @param {Function} callback that receives complete config"
  ],
  [
    "function loadFile(options, config, dir, ready) {\n  if (!ready) {\n    ready = function () {}",
    "* Looks for a config in the current working directory, and a config in the\n * user's home directory, merging the two together, giving priority to local\n * config. This can then be overwritten later by command line arguments\n *\n * @param  {Function} ready callback to pass loaded settings to"
  ],
  [
    "function nodemon(settings) {\n  bus.emit('boot');\n  nodemon.reset();\n  \n  /** @type {import('..').NodemonSettings}",
    "* @param {import('..').NodemonSettings | string} settings\n * @returns {import('..').Nodemon}"
  ],
  [
    "function add(rules, which, rule) {\n  if (!{ ignore: 1, watch: 1}",
    "* Converts file patterns or regular expressions to nodemon\n * compatible RegExp matching rules. Note: the `rules` argument\n * object is modified to include the new rule and new RegExp\n *\n * ### Example:\n *\n *     var rules = { watch: [], ignore: [] };\n *     add(rules, 'watch', '*.js');\n *     add(rules, 'ignore', '/public/');\n *     add(rules, 'watch', ':(\\d)*\\.js'); // note: string based regexp\n *     add(rules, 'watch', /\\d*\\.js/);\n *\n * @param {Object} rules containing `watch` and `ignore`. Also updated during\n *                       execution\n * @param {String} which must be either \"watch\" or \"ignore\"\n * @param {String|RegExp} rule the actual rule."
  ],
  [
    "function load(filename, callback) {\n  parse(filename, function (err, result) {\n    if (err) {\n      // we should have bombed already, but\n      utils.log.error(err);\n      callback(err);\n    }",
    "* Loads a nodemon config file and populates the ignore\n * and watch rules with it's contents, and calls callback\n * with the new rules\n *\n * @param  {String} filename\n * @param  {Function} callback"
  ]
]