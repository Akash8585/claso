[
  {
    "code": "File: src/core/instance/render.ts (modified)\n  import { isUpdatingChildComponent } from './lifecycle'\n  import type { Component } from 'types/component'\n-import { setCurrentInstance } from 'v3/currentInstance'\n+import { currentInstance, setCurrentInstance } from 'v3/currentInstance'\n  import { syncSetupSlots } from 'v3/apiSetup'\n  export function initRender(vm: Component) {\n      // to the data on the placeholder node.\n      vm.$vnode = _parentVnode!\n      // render self\n+    const prevInst = currentInstance\n+    const prevRenderInst = currentRenderingInstance\n      let vnode\n      try {\n-      // There's no need to maintain a stack because all render fns are called\n-      // separately from one another. Nested component's render fns are called\n-      // when parent component is patched.\n        setCurrentInstance(vm)\n        currentRenderingInstance = vm\n        vnode = render.call(vm._renderProxy, vm.$createElement)\n          vnode = vm._vnode",
    "comment": "fix: account for nested render calls",
    "language": "diff",
    "repo": "vuejs/vue",
    "sha": "db9c566032da0ec5cd758a3e8525e9733874c1e5",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/server-renderer/src/render-context.ts (modified)\n    renderStates: Array<RenderState>\n    write: (text: string, next: Function) => void\n    renderNode: (node: VNode, isRoot: boolean, context: RenderContext) => void\n-  //@ts-expect-error\n-  next: () => void\n    done: (err?: Error) => void\n    modules: Array<(node: VNode) => string | null>\n      this.get = cache && normalizeAsync(cache, 'get')\n      this.has = cache && normalizeAsync(cache, 'has')\n-    //@ts-expect-error\n      this.next = this.next.bind(this)\n    }\n-  //@ts-expect-error\n    next() {\n      // eslint-disable-next-line\n      while (true) {\n\nFile: packages/server-renderer/test/ssr-basic-renderer.spec.ts (modified)\n  import Vue from 'vue'\n  import renderToString from 'server/index-basic'\n+import { _it } from './utils'\n  describe('SSR: basicRenderer', () => {\n-  it('should work', done => {\n+  _it('should work', done => {\n      renderToString(\n        new Vue({\n          template: `\n    })\n    // #5941\n-  it('should work properly when accessing $ssrContext in root component', done => {\n+  _it('should work properly when accessing $ssrContext in root component', done => {\n      let ssrContext\n      renderToString(\n        new Vue({\n\nFile: packages/server-renderer/test/ssr-bundle-render.spec.ts (modified)\n  describe('SSR: bundle renderer', () => {\n    createAssertions(true)\n-  createAssertions(false)\n+  // createAssertions(false)\n  })\n  function createAssertions(runInNewContext) {",
    "comment": "test: upgrade vitest + fix ssr tests for Node 18+",
    "language": "diff",
    "repo": "vuejs/vue",
    "sha": "78ef6271d237e83fbd647567f2b79a5786f09d3d",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/types/utils.ts (modified)\n-// If the the type T accepts type \"any\", output type Y, otherwise output type N.\n+// If the type T accepts type \"any\", output type Y, otherwise output type N.\n  // https://stackoverflow.com/questions/49927523/disallow-call-with-any/49928360#49928360\n  export type IfAny<T, Y, N> = 0 extends 1 & T ? Y : N",
    "comment": "chore: fix typo in utils.ts (#12927) [skip ci]",
    "language": "diff",
    "repo": "vuejs/vue",
    "sha": "d52fbff7a77d8d9c032a74a21c34e2c122add5d9",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: dynamic_programming/bitmask.py (modified)\n              return self.dp[mask][task_no]\n          # Number of ways when we don't this task in the arrangement\n-        total_ways_util = self.count_ways_until(mask, task_no + 1)\n+        total_ways_until = self.count_ways_until(mask, task_no + 1)\n          # now assign the tasks one by one to all possible persons and recursively\n          # assign for the remaining tasks.\n                  # assign this task to p and change the mask value. And recursively\n                  # assign tasks with the new mask value.\n-                total_ways_util += self.count_ways_until(mask | (1 << p), task_no + 1)\n+                total_ways_until += self.count_ways_until(mask | (1 << p), task_no + 1)\n          # save the value.\n-        self.dp[mask][task_no] = total_ways_util\n+        self.dp[mask][task_no] = total_ways_until\n          return self.dp[mask][task_no]",
    "comment": "fix: correct typo \"util\" to \"until\" (#12653)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "a4576dc2a42cbfc7585a7ce6f28917cb97f83c45",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: modules/sd_schedulers.py (modified)\n      beta = 0.6\n      timesteps = 1 - np.linspace(0, 1, n)\n      timesteps = [stats.beta.ppf(x, alpha, beta) for x in timesteps]\n-    sigmas = [sigma_min + ((x)*(sigma_max-sigma_min)) for x in timesteps] + [0.0]\n-    sigmas = torch.FloatTensor(sigmas).to(device)\n-    return sigmas\n+    sigmas = [sigma_min + (x * (sigma_max-sigma_min)) for x in timesteps]\n+    sigmas += [0.0]\n+    return torch.FloatTensor(sigmas).to(device)\n  schedulers = [",
    "comment": "refactor: syntax and add 0.0 on new line",
    "language": "diff",
    "repo": "AUTOMATIC1111/stable-diffusion-webui",
    "sha": "7e1bd3e3c30e1d7e95f719a925f11d9225251f7c",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: lib/utils.js (modified)\n    return (prototype === null || prototype === Object.prototype || Object.getPrototypeOf(prototype) === null) && !(toStringTag in val) && !(iterator in val);\n  }\n+/**\n+ * Determine if a value is an empty object (safely handles Buffers)\n+ *\n+ * @param {*} val The value to test\n+ *\n+ * @returns {boolean} True if value is an empty object, otherwise false\n+ */\n+const isEmptyObject = (val) => {\n+  // Early return for non-objects or Buffers to prevent RangeError\n+  if (!isObject(val) || isBuffer(val)) {\n+    return false;\n+  }\n+  \n+  try {\n+    return Object.keys(val).length === 0 && Object.getPrototypeOf(val) === Object.prototype;\n+  } catch (e) {\n+    // Fallback for any other objects that might cause RangeError with Object.keys()\n+    return false;\n\nFile: test/unit/utils/utils.js (modified)\n        assert.strictEqual(JSON.stringify(jsonObject), JSON.stringify({x: 1, y:2, obj: {ok: 1}}))\n      });\n    });\n+\n+  describe('Buffer RangeError Fix', function () {\n+    it('should handle large Buffer in isEmptyObject without RangeError', function () {\n+      // Create a big buffer that used to cause the error\n+      const largeBuffer = Buffer.alloc(1024 * 1024 * 200); // 200MB\n+      \n+      // This used to throw: RangeError: Invalid array length\n+      // Now it should work fine\n+      const result = utils.isEmptyObject(largeBuffer);\n+      \n+      // Buffer should not be considered an empty object\n+      assert.strictEqual(result, false);\n+    });\n+\n+    it('should handle large Buffer in forEach without RangeError', function () {\n+      const largeBuffer = Buffer.alloc(1024 * 1024 * 200); // 200MB\n+      let count = 0;",
    "comment": "fix: prevent RangeError when using large Buffers (#6961)",
    "language": "diff",
    "repo": "axios/axios",
    "sha": "a2214ca1bc60540baf2c80573cea3a0ff91ba9d1",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: test/module/typings/esm/index.ts (modified)\n  import axios, {\n-  InternalAxiosRequestConfig,\n    AxiosRequestConfig,\n    AxiosHeaders,\n    AxiosRequestHeaders,\n\nFile: test/specs/defaults.spec.js (modified)\n  import defaults from '../../lib/defaults';\n-import utils from '../../lib/utils';\n  import AxiosHeaders from '../../lib/core/AxiosHeaders';\n  describe('defaults', function () {\n\nFile: test/specs/headers.spec.js (modified)\n-import assert from \"assert\";\n-\n  const {AxiosHeaders} = axios;\n  function testHeaderValue(headers, key, val) {",
    "comment": "refactor: remove unused import in test (#6922)",
    "language": "diff",
    "repo": "axios/axios",
    "sha": "ee7799e13c0783c0fdfa656613bb1af6f5e53ccd",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: test/module/typings/esm/index.ts (modified)\n    headers.get('x');\n  })();\n-// AxiosHeaders instance assigment\n+// AxiosHeaders instance assignment\n  {\n    const requestInterceptorId: number = axios.interceptors.request.use(\n\nFile: test/specs/defaults.spec.js (modified)\n      });\n    });\n-  it('should resistent to ReDoS attack', function (done) {\n+  it('should resistant to ReDoS attack', function (done) {\n      const instance = axios.create();\n      const start = performance.now();\n      const slashes = '/'.repeat(100000);",
    "comment": "chore: fix typos in test (#6923)",
    "language": "diff",
    "repo": "axios/axios",
    "sha": "eb0a2db04beda089e6bdcb2820f193ed2faecbc3",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: .prettier.config.js (added)\n+export default {\n+  semi: false,\n+  singleQuote: true,\n+  trailingComma: 'all',\n+}\n\nFile: eslint.config.js (added)\n+import globals from \"globals\";\n+import pluginJs from \"@eslint/js\";\n+import tseslint from \"typescript-eslint\";\n+\n+\n+export default [\n+  {files: [\"**/*.{js,mjs,cjs,ts}\"]},\n+  {languageOptions: { globals: globals.node }},\n+  pluginJs.configs.recommended,\n+  ...tseslint.configs.recommended,\n+];",
    "comment": "chore: update linter",
    "language": "diff",
    "repo": "typicode/json-server",
    "sha": "161e01f5232db335afca6061515b4f542accd5c0",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/service.ts (modified)\n      }\n      // Convert query params to conditions\n-    const conds: Record<string, [Condition, string | string[]]> = {}\n+    const conds: [string, Condition, string | string[]][] = []\n      for (const [key, value] of Object.entries(query)) {\n        if (value === undefined || typeof value !== 'string') {\n          continue\n        const op = reArr?.at(1)\n        if (op && isCondition(op)) {\n          const field = key.replace(re, '')\n-        conds[field] = [op, value]\n+        conds.push([field, op, value])\n          continue\n        }\n        if (\n        ) {\n          continue\n        }\n-      conds[key] = [Condition.default, value]\n+      conds.push([key, Condition.default, value])",
    "comment": "fix: filtering with multiple conditions",
    "language": "diff",
    "repo": "typicode/json-server",
    "sha": "e6055e621d326f1bbd0f8840a47d789eb8baa89c",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/app.ts (modified)\n    app.delete('/:name/:id', async (req, res, next) => {\n      const { name = '', id = '' } = req.params\n-    res.locals['data'] = await service.destroyById(name, id, req.query['dependent'])\n+    res.locals['data'] = await service.destroyById(name, id, req.query['_dependent'])\n      next()\n    })",
    "comment": "fix: The query parameter is incorrect. #1551 (#1552)",
    "language": "diff",
    "repo": "typicode/json-server",
    "sha": "1b7c0fb1b227dd9242d85d6bb9767b026cd79cb2",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/service.test.ts (modified)\n    id: '1',\n    title: 'a',\n    views: 100,\n+  published: true,\n    author: { name: 'foo' },\n    tags: ['foo', 'bar'],\n  }\n  const post2 = {\n    id: '2',\n    title: 'b',\n    views: 200,\n+  published: false,\n    author: { name: 'bar' },\n    tags: ['bar'],\n  }\n  const post3 = {\n    id: '3',\n    title: 'c',\n    views: 300,\n+  published: false,\n\nFile: src/service.ts (modified)\n              }\n              // item_ne=value\n              case Condition.ne: {\n-              if (!(itemValue != paramValue)) return false\n+              switch (typeof itemValue) {\n+                case 'number':\n+                  return itemValue !== parseInt(paramValue)\n+                case 'string':\n+                  return itemValue !== paramValue\n+                case 'boolean':\n+                  return itemValue !== (paramValue === 'true')\n+              }\n                break\n              }\n              // item=value\n              case Condition.default: {\n-              if (!(itemValue == paramValue)) return false\n+              switch (typeof itemValue) {\n+                case 'number':\n+                  return itemValue === parseInt(paramValue)",
    "comment": "fix: boolean handling in GET list",
    "language": "diff",
    "repo": "typicode/json-server",
    "sha": "827c5c31a4da829b036aa050c4bb678a3b3601f3",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/bin.ts (modified)\n  #!/usr/bin/env node\n  import { existsSync, readFileSync, writeFileSync } from 'node:fs'\n-import { extname, join } from 'node:path'\n+import { extname } from 'node:path'\n  import { parseArgs } from 'node:util'\n  import chalk from 'chalk'\n  import { DataFile, JSONFile } from 'lowdb/node'\n  import { PackageJson } from 'type-fest'\n+import { fileURLToPath } from 'node:url'\n  import { createApp } from './app.js'\n  import { Observer } from './observer.js'\n  import { Data } from './service.js'\n      // --version\n      if (values.version) {\n        const pkg = JSON.parse(\n-        readFileSync(join(__dirname, '../package.json'), 'utf8'),\n+        readFileSync(\n+          fileURLToPath(new URL('../package.json', import.meta.url)),\n+          'utf-8',\n+        ),",
    "comment": "fix: --version option",
    "language": "diff",
    "repo": "typicode/json-server",
    "sha": "d266622ab479f6b51781cac153e64140b6cfc52f",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: server/model/monitor.js (modified)\n      /**\n       * Send a notification about a monitor\n       * @param {boolean} isFirstBeat Is this beat the first of this monitor?\n-     * @param {Monitor} monitor The monitor to send a notificaton about\n+     * @param {Monitor} monitor The monitor to send a notification about\n       * @param {Bean} bean Status information about monitor\n       * @returns {void}\n       */\n\nFile: server/notification-providers/promosms.js (modified)\n              notification.promosmsAllowLongSMS = false;\n          }\n-        //TODO: Add option for enabling special characters. It will decrese message max length from 160 to 70 chars.\n+        //TODO: Add option for enabling special characters. It will decrease message max length from 160 to 70 chars.\n          //Lets remove non ascii char\n          let cleanMsg = msg.replace(/[^\\x00-\\x7F]/g, \"\");\n\nFile: server/socket-handlers/api-key-socket-handler.js (modified)\n              log.debug(\"apikeys\", \"Added API Key\");\n              log.debug(\"apikeys\", key);\n-            // Append key ID and prefix to start of key seperated by _, used to get\n+            // Append key ID and prefix to start of key separated by _, used to get\n              // correct hash when validating key.\n              let formattedKey = \"uk\" + bean.id + \"_\" + clearKey;\n              await sendAPIKeyList(socket);\n\nFile: server/uptime-calculator.js (modified)\n          let totalPing = 0;\n          let endTimestamp;\n-        // Get the eariest timestamp of the required period based on the type\n+        // Get the earliest timestamp of the required period based on the type\n          switch (type) {\n              case \"day\":\n                  endTimestamp = key - 86400 * (num - 1);\n          let endTimestamp;\n-        // Get the eariest timestamp of the required period based on the type\n+        // Get the earliest timestamp of the required period based on the type\n          switch (type) {\n              case \"day\":\n                  endTimestamp = key - 86400 * (num - 1);",
    "comment": "chore: fix some minor issues in comments (#5984)",
    "language": "diff",
    "repo": "louislam/uptime-kuma",
    "sha": "d490285a447c4401e9986f6dbed3995454042fb5",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: extra/close-incorrect-issue.js (modified)\n                  owner: issue.owner,\n                  repo: issue.repo,\n                  issue_number: issue.number,\n-                body: `@${username}: Hello! :wave:\\n\\nThis issue is being automatically closed because it does not follow the issue template. Please **DO NOT open blank issues and use our [issue-templates](https://github.com/louislam/uptime-kuma/issues/new/choose) instead**.\\nBlank Issues do not contain the context nessesary for a good discussions.`\n+                body: `@${username}: Hello! :wave:\\n\\nThis issue is being automatically closed because it does not follow the issue template. Please **DO NOT open blank issues and use our [issue-templates](https://github.com/louislam/uptime-kuma/issues/new/choose) instead**.\\nBlank Issues do not contain the context necessary for a good discussions.`\n              });\n              // Close the issue\n\nFile: server/database.js (modified)\n  class Database {\n      /**\n-     * Boostrap database for SQLite\n+     * Bootstrap database for SQLite\n       * @type {string}\n       */\n      static templatePath = \"./db/kuma.db\";\n\nFile: server/notification-providers/flashduty.js (modified)\n      }\n      /**\n-     * Generate a monitor url from the monitors infomation\n+     * Generate a monitor url from the monitors information\n       * @param {object} monitorInfo Monitor details\n       * @returns {string|undefined} Monitor URL\n       */\n\nFile: server/notification-providers/pushdeer.js (modified)\n      async send(notification, msg, monitorJSON = null, heartbeatJSON = null) {\n          const okMsg = \"Sent Successfully.\";\n          const serverUrl = notification.pushdeerServer || \"https://api2.pushdeer.com\";\n-        // capture group below is nessesary to prevent an ReDOS-attack\n+        // capture group below is necessary to prevent an ReDOS-attack\n          const url = `${serverUrl.trim().replace(/([^/])\\/+$/, \"$1\")}/message/push`;\n          let valid = msg != null && monitorJSON != null && heartbeatJSON != null;",
    "comment": "chore: fix typos in code comments (#5966)",
    "language": "diff",
    "repo": "louislam/uptime-kuma",
    "sha": "54b0c89ea5a439110f67b6e7c985771cf983c161",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: backend/prompts/screenshot_system_prompts.py (modified)\n  You are an expert Tailwind developer\n  You take screenshots of a reference web page from the user, and then build single page apps \n  using Tailwind, HTML and JS.\n-You might also be given a screenshot(The second image) of a web page that you have already built, and asked to\n-update it to look more like the reference image(The first image).\n  - Make sure the app looks exactly like the screenshot.\n  - Pay close attention to background color, text color, font size, font family, \n  You are an expert CSS developer\n  You take screenshots of a reference web page from the user, and then build single page apps \n  using CSS, HTML and JS.\n-You might also be given a screenshot(The second image) of a web page that you have already built, and asked to\n-update it to look more like the reference image(The first image).\n  - Make sure the app looks exactly like the screenshot.\n  - Pay close attention to background color, text color, font size, font family, \n  You are an expert Bootstrap developer\n  You take screenshots of a reference web page from the user, and then build single page apps \n  using Bootstrap, HTML and JS.\n-You might also be given a screenshot(The second image) of a web page that you have already built, and asked to\n-update it to look more like the reference image(The first image).\n  - Make sure the app looks exactly like the screenshot.\n\nFile: backend/tests/test_prompt_summary.py (modified)\n-import pytest\n  from utils import format_prompt_summary\n              \"role\": \"user\",\n              \"content\": [\n                  {\"type\": \"text\", \"text\": \"hello world\"},\n-                {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/png;base64,AAA\"}},\n-                {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/png;base64,BBB\"}},\n+                {\n+                    \"type\": \"image_url\",\n+                    \"image_url\": {\"url\": \"data:image/png;base64,AAA\"},\n+                },\n+                {\n+                    \"type\": \"image_url\",\n+                    \"image_url\": {\"url\": \"data:image/png;base64,BBB\"},\n+                },\n              ],\n          },\n      ]\n\nFile: frontend/src/App.tsx (modified)\n    const {\n      disableInSelectAndEditMode,\n      setUpdateInstruction,\n+    updateImage,\n+    setUpdateImage,\n      appState,\n      setAppState,\n    } = useAppStore();\n      const updatedHistory = [\n        ...historyTree,\n-      { text: modifiedUpdateInstruction, images: [] },\n+      { text: modifiedUpdateInstruction, images: updateImage ? [updateImage] : [] },\n      ];\n      doGenerateCode({\n      });\n      setUpdateInstruction(\"\");\n+    setUpdateImage(null);\n    }\n    const handleTermDialogOpenChange = (open: boolean) => {\n\nFile: frontend/src/components/sidebar/Sidebar.tsx (modified)\n  import { useEffect, useRef, useState } from \"react\";\n  import HistoryDisplay from \"../history/HistoryDisplay\";\n  import Variants from \"../variants/Variants\";\n+import UpdateImageUpload from \"../UpdateImageUpload\";\n  interface SidebarProps {\n    showSelectAndEditFeature: boolean;\n    const textareaRef = useRef<HTMLTextAreaElement>(null);\n    const [isErrorExpanded, setIsErrorExpanded] = useState(false);\n-  const { appState, updateInstruction, setUpdateInstruction } = useAppStore();\n+  const { appState, updateInstruction, setUpdateInstruction, updateImage, setUpdateImage } = useAppStore();\n    const { inputMode, referenceImages, head, commits } = useProjectStore();\n          !isSelectedVariantError && (\n            <div>\n              <div className=\"grid w-full gap-2\">\n+              <UpdateImageUpload \n+                updateImage={updateImage} \n+                setUpdateImage={setUpdateImage} \n+              />\n                <Textarea\n                  ref={textareaRef}",
    "comment": "feat: add frontend support for images in follow-up edits",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "bbda0b37fecb5440967a61ab600e10b09e8c4dea",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: backend/prompts/__init__.py (modified)\n-from typing import Union, Any\n+from typing import Union, Any, cast\n  from openai.types.chat import ChatCompletionMessageParam, ChatCompletionContentPartParam\n  from custom_types import InputMode\n          prompt_messages = assemble_imported_code_prompt(original_imported_code, stack)\n          for index, item in enumerate(params[\"history\"][1:]):\n              role = \"user\" if index % 2 == 0 else \"assistant\"\n-            message: ChatCompletionMessageParam = {\n-                \"role\": role,\n-                \"content\": item[\"text\"],\n-            }\n+            message = create_message_from_history_item(item, role)\n              prompt_messages.append(message)\n      else:\n          # Assemble the prompt for non-imported code\n              # Transform the history tree into message format\n              for index, item in enumerate(params[\"history\"]):\n                  role = \"assistant\" if index % 2 == 0 else \"user\"\n-                message: ChatCompletionMessageParam = {\n-                    \"role\": role,",
    "comment": "feat: add support for images in update history",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "4f1b5f1fe0e3ff551e1be5c47dd17aca9a4d3caa",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/markitdown/src/markitdown/_exceptions.py (modified)\n                  message = f\"File conversion failed after {len(attempts)} attempts:\\n\"\n                  for attempt in attempts:\n                      if attempt.exc_info is None:\n-                        message += \" -  {type(attempt.converter).__name__} provided no execution info.\"\n+                        message += f\" -  {type(attempt.converter).__name__} provided no execution info.\"\n                      else:\n                          message += f\" - {type(attempt.converter).__name__} threw {attempt.exc_info[0].__name__} with message: {attempt.exc_info[1]}\\n\"",
    "comment": "fix: correct f-string formatting in FileConversionException (#1121)",
    "language": "diff",
    "repo": "microsoft/markitdown",
    "sha": "75140a90e2f78b44061c1798d7ece2995559bd9d",
    "quality_score": 1.0,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/compiler-sfc/test/prefixIdentifiers.spec.ts (modified)\n        var _vm = this,\n          _c = _vm._self._c\n        return _c(\n-        \\\\\"div\\\\\",\n-        { attrs: { id: \\\\\"app\\\\\" } },\n+        \"div\",\n+        { attrs: { id: \"app\" } },\n          [\n-          _c(\\\\\"div\\\\\", { style: { color: _vm.color } }, [_vm._v(_vm._s(_vm.foo))]),\n-          _vm._v(\\\\\" \\\\\"),\n+          _c(\"div\", { style: { color: _vm.color } }, [_vm._v(_vm._s(_vm.foo))]),\n+          _vm._v(\" \"),\n            _vm._l(_vm.list, function (i) {\n-            return _c(\\\\\"p\\\\\", [_vm._v(_vm._s(i))])\n+            return _c(\"p\", [_vm._v(_vm._s(i))])\n            }),\n-          _vm._v(\\\\\" \\\\\"),\n-          _c(\\\\\"foo\\\\\", {\n+          _vm._v(\" \"),\n+          _c(\"foo\", {\n\nFile: test/unit/features/global-api/config.spec.ts (modified)\n        expect(vm.computed).toBe('1,2,3')\n        expect(vm.data).toBe('1,2,3')\n-      vm.prop.push(4, 5)\n+      vm.prop = [...vm.prop, 4, 5]\n        expect(vm.computed).toBe('1,2,3,4,5')\n        expect(vm.data).toBe('1,2,3,4,5')\n        Vue.config.async = true",
    "comment": "test: fix tests",
    "language": "diff",
    "repo": "vuejs/vue",
    "sha": "d30f6fd25f90973a84fadb43eef75a54c4b42ea2",
    "quality_score": 0.8999999999999999,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/modeling_flash_attention_utils.py (modified)\n      query_states, key_states, value_states = fa_peft_integration_check(\n          query_states, key_states, value_states, target_dtype\n      )\n-    use_mask = position_ids is not None or all([cu_seq_lens_q, cu_seq_lens_k, max_length_q, max_length_k])\n+    use_mask = position_ids is not None or all(k is not None for k in [cu_seq_lens_q, cu_seq_lens_k, max_length_q, max_length_k])\n      if attention_mask is not None:\n          q, k, v, idx, (cu_q, cu_k), (mq, mk) = _upad_input(\n              query_states, key_states, value_states, attention_mask, query_length, unpad_fn",
    "comment": "Fix: explicit not none check for tensors in flash attention (#39639)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "4600c27c4f3ba6afd394da78045f0553e18ef934",
    "quality_score": 0.8999999999999999,
    "comment_type": "commit_message"
  },
  {
    "code": "File: lib/core/Axios.js (modified)\n      if (!synchronousRequestInterceptors) {\n        const chain = [dispatchRequest.bind(this), undefined];\n-      chain.unshift.apply(chain, requestInterceptorChain);\n-      chain.push.apply(chain, responseInterceptorChain);\n+      chain.unshift(...requestInterceptorChain);\n+      chain.push(...responseInterceptorChain);\n        len = chain.length;\n        promise = Promise.resolve(config);\n\nFile: lib/helpers/throttle.js (modified)\n        clearTimeout(timer);\n        timer = null;\n      }\n-    fn.apply(null, args);\n+    fn(...args);\n    }\n    const throttled = (...args) => {",
    "comment": "refactor: use spread operator instead of '.apply()' (#6938)",
    "language": "diff",
    "repo": "axios/axios",
    "sha": "6161947d9d3496ae75909a2ded98fa43ecb7e572",
    "quality_score": 0.8999999999999999,
    "comment_type": "commit_message"
  },
  {
    "code": "File: bin/RepoBot.js (modified)\n        templates\n      } = options || {};\n-    this.templates = Object.assign({\n-      published: NOTIFY_PR_TEMPLATE\n-    }, templates);\n+    this.templates = {\n+      published: NOTIFY_PR_TEMPLATE,\n+      ...templates\n+    };\n      this.github = api || new GithubAPI(owner, repo);\n\nFile: lib/core/mergeConfig.js (modified)\n      headers: (a, b , prop) => mergeDeepProperties(headersToObject(a), headersToObject(b),prop, true)\n    };\n-  utils.forEach(Object.keys(Object.assign({}, config1, config2)), function computeConfigValue(prop) {\n+  utils.forEach(Object.keys({...config1, ...config2}), function computeConfigValue(prop) {\n      const merge = mergeMap[prop] || mergeDeepProperties;\n      const configValue = merge(config1[prop], config2[prop], prop);\n      (utils.isUndefined(configValue) && merge !== mergeDirectKeys) || (config[prop] = configValue);\n\nFile: lib/helpers/toURLEncodedForm.js (modified)\n  import platform from '../platform/index.js';\n  export default function toURLEncodedForm(data, options) {\n-  return toFormData(data, new platform.classes.URLSearchParams(), Object.assign({\n+  return toFormData(data, new platform.classes.URLSearchParams(), {\n      visitor: function(value, key, path, helpers) {\n        if (platform.isNode && utils.isBuffer(value)) {\n          this.append(key, value.toString('base64'));\n          return false;\n        }\n        return helpers.defaultVisitor.apply(this, arguments);\n-    }\n-  }, options));\n+    },\n+    ...options\n+  });\n  }",
    "comment": "refactor: use an object spread instead of Object.assign (#6939)",
    "language": "diff",
    "repo": "axios/axios",
    "sha": "a1d16dd9c59af11abd687b42bbeab1d50d01654e",
    "quality_score": 0.8999999999999999,
    "comment_type": "commit_message"
  },
  {
    "code": "File: whisper/timing.py (modified)\n      x_skew = x_skew.T.contiguous()\n      cost = torch.ones(N + M + 2, M + 2) * np.inf\n      cost[0, 0] = 0\n-    cost = cost.cuda()\n+    cost = cost.to(x.device)\n      trace = torch.zeros_like(cost, dtype=torch.int32)\n      dtw_kernel[(1,)](",
    "comment": "Fix: Ensure DTW cost tensor is on the same device as input tensor (#2561)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "679ae1d14167541384b4e732f80847e1c5095b19",
    "quality_score": 0.8999999999999999,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/app.test.ts (modified)\n    const arr: Test[] = [\n      // Static\n      { method: 'GET', url: '/', statusCode: 200 },\n-    { method: 'GET', url: '/output.css', statusCode: 200 },\n+    { method: 'GET', url: '/test.html', statusCode: 200 },\n      { method: 'GET', url: `/${file}`, statusCode: 200 },\n      // CORS\n\nFile: src/app.ts (modified)\n    const app = new App()\n    // Static files\n-  app.use(sirv(join(__dirname, '../public'), { dev: !isProduction }))\n+  app.use(sirv('public', { dev: !isProduction }))\n    options.static\n      ?.map((path) => (isAbsolute(path) ? path : join(process.cwd(), path)))\n      .forEach((dir) => app.use(sirv(dir, { dev: !isProduction })))",
    "comment": "fix: public",
    "language": "diff",
    "repo": "typicode/json-server",
    "sha": "d0307026e4440ff16e827293f956b25b77606619",
    "quality_score": 0.8999999999999999,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/app.ts (modified)\n    app.get('/:name', (req, res, next) => {\n      const { name = '' } = req.params\n-    res.locals['data'] = service.find(name, req.query)\n+    const query = Object.fromEntries(Object.entries(req.query)\n+      .map(([key, value]) => {\n+        if (['_start', '_end', '_limit', '_page', '_per_page'].includes(key) && typeof value === 'string') {\n+          return [key, parseInt(value)]\n+        } else {\n+          return [key, value]\n+        }\n+      })\n+      .filter(([_, value]) => !Number.isNaN(value))\n+    )\n+    res.locals['data'] = service.find(name, query)\n      next()\n    })\n\nFile: src/service.ts (modified)\n      name: string,\n      query: {\n        [key: string]: unknown\n-    } & {\n        _embed?: string[]\n        _sort?: string\n        _start?: number\n      const start = query._start\n      const end = query._end\n      const limit = query._limit\n-    if (start === undefined && limit) {\n-      return sorted.slice(0, limit)\n+    if (start !== undefined) {\n+      if (end !== undefined) {\n+        return sorted.slice(start, end)\n+      }\n+      return sorted.slice(start, start + (limit || 0))\n      }\n-    if (start && limit) {\n-      return sorted.slice(start, start + limit)",
    "comment": "fix: pagination",
    "language": "diff",
    "repo": "typicode/json-server",
    "sha": "3710dce4a025da99ffce4b424a51b6c9d07861bb",
    "quality_score": 0.8999999999999999,
    "comment_type": "commit_message"
  },
  {
    "code": "File: test/e2e/specs/status-page.spec.js (modified)\n          await expect(page.getByTestId(\"update-countdown-text\")).toContainText(\"00:\");\n          const updateCountdown = Number((await page.getByTestId(\"update-countdown-text\").textContent()).match(/(\\d+):(\\d+)/)[2]);\n-        expect(updateCountdown).toBeGreaterThanOrEqual(refreshInterval); // cant be certain when the timer will start, so ensure it's within expected range\n-        expect(updateCountdown).toBeLessThanOrEqual(refreshInterval + 10);\n+        expect(updateCountdown).toBeGreaterThanOrEqual(refreshInterval - 10); // cant be certain when the timer will start, so ensure it's within expected range\n+        expect(updateCountdown).toBeLessThanOrEqual(refreshInterval);\n          await expect(page.locator(\"body\")).toHaveClass(theme);",
    "comment": "fix: refresh interval getting incremented by 10 on status page despite a minimum of 5 (#5961)",
    "language": "diff",
    "repo": "louislam/uptime-kuma",
    "sha": "487cb8fdc5905237625e62496b56f7b41455406c",
    "quality_score": 0.8999999999999999,
    "comment_type": "commit_message"
  },
  {
    "code": "File: db/knex_migrations/2025-06-24-0000-add-audience-to-oauth.js (added)\n+exports.up = function (knex) {\n+    return knex.schema\n+        .alterTable(\"monitor\", function (table) {\n+            table.string(\"oauth_audience\").nullable().defaultTo(null);\n+        });\n+};\n+\n+exports.down = function (knex) {\n+    return knex.schema.alterTable(\"monitor\", function (table) {\n+        table.string(\"oauth_audience\").alter();\n+    });\n+};\n\nFile: server/model/monitor.js (modified)\n                  oauth_client_secret: this.oauth_client_secret,\n                  oauth_token_url: this.oauth_token_url,\n                  oauth_scopes: this.oauth_scopes,\n+                oauth_audience: this.oauth_audience,\n                  oauth_auth_method: this.oauth_auth_method,\n                  pushToken: this.pushToken,\n                  databaseConnectionString: this.databaseConnectionString,\n       */\n      async makeOidcTokenClientCredentialsRequest() {\n          log.debug(\"monitor\", `[${this.name}] The oauth access-token undefined or expired. Requesting a new token`);\n-        const oAuthAccessToken = await getOidcTokenClientCredentials(this.oauth_token_url, this.oauth_client_id, this.oauth_client_secret, this.oauth_scopes, this.oauth_auth_method);\n+        const oAuthAccessToken = await getOidcTokenClientCredentials(this.oauth_token_url, this.oauth_client_id, this.oauth_client_secret, this.oauth_scopes, this.oauth_audience, this.oauth_auth_method);\n          if (this.oauthAccessToken?.expires_at) {\n              log.debug(\"monitor\", `[${this.name}] Obtained oauth access-token. Expires at ${new Date(this.oauthAccessToken?.expires_at * 1000)}`);\n          } else {\n\nFile: server/server.js (modified)\n                  bean.oauth_auth_method = monitor.oauth_auth_method;\n                  bean.oauth_token_url = monitor.oauth_token_url;\n                  bean.oauth_scopes = monitor.oauth_scopes;\n+                bean.oauth_audience = monitor.oauth_audience;\n                  bean.tlsCa = monitor.tlsCa;\n                  bean.tlsCert = monitor.tlsCert;\n                  bean.tlsKey = monitor.tlsKey;\n\nFile: server/util-server.js (modified)\n  };\n  /**\n- * Decodes a jwt and returns the payload portion without verifying the jqt.\n+ * Decodes a jwt and returns the payload portion without verifying the jwt.\n   * @param {string} jwt The input jwt as a string\n   * @returns {object} Decoded jwt payload object\n   */\n  };\n  /**\n- * Gets a Access Token form a oidc/oauth2 provider\n- * @param {string} tokenEndpoint The token URI form the auth service provider\n+ * Gets an Access Token from an oidc/oauth2 provider\n+ * @param {string} tokenEndpoint The token URI from the auth service provider\n   * @param {string} clientId The oidc/oauth application client id\n   * @param {string} clientSecret The oidc/oauth application client secret\n- * @param {string} scope The scope the for which the token should be issued for\n- * @param {string} authMethod The method on how to sent the credentials. Default client_secret_basic\n+ * @param {string} scope The scope(s) for which the token should be issued for\n+ * @param {string} audience The audience for which the token should be issued for\n+ * @param {string} authMethod The method used to send the credentials. Default client_secret_basic",
    "comment": "feat: Add optional audience for http-monitors via the oauth2 client credentials flow (#5950)",
    "language": "diff",
    "repo": "louislam/uptime-kuma",
    "sha": "9506b3a16be54334e4d2f3f683e780b92471ca87",
    "quality_score": 0.8999999999999999,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/markitdown/src/markitdown/converter_utils/docx/math/omml.py (modified)\n  On 25/03/2025\n  \"\"\"\n-import xml.etree.ElementTree as ET\n+from defusedxml import ElementTree as ET\n  from .latex_dict import (\n      CHARS,",
    "comment": "fix: python.lang.security.use-defused-xml-parse.use-defused-xml-parse-packages-markitdown-src-markitdown-converter_utils-docx-math-omml.py (#1251)",
    "language": "diff",
    "repo": "microsoft/markitdown",
    "sha": "39e72529406d8daa6860d2d14954fd7d63d6572a",
    "quality_score": 0.8999999999999999,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/app.ts (modified)\n    app.delete('/:name/:id', async (req, res, next) => {\n      const { name = '', id = '' } = req.params\n-    res.locals['data'] = await service.destroyById(name, id)\n+    res.locals['data'] = await service.destroyById(name, id, req.query['dependent'])\n      next()\n    })\n\nFile: src/service.ts (modified)\n      name: string,\n      query: {\n        [key: string]: unknown\n-      _embed?: string[]\n+      _embed?: string | string[]\n        _sort?: string\n        _start?: number\n        _end?: number\n    async destroyById(\n      name: string,\n      id: string,\n-    dependents: string[] = [],\n+    dependent?: string | string[],\n    ): Promise<Item | undefined> {\n      const items = this.#get(name)\n      if (items === undefined || !Array.isArray(items)) return\n      items.splice(index, 1)[0]\n      nullifyForeignKey(this.#db, name, id)\n+    const dependents = ensureArray(dependent)\n      deleteDependents(this.#db, name, dependents)",
    "comment": "feat: rename _embed to _dependent for delete action",
    "language": "diff",
    "repo": "typicode/json-server",
    "sha": "86d0c26b6e8fbc7da296498bec35738b92dd48ed",
    "quality_score": 0.8,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/core/components/keep-alive.ts (modified)\n  }\n  function pruneCache(\n-  keepAliveInstance: { cache: CacheEntryMap; keys: string[]; _vnode: VNode },\n+  keepAliveInstance: {\n+    cache: CacheEntryMap\n+    keys: string[]\n+    _vnode: VNode\n+    $vnode: VNode\n+  },\n    filter: Function\n  ) {\n-  const { cache, keys, _vnode } = keepAliveInstance\n+  const { cache, keys, _vnode, $vnode } = keepAliveInstance\n    for (const key in cache) {\n      const entry = cache[key]\n      if (entry) {\n        }\n      }\n    }\n+  $vnode.componentOptions!.children = undefined\n\nFile: src/core/vdom/create-component.ts (modified)\n        vnode, // new parent vnode\n        options.children // new children\n      )\n-    // #12187 unset children reference after use to avoid memory leak\n-    options.children = undefined\n    },\n    insert(vnode: MountedComponentVNode) {",
    "comment": "fix(keep-alive): fix memory leak without breaking transition tests",
    "language": "diff",
    "repo": "vuejs/vue",
    "sha": "e0747f40a879b4000a1959d21377b51d1f1ed988",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/core/vdom/create-component.ts (modified)\n        vnode, // new parent vnode\n        options.children // new children\n      )\n+    // #12187 unset children reference after use to avoid memory leak\n+    options.children = undefined\n    },\n    insert(vnode: MountedComponentVNode) {",
    "comment": "fix(keep-alive): fix keep-alive memory leak",
    "language": "diff",
    "repo": "vuejs/vue",
    "sha": "2632249925e632e56f6dfc8fdbcf682c82e4081b",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/v3/reactivity/ref.ts (modified)\n    return !!(r && (r as Ref).__v_isRef === true)\n  }\n-export function ref<T extends object>(\n-  value: T\n-): [T] extends [Ref] ? T : Ref<UnwrapRef<T>>\n+export function ref<T extends Ref>(value: T): T\n  export function ref<T>(value: T): Ref<UnwrapRef<T>>\n  export function ref<T = any>(): Ref<T | undefined>\n  export function ref(value?: unknown) {\n  export type ShallowRef<T = any> = Ref<T> & { [ShallowRefMarker]?: true }\n-export function shallowRef<T extends object>(\n-  value: T\n-): T extends Ref ? T : ShallowRef<T>\n+export function shallowRef<T>(value: T | Ref<T>): Ref<T> | ShallowRef<T>\n+export function shallowRef<T extends Ref>(value: T): T\n  export function shallowRef<T>(value: T): ShallowRef<T>\n  export function shallowRef<T = any>(): ShallowRef<T | undefined>\n  export function shallowRef(value?: unknown) {\n\nFile: types/test/v3/reactivity-test.ts (modified)\n    set,\n    del\n  } from '../../index'\n-import { describe, expectType } from '../utils'\n+import { IsUnion, describe, expectType } from '../utils'\n  function plainType(arg: number | Ref<number>) {\n    // ref coercing\n    // @ts-expect-error\n    del([], 'fse', 123)\n  })\n+\n+\n+{\n+  //#12978\n+  type Steps = { step: '1' } | { step: '2' }\n+  const shallowUnionGenParam = shallowRef<Steps>({ step: '1' })\n+  const shallowUnionAsCast = shallowRef({ step: '1' } as Steps)\n+\n+  expectType<IsUnion<typeof shallowUnionGenParam>>(false)\n+  expectType<IsUnion<typeof shallowUnionAsCast>>(false)\n\nFile: types/test/v3/watch-test.ts (modified)\n-import { ref, computed, watch } from '../../index'\n+import { ref, computed, watch, shallowRef } from '../../index'\n  import { expectType } from '../utils'\n  const source = ref('foo')\n    // no type error\n    console.log(value2.a)\n  })\n+\n+{\n+  //#12978\n+  type Steps = { step: '1' } | { step: '2' }\n+  const shallowUnionGenParam = shallowRef<Steps>({ step: '1' })\n+  const shallowUnionAsCast = shallowRef({ step: '1' } as Steps)\n+\n+  watch(shallowUnionGenParam, value => {\n+    expectType<Steps>(value)\n+  })\n+  watch(shallowUnionAsCast, value => {\n+    expectType<Steps>(value)\n+  })",
    "comment": "fix(types): fix shallowRef's return type  (#12979)",
    "language": "diff",
    "repo": "vuejs/vue",
    "sha": "a174c29dab2cf655b06f7870e0ac5a78ef35ec8a",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/compiler-sfc/src/compileScript.ts (modified)\n  }\n  function extractEventNames(\n-  eventName: Identifier | RestElement,\n+  eventName: ArrayPattern | Identifier | ObjectPattern | RestElement,\n    emits: Set<string>\n  ) {\n    if (\n\nFile: packages/compiler-sfc/src/rewriteDefault.ts (modified)\n    }).program.body\n    ast.forEach(node => {\n      if (node.type === 'ExportDefaultDeclaration') {\n-      if (node.declaration.type === 'ClassDeclaration') {\n-        s.overwrite(node.start!, node.declaration.id.start!, `class `)\n+      if (node.declaration.type === 'ClassDeclaration' && node.declaration.id) {\n+        let start: number =\n+          node.declaration.decorators && node.declaration.decorators.length > 0\n+            ? node.declaration.decorators[\n+                node.declaration.decorators.length - 1\n+              ].end!\n+            : node.start!\n+        s.overwrite(start, node.declaration.id.start!, ` class `)\n          s.append(`\\nconst ${as} = ${node.declaration.id.name}`)\n        } else {\n          s.overwrite(node.start!, node.declaration.start!, `const ${as} = `)\n\nFile: packages/compiler-sfc/test/rewriteDefault.spec.ts (modified)\n      ).toMatchInlineSnapshot(`\n        \"/*\n        export default class Foo {}*/\n-      class Bar {}\n+       class Bar {}\n        const script = Bar\"\n      `)\n    })\n    test('@Component\\nexport default class w/ comments', async () => {\n      expect(\n-      rewriteDefault(`// export default\\n@Component\\nexport default class Foo {}`, 'script')\n+      rewriteDefault(\n+        `// export default\\n@Component\\nexport default class Foo {}`,\n+        'script'\n+      )\n      ).toMatchInlineSnapshot(`\n        \"// export default\n        @Component\n    test('@Component\\nexport default class w/ comments 3', async () => {\n      expect(",
    "comment": "fix(compiler-sfc): fix rewriteDefault edge cases",
    "language": "diff",
    "repo": "vuejs/vue",
    "sha": "25f97a5033187372e7b8c591c79336197ee5c833",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/compiler-sfc/src/compileScript.ts (modified)\n        startOffset,\n        `\\nconst ${propsIdentifier} = __props${\n          propsTypeDecl ? ` as ${genSetupPropsType(propsTypeDecl)}` : ``\n-      }\\n`\n+      };\\n`\n      )\n    }\n    if (propsDestructureRestId) {\n      s.prependLeft(\n        startOffset,\n        `\\nconst ${propsDestructureRestId} = ${helper(\n          `createPropsRestProxy`\n-      )}(__props, ${JSON.stringify(Object.keys(propsDestructuredBindings))})\\n`\n+      )}(__props, ${JSON.stringify(Object.keys(propsDestructuredBindings))});\\n`\n      )\n    }",
    "comment": "fix(compiler-sfc): add semicolon after `defineProps` statement (#12879)",
    "language": "diff",
    "repo": "vuejs/vue",
    "sha": "51fef2ca69459c1175e105991f60511f1996e0c8",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/compiler-sfc/src/compileScript.ts (modified)\n              } else {\n                let start = decl.start! + startOffset\n                let end = decl.end! + startOffset\n-              if (i < total - 1) {\n-                // not the last one, locate the start of the next\n+              if (i === 0) {\n+                // first one, locate the start of the next\n                  end = node.declarations[i + 1].start! + startOffset\n                } else {\n-                // last one, locate the end of the prev\n+                // not first one, locate the end of the prev\n                  start = node.declarations[i - 1].end! + startOffset\n                }\n                s.remove(start, end)\n\nFile: packages/compiler-sfc/test/compileScript.spec.ts (modified)\n      expect(content).toMatch(`emits: ['a'],`)\n    })\n+  // vuejs/core #6757\n+  test('defineProps/defineEmits in multi-variable declaration fix #6757 ', () => {\n+    const { content } = compile(`\n+    <script setup>\n+    const a = 1,\n+          props = defineProps(['item']),\n+          emit = defineEmits(['a']);\n+    </script>\n+  `)\n+    assertCode(content)\n+    expect(content).toMatch(`const a = 1;`) // test correct removal\n+    expect(content).toMatch(`props: ['item'],`)\n+    expect(content).toMatch(`emits: ['a'],`)\n+  })\n+\n    test('defineProps/defineEmits in multi-variable declaration (full removal)', () => {\n      const { content } = compile(`\n      <script setup>",
    "comment": "fix(compiler-sfc): fix macro usage in multi-variable declaration (#12873)",
    "language": "diff",
    "repo": "vuejs/vue",
    "sha": "d27c128b7cb1640f3aa185a5ecdea4ff35763794",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/shared/util.ts (modified)\n    modules: Array<{ staticKeys?: string[] } /* ModuleOptions */>\n  ): string {\n    return modules\n-    .reduce((keys, m) => {\n-      return keys.concat(m.staticKeys || [])\n-    }, [] as string[])\n+    .reduce<string[]>((keys, m) => keys.concat(m.staticKeys || []), [])\n      .join(',')\n  }",
    "comment": "refactor(shared): update `genStaticKeys` (#13010)",
    "language": "diff",
    "repo": "vuejs/vue",
    "sha": "49b6bd4264c25ea41408f066a1835f38bf6fe9f1",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: dynamic_programming/matrix_chain_order.py (modified)\n  Implementation of Matrix Chain Multiplication\n  Time Complexity: O(n^3)\n  Space Complexity: O(n^2)\n+\n+Reference: https://en.wikipedia.org/wiki/Matrix_chain_multiplication\n  \"\"\"\n-def matrix_chain_order(array):\n+def matrix_chain_order(array: list[int]) -> tuple[list[list[int]], list[list[int]]]:\n+    \"\"\"\n+    >>> matrix_chain_order([10, 30, 5])\n+    ([[0, 0, 0], [0, 0, 1500], [0, 0, 0]], [[0, 0, 0], [0, 0, 1], [0, 0, 0]])\n+    \"\"\"\n      n = len(array)\n-    matrix = [[0 for x in range(n)] for x in range(n)]\n-    sol = [[0 for x in range(n)] for x in range(n)]\n+    matrix = [[0 for _ in range(n)] for _ in range(n)]\n+    sol = [[0 for _ in range(n)] for _ in range(n)]\n      for chain_length in range(2, n):\n          for a in range(1, n - chain_length + 1):\n      return matrix, sol",
    "comment": "Update matrix_chain_order calculation with more details and test. (#12759)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "a8ad2db2b966fcbd2e6a82b8f1544116838c02b0",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: scripts/validate_filenames.py (modified)\n-#!python\n+#!/usr/bin/env python3\n+\n  import os\n  try:",
    "comment": "Add a proper shebang line to scripts/validate_filenames.py (#12733)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "f721e598e5677ed368b7dbd119092eb409ab2fb0",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/data-structures/heap/Heap.js (modified)\n    /* istanbul ignore next */\n    pairIsInCorrectOrder(firstElement, secondElement) {\n      throw new Error(`\n-      You have to implement heap pair comparision method\n+      You have to implement heap pair comparison method\n        for ${firstElement} and ${secondElement} values.\n      `);\n    }",
    "comment": "Fix four typos (#1139)",
    "language": "diff",
    "repo": "trekhleb/javascript-algorithms",
    "sha": "e5b5944c6849389bbfc39f476fa7a1f97d8ce4c6",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/algorithms/search/binary-search/binarySearch.js (modified)\n  export default function binarySearch(sortedArray, seekElement, comparatorCallback) {\n    // Let's create comparator from the comparatorCallback function.\n-  // Comparator object will give us common comparison methods like equal() and lessThen().\n+  // Comparator object will give us common comparison methods like equal() and lessThan().\n    const comparator = new Comparator(comparatorCallback);\n    // These two indices will contain current array (sub-array) boundaries.",
    "comment": "fix binary search typo (#1079)",
    "language": "diff",
    "repo": "trekhleb/javascript-algorithms",
    "sha": "8959566a36b3b7c28064da04c1522f3920956e22",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/algorithms/image-processing/seam-carving/__tests__/resizeImageWidth.node.js (added)\n+import fs from 'fs';\n+import { PNG } from 'pngjs';\n+\n+import resizeImageWidth from '../resizeImageWidth';\n+\n+const testImageBeforePath = './src/algorithms/image-processing/seam-carving/__tests__/test-image-before.png';\n+const testImageAfterPath = './src/algorithms/image-processing/seam-carving/__tests__/test-image-after.png';\n+\n+/**\n+ * Compares two images and finds the number of different pixels.\n+ *\n+ * @param {ImageData} imgA - ImageData for the first image.\n+ * @param {ImageData} imgB - ImageData for the second image.\n+ * @param {number} threshold - Color difference threshold [0..255]. Smaller - stricter.\n+ * @returns {number} - Number of different pixels.\n+ */\n+function pixelsDiff(imgA, imgB, threshold = 0) {\n+  if (imgA.width !== imgB.width || imgA.height !== imgB.height) {\n+    throw new Error('Images must have the same size');\n+  }\n\nFile: src/algorithms/image-processing/seam-carving/__tests__/resizeImageWidth.test.js (removed)\n-import { createCanvas, loadImage } from 'canvas';\n-import resizeImageWidth from '../resizeImageWidth';\n-\n-const testImageBeforePath = './src/algorithms/image-processing/seam-carving/__tests__/test-image-before.jpg';\n-const testImageAfterPath = './src/algorithms/image-processing/seam-carving/__tests__/test-image-after.jpg';\n-\n-/**\n- * Compares two images and finds the number of different pixels.\n- *\n- * @param {ImageData} imgA - ImageData for the first image.\n- * @param {ImageData} imgB - ImageData for the second image.\n- * @param {number} threshold - Color difference threshold [0..255]. Smaller - stricter.\n- * @returns {number} - Number of different pixels.\n- */\n-function pixelsDiff(imgA, imgB, threshold = 0) {\n-  if (imgA.width !== imgB.width || imgA.height !== imgB.height) {\n-    throw new Error('Images must have the same size');\n-  }\n-\n-  let differentPixels = 0;",
    "comment": "Fix repo build for M1 MacBooks (#1029)",
    "language": "diff",
    "repo": "trekhleb/javascript-algorithms",
    "sha": "14c563619c300145de3d07d8e67024dc05869606",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/algorithms/sets/longest-common-subsequence/__test__/longestCommonSubsequenceRecursive.test.js (added)\n+import longestCommonSubsequence from '../longestCommonSubsequenceRecursive';\n+\n+describe('longestCommonSubsequenceRecursive', () => {\n+  it('should find longest common substring between two strings', () => {\n+    expect(longestCommonSubsequence('', '')).toBe('');\n+    expect(longestCommonSubsequence('ABC', '')).toBe('');\n+    expect(longestCommonSubsequence('', 'ABC')).toBe('');\n+    expect(longestCommonSubsequence('ABABC', 'BABCA')).toBe('BABC');\n+    expect(longestCommonSubsequence('BABCA', 'ABCBA')).toBe('ABCA');\n+    expect(longestCommonSubsequence('sea', 'eat')).toBe('ea');\n+    expect(longestCommonSubsequence('algorithms', 'rithm')).toBe('rithm');\n+    expect(longestCommonSubsequence(\n+      'Algorithms and data structures implemented in JavaScript',\n+      'Here you may find Algorithms and data structures that are implemented in JavaScript',\n+    )).toBe('Algorithms and data structures implemented in JavaScript');\n+  });\n+});\n\nFile: src/algorithms/sets/longest-common-subsequence/longestCommonSubsequenceRecursive.js (renamed)\n  /* eslint-disable no-param-reassign */\n  /**\n- * Longest Common Substring (LCS) (Recursive Approach).\n+ * Longest Common Subsequence (LCS) (Recursive Approach).\n   *\n   * @param {string} string1\n   * @param {string} string2\n   * @return {number}\n   */\n-export default function longestCommonSubstringRecursive(string1, string2) {\n+export default function longestCommonSubsequenceRecursive(string1, string2) {\n    /**\n     *\n     * @param {string} s1\n     * @param {string} s2\n-   * @return {string} - returns the LCS (Longest Common Substring)\n+   * @return {string} - returns the LCS (Longest Common Subsequence)\n     */\n    const lcs = (s1, s2, memo = {}) => {\n      if (!s1 || !s2) return '';\n\nFile: src/algorithms/string/longest-common-substring/__test__/longestCommonSubstringRecursive.test.js (removed)\n-import longestCommonSubstring from '../longestCommonSubstringRecursive';\n-\n-describe('longestCommonSubstringRecursive', () => {\n-  it('should find longest common substring between two strings', () => {\n-    expect(longestCommonSubstring('', '')).toBe('');\n-    expect(longestCommonSubstring('ABC', '')).toBe('');\n-    expect(longestCommonSubstring('', 'ABC')).toBe('');\n-    expect(longestCommonSubstring('ABABC', 'BABCA')).toBe('BABC');\n-    expect(longestCommonSubstring('BABCA', 'ABCBA')).toBe('ABCA');\n-    expect(longestCommonSubstring('sea', 'eat')).toBe('ea');\n-    expect(longestCommonSubstring('algorithms', 'rithm')).toBe('rithm');\n-    expect(longestCommonSubstring(\n-      'Algorithms and data structures implemented in JavaScript',\n-      'Here you may find Algorithms and data structures that are implemented in JavaScript',\n-    )).toBe('Algorithms and data structures implemented in JavaScript');\n-  });\n-});",
    "comment": "Add a recursive version of the Longest Common Subsequence.",
    "language": "diff",
    "repo": "trekhleb/javascript-algorithms",
    "sha": "5fc33c0f0ae5c6ccddaf8c40a8af483fb750a3ad",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/algorithms/string/longest-common-substring/__test__/longestCommonSubstring.test.js (modified)\n      expect(longestCommonSubstring('', 'ABC')).toBe('');\n      expect(longestCommonSubstring('ABABC', 'BABCA')).toBe('BABC');\n      expect(longestCommonSubstring('BABCA', 'ABCBA')).toBe('ABC');\n+    expect(longestCommonSubstring('sea', 'eat')).toBe('ea');\n+    expect(longestCommonSubstring('algorithms', 'rithm')).toBe('rithm');\n      expect(longestCommonSubstring(\n        'Algorithms and data structures implemented in JavaScript',\n        'Here you may find Algorithms and data structures that are implemented in JavaScript',\n\nFile: src/algorithms/string/longest-common-substring/__test__/longestCommonSubstringRecursive.test.js (added)\n+import longestCommonSubstring from '../longestCommonSubstringRecursive';\n+\n+describe('longestCommonSubstringRecursive', () => {\n+  it('should find longest common substring between two strings', () => {\n+    expect(longestCommonSubstring('', '')).toBe('');\n+    expect(longestCommonSubstring('ABC', '')).toBe('');\n+    expect(longestCommonSubstring('', 'ABC')).toBe('');\n+    expect(longestCommonSubstring('ABABC', 'BABCA')).toBe('BABC');\n+    expect(longestCommonSubstring('BABCA', 'ABCBA')).toBe('ABCA');\n+    expect(longestCommonSubstring('sea', 'eat')).toBe('ea');\n+    expect(longestCommonSubstring('algorithms', 'rithm')).toBe('rithm');\n+    expect(longestCommonSubstring(\n+      'Algorithms and data structures implemented in JavaScript',\n+      'Here you may find Algorithms and data structures that are implemented in JavaScript',\n+    )).toBe('Algorithms and data structures implemented in JavaScript');\n+  });\n+});\n\nFile: src/algorithms/string/longest-common-substring/longestCommonSubstring.js (modified)\n  /**\n+ * Longest Common Substring (LCS) (Dynamic Programming Approach).\n+ *\n   * @param {string} string1\n   * @param {string} string2\n   * @return {string}\n\nFile: src/algorithms/string/longest-common-substring/longestCommonSubstringRecursive.js (added)\n+/* eslint-disable no-param-reassign */\n+/**\n+ * Longest Common Substring (LCS) (Recursive Approach).\n+ *\n+ * @param {string} string1\n+ * @param {string} string2\n+ * @return {number}\n+ */\n+export default function longestCommonSubstringRecursive(string1, string2) {\n+  /**\n+   *\n+   * @param {string} s1\n+   * @param {string} s2\n+   * @return {string} - returns the LCS (Longest Common Substring)\n+   */\n+  const lcs = (s1, s2, memo = {}) => {\n+    if (!s1 || !s2) return '';\n+\n+    if (memo[`${s1}:${s2}`]) return memo[`${s1}:${s2}`];\n+",
    "comment": "Add a recursive version of the Longest Common Substring.",
    "language": "diff",
    "repo": "trekhleb/javascript-algorithms",
    "sha": "c9f1caf1ca87cfa23a5a12046a8359f93f337971",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/data-structures/lru-cache/LRUCache.js (modified)\n   * Implementation of the LRU (Least Recently Used) Cache\n   * based on the HashMap and Doubly Linked List data-structures.\n   *\n- * Current implementation allows to have fast (O(1)) read and write operations.\n+ * Current implementation allows to have fast O(1) (in average) read and write operations.\n   *\n   * At any moment in time the LRU Cache holds not more that \"capacity\" number of items in it.\n   */\n    /**\n     * Returns the cached value by its key.\n-   * Time complexity: O(1).\n+   * Time complexity: O(1) in average.\n     * @param {string} key\n     * @returns {any}\n     */\n    /**\n     * Sets the value to cache by its key.\n-   * Time complexity: O(1).\n+   * Time complexity: O(1) in average.\n     * @param {string} key\n\nFile: src/data-structures/lru-cache/LRUCacheOnMap.js (added)\n+/* eslint-disable no-restricted-syntax, no-unreachable-loop */\n+\n+/**\n+ * Implementation of the LRU (Least Recently Used) Cache\n+ * based on the (ordered) Map data-structure.\n+ *\n+ * Current implementation allows to have fast O(1) (in average) read and write operations.\n+ *\n+ * At any moment in time the LRU Cache holds not more that \"capacity\" number of items in it.\n+ */\n+class LRUCacheOnMap {\n+  /**\n+   * Creates a cache instance of a specific capacity.\n+   * @param {number} capacity\n+   */\n+  constructor(capacity) {\n+    this.capacity = capacity; // How many items to store in cache at max.\n+    this.items = new Map(); // The ordered hash map of all cached items.\n+  }\n+",
    "comment": "Add an example of the LRU Cache based on the Map.",
    "language": "diff",
    "repo": "trekhleb/javascript-algorithms",
    "sha": "4b4d77071cee9c6ba50390ec3635a9755828daa3",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: tensorflow/python/compat/compat.py (modified)\n  # This value changes every day with an automatic CL. It can be modified in code\n  # via `forward_compatibility_horizon()` or with the environment variable\n  # TF_FORWARD_COMPATIBILITY_DELTA_DAYS, which is added to the compatibility date.\n-_FORWARD_COMPATIBILITY_HORIZON = datetime.date(2025, 7, 24)\n+_FORWARD_COMPATIBILITY_HORIZON = datetime.date(2025, 7, 25)\n  _FORWARD_COMPATIBILITY_DELTA_DAYS_VAR_NAME = \"TF_FORWARD_COMPATIBILITY_DELTA_DAYS\"\n  _FORWARD_COMPATIBILITY_DATE_NUMBER = None",
    "comment": "compat: Update forward compatibility horizon to 2025-07-25",
    "language": "diff",
    "repo": "tensorflow/tensorflow",
    "sha": "c9c213e91fd80daa2a18de72fc4435334a3eafda",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: modules/sd_samplers_kdiffusion.py (modified)\n              if scheduler.need_inner_model:\r\n                  sigmas_kwargs['inner_model'] = self.model_wrap\r\n+            \r\n+            if scheduler.label == 'Beta':\r\n+                p.extra_generation_params[\"Beta schedule alpha\"] = opts.beta_dist_alpha\r\n+                p.extra_generation_params[\"Beta schedule beta\"] = opts.beta_dist_beta\r\n              sigmas = scheduler.function(n=steps, **sigmas_kwargs, device=devices.cpu)\r",
    "comment": "always add alpha/beta to extra_generation_params when schedule is Beta",
    "language": "diff",
    "repo": "AUTOMATIC1111/stable-diffusion-webui",
    "sha": "9de7084884f4266a19e4ed3f503687927c845c4d",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: scripts/xyz_grid.py (modified)\n      AxisOption(\"Schedule min sigma\", float, apply_override(\"sigma_min\")),\r\n      AxisOption(\"Schedule max sigma\", float, apply_override(\"sigma_max\")),\r\n      AxisOption(\"Schedule rho\", float, apply_override(\"rho\")),\r\n+    AxisOption(\"Beta schedule alpha\", float, apply_override(\"beta_dist_alpha\")),\r\n+    AxisOption(\"Beta schedule beta\", float, apply_override(\"beta_dist_beta\")),\r\n      AxisOption(\"Eta\", float, apply_field(\"eta\")),\r\n      AxisOption(\"Clip skip\", int, apply_override('CLIP_stop_at_last_layers')),\r\n      AxisOption(\"Denoising\", float, apply_field(\"denoising_strength\")),\r",
    "comment": "add beta schedule opts to xyz options",
    "language": "diff",
    "repo": "AUTOMATIC1111/stable-diffusion-webui",
    "sha": "e285af6e4817a34c86212d1b640aad7225f0c022",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: modules/shared_options.py (modified)\n      'uni_pc_lower_order_final': OptionInfo(True, \"UniPC lower order final\", infotext='UniPC lower order final'),\r\n      'sd_noise_schedule': OptionInfo(\"Default\", \"Noise schedule for sampling\", gr.Radio, {\"choices\": [\"Default\", \"Zero Terminal SNR\"]}, infotext=\"Noise Schedule\").info(\"for use with zero terminal SNR trained models\"),\r\n      'skip_early_cond': OptionInfo(0.0, \"Ignore negative prompt during early sampling\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.0, \"step\": 0.01}, infotext=\"Skip Early CFG\").info(\"disables CFG on a proportion of steps at the beginning of generation; 0=skip none; 1=skip all; can both improve sample diversity/quality and speed up sampling\"),\r\n+    'beta_dist_alpha': OptionInfo(0.6, \"Beta scheduler - alpha\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.0, \"step\": 0.01}, infotext='Beta scheduler alpha').info('Default = 0.6; the alpha parameter of the beta distribution used in Beta sampling'),\r\n+    'beta_dist_beta': OptionInfo(0.6, \"Beta scheduler - beta\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.0, \"step\": 0.01}, infotext='Beta scheduler beta').info('Default = 0.6; the beta parameter of the beta distribution used in Beta sampling'),\r\n  }))\r\n  options_templates.update(options_section(('postprocessing', \"Postprocessing\", \"postprocessing\"), {\r",
    "comment": "add new options 'beta_dist_alpha', 'beta_dist_beta'",
    "language": "diff",
    "repo": "AUTOMATIC1111/stable-diffusion-webui",
    "sha": "3a5a66775c4c9dd03ffbf3c1696ae0db64a71793",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: extensions-builtin/Lora/networks.py (modified)\n      if weights_backup is not None:\r\n          if isinstance(self, torch.nn.MultiheadAttention):\r\n              restore_weights_backup(self, 'in_proj_weight', weights_backup[0])\r\n-            restore_weights_backup(self.out_proj, 'weight', weights_backup[0])\r\n+            restore_weights_backup(self.out_proj, 'weight', weights_backup[1])\r\n          else:\r\n              restore_weights_backup(self, 'weight', weights_backup)\r\n      bias_backup = getattr(self, \"network_bias_backup\", None)\r\n      if bias_backup is None and wanted_names != ():\r\n          if isinstance(self, torch.nn.MultiheadAttention) and self.out_proj.bias is not None:\r\n-            bias_backup = store_weights_backup(self.out_proj)\r\n+            bias_backup = store_weights_backup(self.out_proj.bias)\r\n          elif getattr(self, 'bias', None) is not None:\r\n              bias_backup = store_weights_backup(self.bias)\r\n          else:\r",
    "comment": "fix bugs in lora support",
    "language": "diff",
    "repo": "AUTOMATIC1111/stable-diffusion-webui",
    "sha": "2b50233f3ffa522d5183bacaee3411b9382cbe2c",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: modules/api/api.py (modified)\n              image.save(output_bytes, format=\"PNG\", pnginfo=(metadata if use_metadata else None), quality=opts.jpeg_quality)\n          elif opts.samples_format.lower() in (\"jpg\", \"jpeg\", \"webp\"):\n-            if image.mode == \"RGBA\":\n+            if image.mode in (\"RGBA\", \"P\"):\n                  image = image.convert(\"RGB\")\n              parameters = image.info.get('parameters', None)\n              exif_bytes = piexif.dump({\n\nFile: modules/shared_state.py (modified)\n              errors.record_exception()\r\n      def assign_current_image(self, image):\r\n-        if shared.opts.live_previews_image_format == 'jpeg' and image.mode == 'RGBA':\r\n+        if shared.opts.live_previews_image_format == 'jpeg' and image.mode in ('RGBA', 'P'):\r\n              image = image.convert('RGB')\r\n          self.current_image = image\r\n          self.id_live_preview += 1\r",
    "comment": "fix OSError: cannot write mode P as JPEG",
    "language": "diff",
    "repo": "AUTOMATIC1111/stable-diffusion-webui",
    "sha": "3d2dbefcde4091ce4e6d915b3eda16ca964097f2",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: scripts/xyz_grid.py (modified)\n  def find_vae(name: str):\r\n-    if name := name.strip().lower() in ('auto', 'automatic'):\r\n+    if (name := name.strip().lower()) in ('auto', 'automatic'):\r\n          return 'Automatic'\r\n      elif name == 'none':\r\n          return 'None'\r",
    "comment": "fix #16169 Py 3.9 compatibility",
    "language": "diff",
    "repo": "AUTOMATIC1111/stable-diffusion-webui",
    "sha": "b1695c1b68f0e52cfe8dc4b9ed28228bd3710336",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/gemma3n/configuration_gemma3n.py (modified)\n          if layer_types is None:\n              self.layer_types = [\n-                \"full_attention\" if i % 5 == 0 else \"sliding_attention\" for i in range(self.num_hidden_layers)\n+                \"full_attention\" if (i + 1) % 5 == 0 else \"sliding_attention\" for i in range(self.num_hidden_layers)\n              ]\n          else:\n              self.layer_types = layer_types\n\nFile: src/transformers/models/gemma3n/convert_gemma3n_weights.py (modified)\n  python src/transformers/models/gemma3n/convert_gemma3n_weights.py \\\n      --variant='gemma3n_e4b' \\\n-    --tokenizer_path=\"$HOME/nano3/checkpoints/tokenizer/gemma-3n-tokenizer.model\" \\\n-    --checkpoint_path=\"$HOME/nano3/checkpoints/g251_orbax/\" \\\n-    --output_path=\"$HOME/nano3/checkpoints/g251_vision_encoder/\"\n+    --tokenizer_path=\"$HOME/tokenizers/gemma-3n-tokenizer.model\" \\\n+    --checkpoint_path=\"$HOME/checkpoints/gemma-3n-orbax/\" \\\n+    --output_path=\"$HOME/checkpoints/gemma-3n-safetensors/\"\n  \"\"\"\n  import json\n              converted_weight = weights\n      elif _MOBILE_NET_CONV in path:\n          if \"Conv_0\" in path:\n-            converted_path = \"conv_stem.conv.weight\"\n-            converted_weight = weights.transpose(3, 2, 1, 0)\n+            converted_path = (\"conv_stem.conv.weight\", \"conv_stem.conv.bias\")\n+            converted_weight = weights.transpose(3, 2, 0, 1)\n+            converted_weight = (converted_weight, np.zeros(converted_weight.shape[0]))\n          elif \"Normalize_0\" in path:\n              converted_path = \"conv_stem.bn.weight\"\n\nFile: src/transformers/models/gemma3n/modular_gemma3n.py (modified)\n          if layer_types is None:\n              self.layer_types = [\n-                \"full_attention\" if i % 5 == 0 else \"sliding_attention\" for i in range(self.num_hidden_layers)\n+                \"full_attention\" if (i + 1) % 5 == 0 else \"sliding_attention\" for i in range(self.num_hidden_layers)\n              ]\n          else:\n              self.layer_types = layer_types",
    "comment": "fix: HWIO to OIHW (#39200)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "d913b39ef391895834e422afe7bd4d31a9196a0d",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/cache_utils.py (modified)\n      See `CacheLayerMixin` for details on common methods that are implemented by all cache layers.\n      \"\"\"\n+    is_sliding = False\n+\n      def update(\n          self,\n          key_states: torch.Tensor,\n      See `CacheLayerMixin` for details on common methods that are implemented by all cache layers.\n      \"\"\"\n+    is_sliding = True\n+\n      def __init__(self, sliding_window, *args, **kwargs):\n          \"\"\"\n          Args:",
    "comment": "Add missing flag for CacheLayer (#39678)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "a98bbc294ceadec160c7e2b7ec57054e2e49ea54",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: tests/models/cohere2/test_modeling_cohere2.py (modified)\n          tokenizer = AutoTokenizer.from_pretrained(model_id, pad_token=\"<PAD>\", padding_side=\"right\")\n          # Load model\n-        device = torch_device\n+        device = \"cpu\"  # TODO (joao / export experts): should be on `torch_device`, but causes GPU OOM\n          dtype = torch.bfloat16\n          cache_implementation = \"static\"\n          attn_implementation = \"sdpa\"\n\nFile: tests/models/gemma/test_modeling_gemma.py (modified)\n          ].shape[-1]\n          # Load model\n-        device = torch_device\n+        device = \"cpu\"  # TODO (joao / export experts): should be on `torch_device`, but causes GPU OOM\n          dtype = torch.bfloat16\n          cache_implementation = \"static\"\n          attn_implementation = \"sdpa\"\n\nFile: tests/models/gemma2/test_modeling_gemma2.py (modified)\n          ].shape[-1]\n          # Load model\n-        device = torch_device\n+        device = \"cpu\"  # TODO (joao / export experts): should be on `torch_device`, but causes GPU OOM\n          dtype = torch.bfloat16\n          cache_implementation = \"static\"\n          attn_implementation = \"sdpa\"\n\nFile: tests/models/llama/test_modeling_llama.py (modified)\n              ].shape[-1]\n              # Load model\n-            device = torch_device\n+            device = \"cpu\"  # TODO (joao / export experts): should be on `torch_device`, but causes GPU OOM\n              dtype = torch.bfloat16\n              cache_implementation = \"static\"\n              attn_implementation = \"sdpa\"\n\nFile: tests/models/olmo/test_modeling_olmo.py (modified)\n          ].shape[-1]\n          # Load model\n-        device = torch_device\n+        device = \"cpu\"  # TODO (joao / export experts): should be on `torch_device`, but causes GPU OOM\n          dtype = torch.bfloat16\n          cache_implementation = \"static\"\n          attn_implementation = \"sdpa\"",
    "comment": "[CI] revert device in `test_export_static_cache` (#39662)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "5d0ba3e479839f3a385799cecc3cf42b4e970797",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py (modified)\n  @auto_docstring\n  class ModernBertDecoderPreTrainedModel(ModernBertPreTrainedModel):\n-    config_class: ModernBertDecoderConfig\n+    config: ModernBertDecoderConfig\n      base_model_prefix = \"model\"\n      _skip_keys_device_placement = [\"past_key_values\"]\n      _no_split_modules = [\"ModernBertDecoderLayer\"]\n\nFile: src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py (modified)\n  @auto_docstring\n  class ModernBertDecoderPreTrainedModel(ModernBertPreTrainedModel):\n-    config_class: ModernBertDecoderConfig\n+    config: ModernBertDecoderConfig\n      base_model_prefix = \"model\"\n      _skip_keys_device_placement = [\"past_key_values\"]\n      _no_split_modules = [\"ModernBertDecoderLayer\"]",
    "comment": "Fix ModernBERT Decoder model (#39671)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "850bdeaa95a945088e049f162ec8aef45d48db50",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: tests/models/efficientloftr/test_modeling_efficientloftr.py (modified)\n      @slow\n      def test_model_from_pretrained(self):\n-        from_pretrained_ids = [\"stevenbucaille/efficientloftr\"]\n+        from_pretrained_ids = [\"zju-community/efficientloftr\"]\n          for model_name in from_pretrained_ids:\n              model = EfficientLoFTRForKeypointMatching.from_pretrained(model_name)\n              self.assertIsNotNone(model)\n  class EfficientLoFTRModelIntegrationTest(unittest.TestCase):\n      @cached_property\n      def default_image_processor(self):\n-        return AutoImageProcessor.from_pretrained(\"stevenbucaille/efficientloftr\") if is_vision_available() else None\n+        return AutoImageProcessor.from_pretrained(\"zju-community/efficientloftr\") if is_vision_available() else None\n      @slow\n      def test_inference(self):\n          model = EfficientLoFTRForKeypointMatching.from_pretrained(\n-            \"stevenbucaille/efficientloftr\", attn_implementation=\"eager\"\n+            \"zju-community/efficientloftr\", attn_implementation=\"eager\"\n          ).to(torch_device)\n          preprocessor = self.default_image_processor\n          images = prepare_imgs()",
    "comment": "[efficientloftr] fix model_id in tests (#39621)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "12b612830dc76a3a91d3fe1486b1bcb77b6ac4c4",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/idefics3/image_processing_idefics3.py (modified)\n          max_image_size = images_kwargs.get(\"max_image_size\", None) or self.max_image_size\n          size = images_kwargs.get(\"size\", None) or self.size\n+        num_patches = num_rows = num_cols = 1\n          if do_image_splitting:\n              height, width = _resize_output_size_rescale_to_max_len(height, width, max_len=size[\"longest_edge\"])\n              height, width = _resize_output_size_scale_below_upper_bound(height, width, max_len=4096)\n                  num_cols = math.ceil(resized_width / max_width)\n                  num_patches = num_rows * num_cols + 1\n-        return num_patches\n+        return num_patches, num_rows, num_cols\n  __all__ = [\"Idefics3ImageProcessor\"]\n\nFile: src/transformers/models/idefics3/image_processing_idefics3_fast.py (modified)\n          encoder_dict.pop(\"return_row_col_info\", None)\n          return encoder_dict\n+    def get_number_of_image_patches(self, height: int, width: int, images_kwargs=None):\n+        \"\"\"\n+        A utility that returns number of image patches for a given image size.\n+\n+        Args:\n+            height (`int`):\n+                Height of the input image.\n+            width (`int`):\n+                Width of the input image.\n+            images_kwargs (`dict`, *optional*)\n+                Any kwargs to override defaults of the image processor.\n+        Returns:\n+            `int`: Number of patches per image.\n+        \"\"\"\n+        do_image_splitting = images_kwargs.get(\"do_image_splitting\", None) or self.do_image_splitting\n+        max_image_size = images_kwargs.get(\"max_image_size\", None) or self.max_image_size\n+        size = images_kwargs.get(\"size\", None) or self.size\n+\n\nFile: src/transformers/models/idefics3/processing_idefics3.py (modified)\n  Processor class for Idefics3.\n  \"\"\"\n-import math\n  import re\n  from itertools import accumulate\n  from typing import TYPE_CHECKING, Optional, Union\n              images_kwargs = Idefics3ProcessorKwargs._defaults.get(\"images_kwargs\", {})\n              images_kwargs.update(kwargs)\n-            num_image_patches = [\n+            num_image_row_cols = [\n                  self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)\n                  for image_size in image_sizes\n              ]\n              base_image_length = self.image_seq_len + 3\n              col_length = self.image_seq_len + 2\n              num_image_tokens = []\n+            num_image_patches = []\n-            for num_patches in num_image_patches:\n-                num_cols = num_rows = int(math.sqrt(num_patches - 1))\n+            for num_patches, num_rows, num_cols in num_image_row_cols:\n\nFile: src/transformers/models/smolvlm/image_processing_smolvlm.py (modified)\n          max_image_size = images_kwargs.get(\"max_image_size\", None) or self.max_image_size\n          size = images_kwargs.get(\"size\", None) or self.size\n+        num_patches = num_rows = num_cols = 1\n          if do_image_splitting:\n              height, width = _resize_output_size_rescale_to_max_len(height, width, max_len=size[\"longest_edge\"])\n              height, width = _resize_output_size_scale_below_upper_bound(height, width, max_len=4096)\n                  num_cols = math.ceil(resized_width / max_width)\n                  num_patches = num_rows * num_cols + 1\n-        return num_patches\n+        return num_patches, num_rows, num_cols\n  __all__ = [\"SmolVLMImageProcessor\"]\n\nFile: src/transformers/models/smolvlm/image_processing_smolvlm_fast.py (modified)\n          encoder_dict.pop(\"return_row_col_info\", None)\n          return encoder_dict\n+    def get_number_of_image_patches(self, height: int, width: int, images_kwargs=None):\n+        \"\"\"\n+        A utility that returns number of image patches for a given image size.\n+\n+        Args:\n+            height (`int`):\n+                Height of the input image.\n+            width (`int`):\n+                Width of the input image.\n+            images_kwargs (`dict`, *optional*)\n+                Any kwargs to override defaults of the image processor.\n+        Returns:\n+            `int`: Number of patches per image.\n+        \"\"\"\n+        do_image_splitting = images_kwargs.get(\"do_image_splitting\", None) or self.do_image_splitting\n+        max_image_size = images_kwargs.get(\"max_image_size\", None) or self.max_image_size\n+        size = images_kwargs.get(\"size\", None) or self.size\n+",
    "comment": "[idefics3] fix for vLLM (#39470)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "e7e6efcbbdaaddae9ac732240659dff8fcdce397",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/eslint-config-airbnb-base/whitespace.js (modified)\n    const path = require('path');\n    const { execSync } = require('child_process');\n-  module.exports = JSON.parse(String(execSync(path.join(__dirname, 'whitespace-async.js'))));\n+  // NOTE: ESLint adds runtime statistics to the output (so it's no longer JSON) if TIMING is set\n+  module.exports = JSON.parse(String(execSync(path.join(__dirname, 'whitespace-async.js'), {\n+    env: {\n+      ...process.env,\n+      TIMING: undefined,\n+    }\n+  })));\n  }\n\nFile: packages/eslint-config-airbnb/whitespace.js (modified)\n    const path = require('path');\n    const { execSync } = require('child_process');\n-  module.exports = JSON.parse(String(execSync(path.join(__dirname, 'whitespace-async.js'))));\n+  // NOTE: ESLint adds runtime statistics to the output (so it's no longer JSON) if TIMING is set\n+  module.exports = JSON.parse(String(execSync(path.join(__dirname, 'whitespace-async.js'), {\n+    env: {\n+      ...process.env,\n+      TIMING: undefined,\n+    }\n+  })));\n  }",
    "comment": "[eslint config] [*] [fix] fix crash in eslint invocation with TIMING env set",
    "language": "diff",
    "repo": "airbnb/javascript",
    "sha": "f45bd1ebc0549d370ccd7cb0ca039b40d3be768b",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/eslint-config-airbnb/rules/react.js (modified)\n      // TODO: semver-major, enable\n      'react/jsx-no-leaked-render': 'off',\n-    // https://github.com/jsx-eslint/eslint-plugin-react/blob/66b58dd4864678eb869a7bf434c72ff7ac530eb1/docs/rules/no-object-type-as-default-prop.md\n      // https://github.com/jsx-eslint/eslint-plugin-react/blob/66b58dd4864678eb869a7bf434c72ff7ac530eb1/docs/rules/no-object-type-as-default-prop.md\n      // TODO: semver-major, enable\n      'react/no-object-type-as-default-prop': 'off',\n-    // https://github.com/jsx-eslint/eslint-plugin-react/blob/66b58dd4864678eb869a7bf434c72ff7ac530eb1/docs/rules/sort-default-props.md\n      // https://github.com/jsx-eslint/eslint-plugin-react/blob/66b58dd4864678eb869a7bf434c72ff7ac530eb1/docs/rules/sort-default-props.md\n      // TODO: semver-major, enable?\n      'react/sort-default-props': ['off', {\n        ignoreCase: false\n      }],\n+\n+    // https://github.com/jsx-eslint/eslint-plugin-react/blob/9668ee0762acd5c23f53cd3a372e2d8d9563944d/docs/rules/forward-ref-uses-ref.md\n+    // TODO: semver-major, enable\n+    'react/forward-ref-uses-ref': 'off',\n+\n+    // https://github.com/jsx-eslint/eslint-plugin-react/blob/9668ee0762acd5c23f53cd3a372e2d8d9563944d/docs/rules/jsx-props-no-spread-multi.md\n+    // TODO: semver-major, enable\n+    'react/jsx-props-no-spread-multi': 'off',",
    "comment": "[eslint config] [patch] add new disabled react rules",
    "language": "diff",
    "repo": "airbnb/javascript",
    "sha": "7a8f5687699bad856ca7bb7df0a731f0597a772a",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/eslint-config-airbnb/rules/react.js (modified)\n        ],\n      }],\n+    // This rule enforces onChange or readonly attribute for checked property of input elements.\n+    // https://github.com/jsx-eslint/eslint-plugin-react/blob/master/docs/rules/checked-requires-onchange-or-readonly.md\n+    'react/checked-requires-onchange-or-readonly': ['off', {\n+      ignoreMissingProperties: false,\n+      ignoreExclusiveCheckedAttribute: false\n+    }],\n+\n      // Prevent missing displayName in a React component definition\n      // https://github.com/jsx-eslint/eslint-plugin-react/blob/master/docs/rules/display-name.md\n      'react/display-name': ['off', { ignoreTranspilerName: false }],",
    "comment": "[eslint config] [deps] update `eslint-plugin-react`",
    "language": "diff",
    "repo": "airbnb/javascript",
    "sha": "932951adc2d503d906c85b01179b4e1256c41896",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/eslint-config-airbnb-base/rules/best-practices.js (modified)\n      // https://eslint.org/docs/rules/no-nonoctal-decimal-escape\n      'no-nonoctal-decimal-escape': 'error',\n+    // Disallow calls to the Object constructor without an argument\n+    // https://eslint.org/docs/latest/rules/no-object-constructor\n+    // TODO: enable, semver-major\n+    'no-object-constructor': 'off',\n+\n      // disallow use of (old style) octal literals\n      // https://eslint.org/docs/rules/no-octal\n      'no-octal': 'error',",
    "comment": "[eslint config] [base] add new disabled rules",
    "language": "diff",
    "repo": "airbnb/javascript",
    "sha": "e95b1f2754258cf219ecfdfdf712b6de5657dd8a",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/eslint-config-airbnb-base/rules/variables.js (modified)\n          message:\n            'Use Number.isNaN instead https://github.com/airbnb/javascript#standard-library--isnan',\n        },\n-    ].concat(confusingBrowserGlobals),\n+    ].concat(confusingBrowserGlobals.map((g) => ({\n+      name: g,\n+      message: `Use window.${g} instead. https://github.com/facebook/create-react-app/blob/HEAD/packages/confusing-browser-globals/README.md`,\n+    }))),\n      // disallow declaration of variables already declared in the outer scope\n      'no-shadow': 'error',",
    "comment": "[eslint config] [base] [patch] Add a message for `confusing-browser-globals`",
    "language": "diff",
    "repo": "airbnb/javascript",
    "sha": "11ab37144b7f846f04f64a29b5beb6e00d74e84b",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/eslint-config-airbnb-base/rules/es6.js (modified)\n      // disallow importing from the same path more than once\n      // https://eslint.org/docs/rules/no-duplicate-imports\n-    // replaced by https://github.com/benmosher/eslint-plugin-import/blob/master/docs/rules/no-duplicates.md\n+    // replaced by https://github.com/import-js/eslint-plugin-import/blob/master/docs/rules/no-duplicates.md\n      'no-duplicate-imports': 'off',\n      // disallow symbol constructor\n\nFile: packages/eslint-config-airbnb-base/rules/imports.js (modified)\n      // Static analysis:\n      // ensure imports point to files/modules that can be resolved\n-    // https://github.com/benmosher/eslint-plugin-import/blob/master/docs/rules/no-unresolved.md\n+    // https://github.com/import-js/eslint-plugin-import/blob/master/docs/rules/no-unresolved.md\n      'import/no-unresolved': ['error', { commonjs: true, caseSensitive: true }],\n      // ensure named imports coupled with named exports\n-    // https://github.com/benmosher/eslint-plugin-import/blob/master/docs/rules/named.md#when-not-to-use-it\n+    // https://github.com/import-js/eslint-plugin-import/blob/master/docs/rules/named.md#when-not-to-use-it\n      'import/named': 'error',\n      // ensure default import coupled with default export\n-    // https://github.com/benmosher/eslint-plugin-import/blob/master/docs/rules/default.md#when-not-to-use-it\n+    // https://github.com/import-js/eslint-plugin-import/blob/master/docs/rules/default.md#when-not-to-use-it\n      'import/default': 'off',\n-    // https://github.com/benmosher/eslint-plugin-import/blob/master/docs/rules/namespace.md\n+    // https://github.com/import-js/eslint-plugin-import/blob/master/docs/rules/namespace.md\n      'import/namespace': 'off',\n      // Helpful warnings:\n      // disallow invalid exports, e.g. multiple defaults\n-    // https://github.com/benmosher/eslint-plugin-import/blob/master/docs/rules/export.md\n+    // https://github.com/import-js/eslint-plugin-import/blob/master/docs/rules/export.md\n\nFile: packages/eslint-config-airbnb/rules/react-a11y.js (modified)\n    rules: {\n      // ensure emoji are accessible\n-    // https://github.com/evcohen/eslint-plugin-jsx-a11y/blob/master/docs/rules/accessible-emoji.md\n+    // https://github.com/jsx-eslint/eslint-plugin-jsx-a11y/blob/master/docs/rules/accessible-emoji.md\n      // disabled; rule is deprecated\n      'jsx-a11y/accessible-emoji': 'off',\n      // Enforce that all elements that require alternative text have meaningful information\n-    // https://github.com/evcohen/eslint-plugin-jsx-a11y/blob/master/docs/rules/alt-text.md\n+    // https://github.com/jsx-eslint/eslint-plugin-jsx-a11y/blob/master/docs/rules/alt-text.md\n      'jsx-a11y/alt-text': ['error', {\n        elements: ['img', 'object', 'area', 'input[type=\"image\"]'],\n        img: [],\n      }],\n      // Enforce that anchors have content\n-    // https://github.com/evcohen/eslint-plugin-jsx-a11y/blob/master/docs/rules/anchor-has-content.md\n+    // https://github.com/jsx-eslint/eslint-plugin-jsx-a11y/blob/master/docs/rules/anchor-has-content.md\n      'jsx-a11y/anchor-has-content': ['error', { components: [] }],\n      // ensure <a> tags are valid\n-    // https://github.com/evcohen/eslint-plugin-jsx-a11y/blob/0745af376cdc8686d85a361ce36952b1fb1ccf6e/docs/rules/anchor-is-valid.md\n+    // https://github.com/jsx-eslint/eslint-plugin-jsx-a11y/blob/0745af376cdc8686d85a361ce36952b1fb1ccf6e/docs/rules/anchor-is-valid.md",
    "comment": "[readme] update eslint plugin repo URLs",
    "language": "diff",
    "repo": "airbnb/javascript",
    "sha": "0e627e2fa32e16296e309af314eb3cdaae00de66",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/eslint-config-airbnb/rules/react-a11y.js (modified)\n        },\n        allowChildren: false,\n      }],\n+\n+    // Ensures anchor text is not ambiguous\n+    // https://github.com/jsx-eslint/eslint-plugin-jsx-a11y/blob/93f78856655696a55309440593e0948c6fb96134/docs/rules/anchor-ambiguous-text.md\n+    // TODO: semver-major, enable\n+    'jsx-a11y/anchor-ambiguous-text': 'off',\n+\n+    // Enforce that aria-hidden=\"true\" is not set on focusable elements.\n+    // https://github.com/jsx-eslint/eslint-plugin-jsx-a11y/blob/93f78856655696a55309440593e0948c6fb96134/docs/rules/no-aria-hidden-on-focusable.md\n+    // TODO: semver-major, enable\n+    'jsx-a11y/no-aria-hidden-on-focusable': 'off',\n+\n+    // Enforces using semantic DOM elements over the ARIA role property.\n+    // https://github.com/jsx-eslint/eslint-plugin-jsx-a11y/blob/93f78856655696a55309440593e0948c6fb96134/docs/rules/prefer-tag-over-role.md\n+    // TODO: semver-major, enable\n+    'jsx-a11y/prefer-tag-over-role': 'off',\n    },\n  };\n\nFile: packages/eslint-config-airbnb/rules/react.js (modified)\n      // https://github.com/jsx-eslint/eslint-plugin-react/blob/c42b624d0fb9ad647583a775ab9751091eec066f/docs/rules/jsx-no-leaked-render.md\n      // TODO: semver-major, enable\n      'react/jsx-no-leaked-render': 'off',\n+\n+    // https://github.com/jsx-eslint/eslint-plugin-react/blob/66b58dd4864678eb869a7bf434c72ff7ac530eb1/docs/rules/no-object-type-as-default-prop.md\n+    // https://github.com/jsx-eslint/eslint-plugin-react/blob/66b58dd4864678eb869a7bf434c72ff7ac530eb1/docs/rules/no-object-type-as-default-prop.md\n+    // TODO: semver-major, enable\n+    'react/no-object-type-as-default-prop': 'off',\n+\n+    // https://github.com/jsx-eslint/eslint-plugin-react/blob/66b58dd4864678eb869a7bf434c72ff7ac530eb1/docs/rules/sort-default-props.md\n+    // https://github.com/jsx-eslint/eslint-plugin-react/blob/66b58dd4864678eb869a7bf434c72ff7ac530eb1/docs/rules/sort-default-props.md\n+    // TODO: semver-major, enable?\n+    'react/sort-default-props': ['off', {\n+      ignoreCase: false\n+    }],\n    },\n    settings: {",
    "comment": "[eslint config] [deps] update `eslint-plugin-react`, `eslint-plugin-jsx-a11y`",
    "language": "diff",
    "repo": "airbnb/javascript",
    "sha": "bf536566cee0b7f239e00f105ca422bb95be2d62",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/eslint-config-airbnb-base/rules/imports.js (modified)\n      // Use this rule to prevent importing packages through relative paths.\n      // https://github.com/benmosher/eslint-plugin-import/blob/1012eb951767279ce3b540a4ec4f29236104bb5b/docs/rules/no-relative-packages.md\n      'import/no-relative-packages': 'error',\n+\n+    // enforce a consistent style for type specifiers (inline or top-level)\n+    // https://github.com/import-js/eslint-plugin-import/blob/d5fc8b670dc8e6903dbb7b0894452f60c03089f5/docs/rules/consistent-type-specifier-style.md\n+    // TODO, semver-major: enable (just in case)\n+    'import/consistent-type-specifier-style': ['off', 'prefer-inline'],\n+\n+    // Reports the use of empty named import blocks.\n+    // https://github.com/import-js/eslint-plugin-import/blob/d5fc8b670dc8e6903dbb7b0894452f60c03089f5/docs/rules/no-empty-named-blocks.md\n+    // TODO, semver-minor: enable\n+    'import/no-empty-named-blocks': 'off',\n    },\n  };",
    "comment": "[*] [deps] update `eslint-plugin-import`",
    "language": "diff",
    "repo": "airbnb/javascript",
    "sha": "917c8d92a30db5bc35ddef85ec417f1aad0dc2c1",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: lib/core/AxiosHeaders.js (modified)\n    }\n    getSetCookie() {\n-    return this[\"set-cookie\"] || [];\n+    return this.get(\"set-cookie\") || [];\n    }\n    get [Symbol.toStringTag]() {",
    "comment": "fix(headers): fix `getSetCookie` by using 'get' method for caseless access; (#6874)",
    "language": "diff",
    "repo": "axios/axios",
    "sha": "d4f7df4b304af8b373488fdf8e830793ff843eb9",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: index.d.ts (modified)\n    url?: string;\n    method?: Method | string;\n    baseURL?: string;\n+  allowAbsoluteUrls?: boolean;\n    transformRequest?: AxiosRequestTransformer | AxiosRequestTransformer[];\n    transformResponse?: AxiosResponseTransformer | AxiosResponseTransformer[];\n    headers?: (RawAxiosRequestHeaders & MethodsHeaders) | AxiosHeaders;\n\nFile: test/module/typings/cjs/index.ts (modified)\n  const config: axios.AxiosRequestConfig = {\n    url: '/user',\n    method: 'get',\n+  allowAbsoluteUrls: false,\n    baseURL: 'https://api.example.com/',\n    transformRequest: (data: any) => '{\"foo\":\"bar\"}',\n    transformResponse: [\n\nFile: test/module/typings/esm/index.ts (modified)\n    url: '/user',\n    method: 'get',\n    baseURL: 'https://api.example.com/',\n+  allowAbsoluteUrls: false,\n    transformRequest: (data: any) => '{\"foo\":\"bar\"}',\n    transformResponse: [\n      (data: any) => ({ baz: 'qux' })",
    "comment": "fix: add missing type for allowAbsoluteUrls (#6818)",
    "language": "diff",
    "repo": "axios/axios",
    "sha": "10fa70ef14fe39558b15a179f0e82f5f5e5d11b2",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: inference/kernel.py (modified)\n      Args:\n          x (torch.Tensor): The quantized weight tensor of shape (M, N).\n-        s (torch.Tensor): The scale tensor of shape (M, N).\n+        s (torch.Tensor): The scale tensor of shape (M//block_size, N//block_size).\n          block_size (int, optional): The block size to use for dequantization. Defaults to 128.\n      Returns:",
    "comment": "fix an args description.",
    "language": "diff",
    "repo": "deepseek-ai/DeepSeek-V3",
    "sha": "4a65fd9221103ff03864337453c238e65d1f4a1b",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: inference/model.py (modified)\n          else:\n              self.register_parameter(\"scale\", None)\n          if bias:\n-            self.bias = nn.Parameter(torch.empty(self.part_out_features))\n+            self.bias = nn.Parameter(torch.empty(out_features))\n          else:\n              self.register_parameter(\"bias\", None)",
    "comment": "Fix Linear Layer Bias Initialization",
    "language": "diff",
    "repo": "deepseek-ai/DeepSeek-V3",
    "sha": "6a30b43249a5710a3adb18c11763222d3fca8756",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: inference/convert.py (modified)\n  def main(hf_ckpt_path, save_path, n_experts, mp):\n+    \"\"\"\n+    Converts and saves model checkpoint files into a specified format.\n+\n+    Args:\n+        hf_ckpt_path (str): Path to the directory containing the input checkpoint files.\n+        save_path (str): Path to the directory where the converted checkpoint files will be saved.\n+        n_experts (int): Total number of experts in the model.\n+        mp (int): Model parallelism factor.\n+        \n+    Returns:\n+        None\n+    \"\"\"\n      torch.set_num_threads(8)\n      n_local_experts = n_experts // mp\n      state_dicts = [{} for _ in range(mp)]\n\nFile: inference/fp8_cast_bf16.py (modified)\n  from kernel import weight_dequant\n  def main(fp8_path, bf16_path):\n+    \"\"\"\n+    Converts FP8 weights to BF16 and saves the converted weights.\n+\n+    This function reads FP8 weights from the specified directory, converts them to BF16,\n+    and saves the converted weights to another specified directory. It also updates the\n+    model index file to reflect the changes.\n+\n+    Args:\n+    fp8_path (str): The path to the directory containing the FP8 weights and model index file.\n+    bf16_path (str): The path to the directory where the converted BF16 weights will be saved.\n+\n+    Raises:\n+    KeyError: If a required scale_inv tensor is missing for a weight.\n+\n+    Notes:\n+    - The function assumes that the FP8 weights are stored in safetensor files.\n+    - The function caches loaded safetensor files to optimize memory usage.\n+    - The function updates the model index file to remove references to scale_inv tensors.\n\nFile: inference/generate.py (modified)\n  def sample(logits, temperature: float = 1.0):\n+    \"\"\"\n+    Samples a token from the logits using temperature scaling.\n+\n+    Args:\n+        logits (torch.Tensor): The logits tensor for token predictions.\n+        temperature (float, optional): Temperature for scaling logits. Defaults to 1.0.\n+\n+    Returns:\n+        torch.Tensor: The sampled token.\n+    \"\"\"\n      logits = logits / max(temperature, 1e-5)\n      probs = torch.softmax(logits, dim=-1)\n      return probs.div_(torch.empty_like(probs).exponential_(1)).argmax(dim=-1)\n      eos_id: int,\n      temperature: float = 1.0\n  ) -> List[List[int]]:\n+    \"\"\"\n+    Generates new tokens based on the given prompt tokens using the specified model.\n+\n\nFile: inference/kernel.py (modified)\n  @triton.jit\n  def act_quant_kernel(x_ptr, y_ptr, s_ptr, BLOCK_SIZE: tl.constexpr):\n+    \"\"\"\n+    Quantizes the input tensor `x_ptr` and stores the result in `y_ptr` and the scaling factor in `s_ptr`.\n+\n+    Args:\n+        x_ptr (triton.Pointer): Pointer to the input tensor.\n+        y_ptr (triton.Pointer): Pointer to the output tensor where quantized values will be stored.\n+        s_ptr (triton.Pointer): Pointer to the output tensor where scaling factors will be stored.\n+        BLOCK_SIZE (tl.constexpr): The size of the block to be processed by each program instance.\n+\n+    Returns:\n+        None\n+    \"\"\"\n      pid = tl.program_id(axis=0)\n      offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n      x = tl.load(x_ptr + offs).to(tl.float32)\n  def act_quant(x: torch.Tensor, block_size: int = 128) -> Tuple[torch.Tensor, torch.Tensor]:\n+    \"\"\"\n+    Quantizes the input tensor `x` using block-wise quantization.",
    "comment": "Enhance documentation and update .gitignore for model conversion scripts",
    "language": "diff",
    "repo": "deepseek-ai/DeepSeek-V3",
    "sha": "a1296f099e2c05699d4f670de1cb147fc7444dc8",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: tests/rules/test_ssh_known_host.py (modified)\n  import os\n  import pytest\n-from thefuck.rules.ssh_known_hosts import match, get_new_command,\\\n+from thefuck.rules.ssh_known_hosts import match, get_new_command, \\\n      side_effect\n  from thefuck.types import Command\n\nFile: tests/test_readme.py (modified)\n          for rule in bundled:\n              if rule.stem != '__init__':\n-                assert rule.stem in readme,\\\n+                assert rule.stem in readme, \\\n                      'Missing rule \"{}\" in README.md'.format(rule.stem)",
    "comment": "#N/A: Fix a couple of issues after new flake8 release",
    "language": "diff",
    "repo": "nvbn/thefuck",
    "sha": "62e0767c5069aeee176b0fe3459068b7703aaa26",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: setup.py (modified)\n                                        'tests', 'tests.*', 'release']),\n        include_package_data=True,\n        zip_safe=False,\n+      python_requires='>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*',\n        install_requires=install_requires,\n        extras_require=extras_require,\n        entry_points={'console_scripts': [",
    "comment": "Add python_requires to help pip",
    "language": "diff",
    "repo": "nvbn/thefuck",
    "sha": "3f71959b1ba9e8359cc0e42226706a742e82ab8b",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: scripts/people.py (modified)\n      github_token: SecretStr\n      github_repository: str\n      httpx_timeout: int = 30\n+    sleep_interval: int = 5\n  def get_graphql_response(\n              discussion_nodes.append(discussion_edge.node)\n          last_edge = discussion_edges[-1]\n          # Handle GitHub secondary rate limits, requests per minute\n-        time.sleep(5)\n+        time.sleep(settings.sleep_interval)\n          discussion_edges = get_graphql_question_discussion_edges(\n              settings=settings, after=last_edge.cursor\n          )",
    "comment": "🔨 Update FastAPI People sleep interval, use external settings (#13888)",
    "language": "diff",
    "repo": "fastapi/fastapi",
    "sha": "ce26b8e1caa3d375dba527612717c3279a290b3d",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: fastapi/dependencies/utils.py (modified)\n      return values, errors\n+def is_union_of_base_models(field_type: Any) -> bool:\n+    \"\"\"Check if field type is a Union where all members are BaseModel subclasses.\"\"\"\n+    from fastapi.types import UnionType\n+\n+    origin = get_origin(field_type)\n+\n+    # Check if it's a Union type (covers both typing.Union and types.UnionType in Python 3.10+)\n+    if origin is not Union and origin is not UnionType:\n+        return False\n+\n+    union_args = get_args(field_type)\n+\n+    for arg in union_args:\n+        if not lenient_issubclass(arg, BaseModel):\n+            return False\n+\n+    return True\n+\n+",
    "comment": "🐛 Fix support for unions when using `Form` (#13827)",
    "language": "diff",
    "repo": "fastapi/fastapi",
    "sha": "9d0d8828cc6b3f0217581d7e607ba8e4d7e0017b",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: whisper/__init__.py (modified)\n      with (\n          io.BytesIO(checkpoint_file) if in_memory else open(checkpoint_file, \"rb\")\n      ) as fp:\n-        checkpoint = torch.load(fp, map_location=device)\n+        kwargs = {\"weights_only\": True} if torch.__version__ >= \"1.13\" else {}\n+        checkpoint = torch.load(fp, map_location=device, **kwargs)\n      del checkpoint_file\n      dims = ModelDimensions(**checkpoint[\"dims\"])",
    "comment": "Fix: Update torch.load to use weights_only=True to prevent security w… (#2451)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "1f8fc975d3f679035f55d9838158ab688e39a82e",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: whisper/model.py (modified)\n  import base64\n  import gzip\n+from contextlib import contextmanager\n  from dataclasses import dataclass\n-from typing import Dict, Iterable, Optional\n+from typing import Dict, Iterable, Optional, Tuple\n  import numpy as np\n  import torch\n  from .decoding import detect_language as detect_language_function\n  from .transcribe import transcribe as transcribe_function\n+try:\n+    from torch.nn.functional import scaled_dot_product_attention\n+\n+    SDPA_AVAILABLE = True\n+except (ImportError, RuntimeError, OSError):\n+    scaled_dot_product_attention = None\n+    SDPA_AVAILABLE = False\n+\n  @dataclass\n  class ModelDimensions:\n\nFile: whisper/timing.py (modified)\n          for i, block in enumerate(model.decoder.blocks)\n      ]\n-    with torch.no_grad():\n+    from .model import disable_sdpa\n+\n+    with torch.no_grad(), disable_sdpa():\n          logits = model(mel.unsqueeze(0), tokens.unsqueeze(0))[0]\n          sampled_logits = logits[len(tokenizer.sot_sequence) :, : tokenizer.eot]\n          token_probs = sampled_logits.softmax(dim=-1)",
    "comment": "using sdpa if available (#2359)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "27f971320a50e65fd510b88be04219a6ade31f9b",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: setup.py (modified)\n-import os\n  import platform\n  import sys\n+from pathlib import Path\n  import pkg_resources\n  from setuptools import find_packages, setup\n      url=\"https://github.com/openai/whisper\",\n      license=\"MIT\",\n      packages=find_packages(exclude=[\"tests*\"]),\n-    install_requires=requirements\n-    + [\n+    install_requires=[\n          str(r)\n          for r in pkg_resources.parse_requirements(\n-            open(os.path.join(os.path.dirname(__file__), \"requirements.txt\"))\n+            Path(__file__).with_name(\"requirements.txt\").open()\n          )\n      ],\n      entry_points={",
    "comment": "Fix triton env marker (#1887)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "8bc8860694949db53c42ba47ddc23786c2e02a8b",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: coding/python/merge_sort.py (added)\n+#!/usr/bin/env python\n+\n+import random\n+from typing import List\n+\n+\n+def merge_sort(arr: List[int]) -> List[int]:\n+    if len(arr) <= 1:\n+        return arr\n+    mid = len(arr) // 2\n+    left = merge_sort(arr[:mid])\n+    right = merge_sort(arr[mid:])\n+    return merge(left, right)\n+\n+\n+def merge(left: List[int], right: List[int]) -> List[int]:\n+    merged = []\n+    i = j = 0\n+    while i < len(left) and j < len(right):\n+        if left[i] <= right[j]:",
    "comment": "Implemented recursive merge sort algorithm with O(n log n) time complexity. (#10553)",
    "language": "diff",
    "repo": "bregman-arie/devops-exercises",
    "sha": "692a680ccb80f7b836861c32e8ce880d011598a2",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/app.test.ts (modified)\n  import { Data } from './service.js'\n  type Test = {\n-  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n+   \n    method: HTTPMethods\n    url: string\n    statusCode: number\n\nFile: src/app.ts (modified)\n      .forEach((dir) => app.use(sirv(dir, { dev: !isProduction })))\n    // CORS\n-  app.use(cors()).options('*', cors())\n+  app\n+    .use((req, res, next) => {\n+      return cors({\n+        allowedHeaders: req.headers['access-control-request-headers']\n+          ?.split(',')\n+          .map((h) => h.trim()),\n+      })(req, res, next)\n+    })\n+    .options('*', cors())\n    // Body parser\n+  // @ts-expect-error expected\n    app.use(json())\n    app.get('/', (_req, res) =>\n    app.get('/:name', (req, res, next) => {\n      const { name = '' } = req.params\n-    const query = Object.fromEntries(Object.entries(req.query)\n-      .map(([key, value]) => {\n\nFile: src/service.ts (modified)\n      const item = items.find((item) => item['id'] === id)\n      if (item === undefined) return\n      const index = items.indexOf(item)\n-    items.splice(index, 1)[0]\n+    items.splice(index, 1)\n      nullifyForeignKey(this.#db, name, id)\n      const dependents = ensureArray(dependent)",
    "comment": "Update deps and allow any header in CORS (#1611)",
    "language": "diff",
    "repo": "typicode/json-server",
    "sha": "780e246c5429837aad9ec5a63c4c5a9d2efbb025",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: d2l/torch.py (modified)\n      d2l.set_figsize()\n      d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e')\n      x1, x2 = d2l.meshgrid(d2l.arange(-5.5, 1.0, 0.1),\n-                          d2l.arange(-3.0, 1.0, 0.1))\n+                          d2l.arange(-3.0, 1.0, 0.1), indexing='ij')\n      d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\n      d2l.plt.xlabel('x1')\n      d2l.plt.ylabel('x2')",
    "comment": "PyTorch: Fix meshgrid indexing",
    "language": "diff",
    "repo": "d2l-ai/d2l-zh",
    "sha": "042c11c11c8dbfbc0b73da0a0a6cec8441f3710b",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: frontend/src/components/preview/PreviewPane.tsx (modified)\n                    variant=\"secondary\"\n                    className=\"flex items-center gap-x-2 mr-4 dark:text-white dark:bg-gray-700 download-btn\"\n                  >\n-                  <FaDownload /> Download\n+                  <FaDownload /> Download Code\n                  </Button>\n                </>\n              )}",
    "comment": "feat: improve download button clarity by changing text to \"Download Code\"",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "7ef35a910f6f1aad1d950f4017ab28986e2bf129",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: backend/routes/generate_code.py (modified)\n          \"\"\"Send a message to the client with debug logging\"\"\"\n          # Print for debugging on the backend\n          if type == \"error\":\n-            print(f\"Error (variant {variantIndex}): {value}\")\n+            print(f\"Error (variant {variantIndex + 1}): {value}\")\n          elif type == \"status\":\n-            print(f\"Status (variant {variantIndex}): {value}\")\n+            print(f\"Status (variant {variantIndex + 1}): {value}\")\n          elif type == \"variantComplete\":\n-            print(f\"Variant {variantIndex} complete\")\n+            print(f\"Variant {variantIndex + 1} complete\")\n          elif type == \"variantError\":\n-            print(f\"Variant {variantIndex} error: {value}\")\n+            print(f\"Variant {variantIndex + 1} error: {value}\")\n          await self.websocket.send_json(\n              {\"type\": type, \"value\": value, \"variantIndex\": variantIndex}\n              # Print the variant models (one per line)\n              print(\"Variant models:\")\n              for index, model in enumerate(variant_models):\n-                print(f\"Variant {index}: {model.value}\")",
    "comment": "feat: change variant display to 1-indexed for better user experience",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "126ed0fb84a0d64638ef5087bb5a35f84bab9132",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: backend/prompts/__init__.py (modified)\n      prompt: PromptContent,\n      history: list[dict[str, Any]],\n      is_imported_from_code: bool,\n-    result_image: str | None,\n  ) -> tuple[list[ChatCompletionMessageParam], dict[str, str]]:\n      image_cache: dict[str, str] = {}\n          # Assemble the prompt for non-imported code\n          if input_mode == \"image\":\n              image_url = prompt[\"images\"][0]\n-            if result_image:\n-                prompt_messages = assemble_prompt(image_url, stack, result_image)\n-            else:\n-                prompt_messages = assemble_prompt(image_url, stack)\n+            prompt_messages = assemble_prompt(image_url, stack)\n          elif input_mode == \"text\":\n              prompt_messages = assemble_text_prompt(prompt[\"text\"], stack)\n          else:\n              # Default to image mode for backward compatibility\n              image_url = prompt[\"images\"][0]\n-            if result_image:\n\nFile: backend/prompts/test_prompts.py (modified)\n  def test_prompts():\n+    # Test that assemble_prompt creates the expected structure without result_image\n      tailwind_prompt = assemble_prompt(\n-        \"image_data_url\", \"html_tailwind\", \"result_image_data_url\"\n+        \"image_data_url\", \"html_tailwind\"\n      )\n-    assert tailwind_prompt[0].get(\"content\") == TAILWIND_SYSTEM_PROMPT\n-    assert tailwind_prompt[1][\"content\"][2][\"text\"] == USER_PROMPT  # type: ignore\n-\n-    html_css_prompt = assemble_prompt(\n-        \"image_data_url\", \"html_css\", \"result_image_data_url\"\n-    )\n-    assert html_css_prompt[0].get(\"content\") == HTML_CSS_SYSTEM_PROMPT\n-    assert html_css_prompt[1][\"content\"][2][\"text\"] == USER_PROMPT  # type: ignore\n-\n-    react_tailwind_prompt = assemble_prompt(\n-        \"image_data_url\", \"react_tailwind\", \"result_image_data_url\"\n-    )\n-    assert react_tailwind_prompt[0].get(\"content\") == REACT_TAILWIND_SYSTEM_PROMPT\n-    assert react_tailwind_prompt[1][\"content\"][2][\"text\"] == USER_PROMPT  # type: ignore\n\nFile: backend/routes/generate_code.py (modified)\n      prompt: PromptContent\n      history: List[Dict[str, Any]]\n      is_imported_from_code: bool\n-    result_image: str | None\n  class ParameterExtractionStage:\n          # Extract imported code flag\n          is_imported_from_code = params.get(\"isImportedFromCode\", False)\n-        \n-        # Extract result image\n-        result_image = params.get(\"resultImage\")\n          return ExtractedParams(\n              stack=validated_stack,\n              prompt=prompt,\n              history=history,\n              is_imported_from_code=is_imported_from_code,\n-            result_image=result_image,\n          )\n      def _get_from_settings_dialog_or_env(\n                  prompt=extracted_params.prompt,\n                  history=extracted_params.history,\n\nFile: backend/tests/test_prompts.py (modified)\n                  prompt=params[\"prompt\"],\n                  history=params.get(\"history\", []),\n                  is_imported_from_code=params.get(\"isImportedFromCode\", False),\n-                result_image=params.get(\"resultImage\"),\n              )\n              # Define expected structure\n              actual: ExpectedResult = {\"messages\": messages, \"image_cache\": image_cache}\n              assert_structure_match(actual, expected)\n-    @pytest.mark.asyncio\n-    async def test_image_mode_create_with_result_image(self) -> None:\n-        \"\"\"Test create generation with before/after images in image mode.\"\"\"\n-        # Setup test data\n-        params: Dict[str, Any] = {\n-            \"prompt\": {\"text\": \"\", \"images\": [self.TEST_IMAGE_URL]},\n-            \"generationType\": \"create\",\n-            \"resultImage\": self.RESULT_IMAGE_URL,\n-        }\n-\n-        # Mock the system prompts\n-        mock_system_prompts: Dict[str, str] = {self.TEST_STACK: self.MOCK_SYSTEM_PROMPT}\n\nFile: backend/tests/test_prompts_additional.py (modified)\n                  prompt=params[\"prompt\"],\n                  history=params.get(\"history\", []),\n                  is_imported_from_code=params.get(\"isImportedFromCode\", False),\n-                result_image=params.get(\"resultImage\"),\n              )\n              # Define expected structure\n                  prompt=params[\"prompt\"],\n                  history=params.get(\"history\", []),\n                  is_imported_from_code=params.get(\"isImportedFromCode\", False),\n-                result_image=params.get(\"resultImage\"),\n              )\n              # Define expected structure - should be text-only messages\n                  prompt=params.get(\"prompt\", {\"text\": \"\", \"images\": []}),\n                  history=params.get(\"history\", []),\n                  is_imported_from_code=params.get(\"isImportedFromCode\", False),\n-                result_image=params.get(\"resultImage\"),\n              )\n              # Define expected structure",
    "comment": "refactor: remove unused result_image functionality",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "774e459c2f43cfe1cdad9378bbb48f372c3503d7",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: backend/tests/test_prompts.py (modified)\n          with patch(\"prompts.SYSTEM_PROMPTS\", mock_system_prompts):\n              # Call the function\n              messages, image_cache = await create_prompt(\n-                params, self.TEST_STACK, \"image\"\n+                stack=self.TEST_STACK,\n+                input_mode=\"image\",\n+                generation_type=params[\"generationType\"],\n+                prompt=params[\"prompt\"],\n+                history=params.get(\"history\", []),\n+                is_imported_from_code=params.get(\"isImportedFromCode\", False),\n+                result_image=params.get(\"resultImage\"),\n              )\n              # Define expected structure\n          with patch(\"prompts.SYSTEM_PROMPTS\", mock_system_prompts):\n              # Call the function\n              messages, image_cache = await create_prompt(\n-                params, self.TEST_STACK, \"image\"\n+                stack=self.TEST_STACK,\n+                input_mode=\"image\",\n+                generation_type=params[\"generationType\"],\n\nFile: backend/tests/test_prompts_additional.py (modified)\n          with patch(\"prompts.SYSTEM_PROMPTS\", mock_system_prompts), \\\n               patch(\"prompts.create_alt_url_mapping\", return_value={\"mock\": \"cache\"}):\n              # Call the function\n-            messages, image_cache = await create_prompt(params, self.TEST_STACK, \"image\")\n+            messages, image_cache = await create_prompt(\n+                stack=self.TEST_STACK,\n+                input_mode=\"image\",\n+                generation_type=params[\"generationType\"],\n+                prompt=params[\"prompt\"],\n+                history=params.get(\"history\", []),\n+                is_imported_from_code=params.get(\"isImportedFromCode\", False),\n+                result_image=params.get(\"resultImage\"),\n+            )\n              # Define expected structure\n              expected: ExpectedResult = {\n          with patch(\"prompts.SYSTEM_PROMPTS\", mock_system_prompts), \\\n               patch(\"prompts.create_alt_url_mapping\", return_value={}):\n              # Call the function\n-            messages, image_cache = await create_prompt(params, self.TEST_STACK, \"image\")\n+            messages, image_cache = await create_prompt(",
    "comment": "fix: update all tests to use new create_prompt function signature",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "bff1a1024430a98760126ce9c7179a9603cb099f",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: backend/prompts/__init__.py (modified)\n  async def create_prompt(\n-    params: dict[str, Any], stack: Stack, input_mode: InputMode\n+    stack: Stack,\n+    input_mode: InputMode,\n+    generation_type: str,\n+    prompt: PromptContent,\n+    history: list[dict[str, Any]],\n+    is_imported_from_code: bool,\n+    result_image: str | None,\n  ) -> tuple[list[ChatCompletionMessageParam], dict[str, str]]:\n      image_cache: dict[str, str] = {}\n      # If this generation started off with imported code, we need to assemble the prompt differently\n-    if params.get(\"isImportedFromCode\"):\n-        original_imported_code = params[\"history\"][0][\"text\"]\n+    if is_imported_from_code:\n+        original_imported_code = history[0][\"text\"]\n          prompt_messages = assemble_imported_code_prompt(original_imported_code, stack)\n-        for index, item in enumerate(params[\"history\"][1:]):\n+        for index, item in enumerate(history[1:]):\n              role = \"user\" if index % 2 == 0 else \"assistant\"\n\nFile: backend/routes/generate_code.py (modified)\n  from image_generation.core import generate_images\n  from prompts import create_prompt\n  from prompts.claude_prompts import VIDEO_PROMPT\n-from prompts.types import Stack\n+from prompts.types import Stack, PromptContent\n  # from utils import pprint_prompt\n  from ws.constants import APP_ERROR_WEB_SOCKET_CODE  # type: ignore\n      anthropic_api_key: str | None\n      openai_base_url: str | None\n      generation_type: Literal[\"create\", \"update\"]\n+    prompt: PromptContent\n+    history: List[Dict[str, Any]]\n+    is_imported_from_code: bool\n+    result_image: str | None\n  class ParameterExtractionStage:\n              raise ValueError(f\"Invalid generation type: {generation_type}\")\n          generation_type = cast(Literal[\"create\", \"update\"], generation_type)\n+        # Extract prompt content\n+        prompt = params.get(\"prompt\", {\"text\": \"\", \"images\": []})\n+        ",
    "comment": "refactor: improve create_prompt architecture to use extracted params with clean separation",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "1bbf0089e03320dae0395b7434e77ecc4ba51587",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: frontend/src/components/sidebar/Sidebar.tsx (modified)\n  import SelectAndEditModeToggleButton from \"../select-and-edit/SelectAndEditModeToggleButton\";\n  import { Button } from \"../ui/button\";\n  import { Textarea } from \"../ui/textarea\";\n-import { useEffect, useRef, useState } from \"react\";\n+import { useEffect, useRef, useState, useCallback } from \"react\";\n  import HistoryDisplay from \"../history/HistoryDisplay\";\n  import Variants from \"../variants/Variants\";\n-import UpdateImageUpload from \"../UpdateImageUpload\";\n+import UpdateImageUpload, { UpdateImagePreview } from \"../UpdateImageUpload\";\n  interface SidebarProps {\n    showSelectAndEditFeature: boolean;\n  }: SidebarProps) {\n    const textareaRef = useRef<HTMLTextAreaElement>(null);\n    const [isErrorExpanded, setIsErrorExpanded] = useState(false);\n+  const [isDragging, setIsDragging] = useState(false);\n    const { appState, updateInstruction, setUpdateInstruction, updateImages, setUpdateImages } = useAppStore();\n+  // Helper function to convert file to data URL\n+  const fileToDataURL = (file: File): Promise<string> => {\n+    return new Promise((resolve, reject) => {\n+      const reader = new FileReader();",
    "comment": "feat: redesign follow-up image upload UI with multiple image support",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "61d10bd7613e48ac85d05a10bc529663c8a6a3a6",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: backend/routes/generate_code.py (modified)\n                  params, stack, input_mode\n              )\n-            print_prompt_summary(prompt_messages)\n+            print_prompt_summary(prompt_messages, truncate=False)\n              return prompt_messages, image_cache\n          except Exception:\n\nFile: backend/tests/test_prompt_summary.py (modified)\n      assert \"PROMPT SUMMARY\" in output\n      assert \"SYSTEM:\" in output\n      assert \"USER: short\" in output\n+\n+\n+def test_format_prompt_summary_no_truncate():\n+    messages = [\n+        {\"role\": \"system\", \"content\": \"This is a very long message that would normally be truncated at 40 characters but should be shown in full\"},\n+    ]\n+\n+    # Test with truncation (default)\n+    summary_truncated = format_prompt_summary(messages)\n+    assert \"...\" in summary_truncated\n+    assert len(summary_truncated.split(\": \", 1)[1]) <= 50  # Role + truncated content\n+\n+    # Test without truncation\n+    summary_full = format_prompt_summary(messages, truncate=False)\n+    assert \"...\" not in summary_full\n+    assert \"shown in full\" in summary_full\n+\n\nFile: backend/utils.py (modified)\n      print(json.dumps(truncate_data_strings(prompt_messages), indent=4))\n-def format_prompt_summary(prompt_messages: List[ChatCompletionMessageParam]) -> str:\n+def format_prompt_summary(prompt_messages: List[ChatCompletionMessageParam], truncate: bool = True) -> str:\n      parts: list[str] = []\n      for message in prompt_messages:\n          role = message[\"role\"]\n              text = str(content)\n          text = text.strip()\n-        if len(text) > 40:\n+        if truncate and len(text) > 40:\n              text = text[:40] + \"...\"\n          img_part = f\" + [{image_count} images]\" if image_count else \"\"\n      return \"\\n\".join(parts)\n-def print_prompt_summary(prompt_messages: List[ChatCompletionMessageParam]):\n-    summary = format_prompt_summary(prompt_messages)\n+def print_prompt_summary(prompt_messages: List[ChatCompletionMessageParam], truncate: bool = True):\n+    summary = format_prompt_summary(prompt_messages, truncate)\n      lines = summary.split('\\n')\n-    # Find the maximum line length, with a minimum of 20 and maximum of 80\n+    # Find the maximum line length, with a minimum of 20",
    "comment": "feat: add expandable prompt summary with truncate parameter",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "9c86bd9abe8768fd0a59589d47d6687f6e3f83a6",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: backend/tests/test_prompt_summary.py (added)\n+import pytest\n+from utils import format_prompt_summary\n+\n+\n+def test_format_prompt_summary():\n+    messages = [\n+        {\"role\": \"system\", \"content\": \"lorem ipsum dolor sit amet\"},\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": \"hello world\"},\n+                {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/png;base64,AAA\"}},\n+                {\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/png;base64,BBB\"}},\n+            ],\n+        },\n+    ]\n+\n+    summary = format_prompt_summary(messages)\n+    assert \"system: lorem ipsum\" in summary\n+    assert \"[2 images]\" in summary\n\nFile: backend/utils.py (modified)\n      print(json.dumps(truncate_data_strings(prompt_messages), indent=4))\n+def format_prompt_summary(prompt_messages: List[ChatCompletionMessageParam]) -> str:\n+    parts: list[str] = []\n+    for message in prompt_messages:\n+        role = message[\"role\"]\n+        content = message[\"content\"]\n+        text = \"\"\n+        image_count = 0\n+\n+        if isinstance(content, list):\n+            for item in content:\n+                if item[\"type\"] == \"text\":\n+                    text += item[\"text\"] + \" \"\n+                elif item[\"type\"] == \"image_url\":\n+                    image_count += 1\n+        else:\n+            text = str(content)\n+\n+        text = text.strip()\n+        if len(text) > 40:",
    "comment": "Add prompt summary utility",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "5d2e7fe8c6f8df4635b8926465090097056f4d82",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: frontend/src/App.tsx (modified)\n              ...baseCommitObject,\n              type: \"ai_edit\" as const,\n              parentHash: head,\n-            inputs: params.prompt,\n+            inputs: params.history\n+              ? params.history[params.history.length - 1]\n+              : { text: \"\", images: [] },\n            };\n      // Create a new commit and set it as the head",
    "comment": "Fix edit commit inputs",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "7795ad2e1500696d2258eedbf9e58102a088eb7b",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/flask/ctx.py (modified)\n      ctx = ctx.copy()\n      def wrapper(*args: t.Any, **kwargs: t.Any) -> t.Any:\n-        with ctx:  # type: ignore[union-attr]\n-            return ctx.app.ensure_sync(f)(*args, **kwargs)  # type: ignore[union-attr]\n+        with ctx:\n+            return ctx.app.ensure_sync(f)(*args, **kwargs)\n      return update_wrapper(wrapper, f)  # type: ignore[return-value]",
    "comment": "update dev dependencies",
    "language": "diff",
    "repo": "pallets/flask",
    "sha": "f04c5e696400badaa52b3450ded53e9091c2078a",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/markitdown/src/markitdown/_markitdown.py (modified)\n                  if docintel_types is not None:\n                      docintel_args[\"file_types\"] = docintel_types\n+                docintel_version = kwargs.get(\"docintel_api_version\")\n+                if docintel_version is not None:\n+                    docintel_args[\"api_version\"] = docintel_version\n+\n                  self.register_converter(\n                      DocumentIntelligenceConverter(**docintel_args),\n                  )",
    "comment": "feat: add Document Intelligence API version selection via kwargs (#1253)",
    "language": "diff",
    "repo": "microsoft/markitdown",
    "sha": "131f0c7739dae5484dc546eca2c294e9c44f7947",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/markitdown-mcp/src/markitdown_mcp/__main__.py (modified)\n  import sys\n-from typing import Any\n  from mcp.server.fastmcp import FastMCP\n  from starlette.applications import Starlette\n  from mcp.server.sse import SseServerTransport\n\nFile: packages/markitdown-sample-plugin/tests/test_sample_plugin.py (modified)\n  #!/usr/bin/env python3 -m pytest\n  import os\n-import pytest\n  from markitdown import MarkItDown, StreamInfo\n  from markitdown_sample_plugin import RtfConverter\n\nFile: packages/markitdown/src/markitdown/__main__.py (modified)\n  import argparse\n  import sys\n  import codecs\n-import locale\n  from textwrap import dedent\n  from importlib.metadata import entry_points\n  from .__about__ import __version__\n                  OR\n                  markitdown < example.pdf\n-                \n+\n                  OR to save to a file use\n-    \n+\n                  markitdown example.pdf -o example.md\n-                \n+\n                  OR\n-                \n+\n\nFile: packages/markitdown/src/markitdown/_base_converter.py (modified)\n-import os\n-import tempfile\n-from warnings import warn\n-from typing import Any, Union, BinaryIO, Optional, List\n+from typing import Any, BinaryIO, Optional\n  from ._stream_info import StreamInfo\n\nFile: packages/markitdown/src/markitdown/_markitdown.py (modified)\n-import copy\n  import mimetypes\n  import os\n  import re\n  import sys\n  import shutil\n-import tempfile\n-import warnings\n  import traceback\n  import io\n  from dataclasses import dataclass\n                  # Sanity check -- make sure the cur_pos is still the same\n                  assert (\n                      cur_pos == file_stream.tell()\n-                ), f\"File stream position should NOT change between guess iterations\"\n+                ), \"File stream position should NOT change between guess iterations\"\n                  _kwargs = {k: v for k, v in kwargs.items()}\n          # Nothing can handle it!\n          raise UnsupportedFormatException(\n-            f\"Could not convert stream to Markdown. No converter attempted a conversion, suggesting that the filetype is simply not supported.\"",
    "comment": "Chore: Make linter happy (#1256)",
    "language": "diff",
    "repo": "microsoft/markitdown",
    "sha": "cb421cf9eae0ec5c8e0a85e8beb035b7d0a00ce5",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/markitdown/src/markitdown/__main__.py (modified)\n          else:\n              charset_hint = None\n-    stream_info: str | None = None\n+    stream_info = None\n      if (\n          extension_hint is not None\n          or mime_type_hint is not None\n\nFile: packages/markitdown/src/markitdown/converters/_bing_serp_converter.py (modified)\n  import io\n  import re\n  import base64\n+import binascii\n  from urllib.parse import parse_qs, urlparse\n  from typing import Any, BinaryIO, Optional\n  from bs4 import BeautifulSoup\n          stream_info: StreamInfo,\n          **kwargs: Any,  # Options to pass to the converter\n      ) -> DocumentConverterResult:\n+        assert stream_info.url is not None\n+\n          # Parse the query parameters\n          parsed_params = parse_qs(urlparse(stream_info.url).query)\n          query = parsed_params.get(\"q\", [\"\"])[0]\n          _markdownify = _CustomMarkdownify()\n          results = list()\n          for result in soup.find_all(class_=\"b_algo\"):\n+            if not hasattr(result, \"find_all\"):\n+                continue\n\nFile: packages/markitdown/src/markitdown/converters/_outlook_msg_converter.py (modified)\n  _dependency_exc_info = None\n  olefile = None\n  try:\n-    import olefile\n+    import olefile  # type: ignore[no-redef]\n  except ImportError:\n      # Preserve the error and stack trace for later\n      _dependency_exc_info = sys.exc_info()\n          # Brue force, check if it's an Outlook file\n          try:\n-            msg = olefile.OleFileIO(file_stream)\n-            toc = \"\\n\".join([str(stream) for stream in msg.listdir()])\n-            return (\n-                \"__properties_version1.0\" in toc\n-                and \"__recip_version1.0_#00000000\" in toc\n-            )\n+            if olefile is not None:\n+                msg = olefile.OleFileIO(file_stream)\n+                toc = \"\\n\".join([str(stream) for stream in msg.listdir()])\n+                return (\n\nFile: packages/markitdown/src/markitdown/converters/_rss_converter.py (modified)\n              file_stream.seek(cur_pos)\n          return False\n-    def _feed_type(self, doc: Any) -> str:\n+    def _feed_type(self, doc: Any) -> str | None:\n          if doc.getElementsByTagName(\"rss\"):\n              return \"rss\"\n          elif doc.getElementsByTagName(\"feed\"):\n          Returns None if the feed type is not recognized or something goes wrong.\n          \"\"\"\n          root = doc.getElementsByTagName(\"rss\")[0]\n-        channel = root.getElementsByTagName(\"channel\")\n-        if not channel:\n-            return None\n-        channel = channel[0]\n+        channel_list = root.getElementsByTagName(\"channel\")\n+        if not channel_list:\n+            raise ValueError(\"No channel found in RSS feed\")\n+        channel = channel_list[0]\n          channel_title = self._get_data_by_tag_name(channel, \"title\")\n          channel_description = self._get_data_by_tag_name(channel, \"description\")\n\nFile: packages/markitdown/src/markitdown/converters/_wikipedia_converter.py (modified)\n  import io\n  import re\n+import bs4\n  from typing import Any, BinaryIO, Optional\n-from bs4 import BeautifulSoup\n  from .._base_converter import DocumentConverter, DocumentConverterResult\n  from .._stream_info import StreamInfo\n      ) -> DocumentConverterResult:\n          # Parse the stream\n          encoding = \"utf-8\" if stream_info.charset is None else stream_info.charset\n-        soup = BeautifulSoup(file_stream, \"html.parser\", from_encoding=encoding)\n+        soup = bs4.BeautifulSoup(file_stream, \"html.parser\", from_encoding=encoding)\n          # Remove javascript and style blocks\n          for script in soup([\"script\", \"style\"]):\n          if body_elm:\n              # What's the title\n-            if title_elm and len(title_elm) > 0:\n-                main_title = title_elm.string  # type: ignore\n-                assert isinstance(main_title, str)\n+            if title_elm and isinstance(title_elm, bs4.Tag):",
    "comment": "Fix remaining mypy errors. (#1132)",
    "language": "diff",
    "repo": "microsoft/markitdown",
    "sha": "5c565b7d79a3ed8c4d8a83862583280e5e98cc1f",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/markitdown/src/markitdown/_markitdown.py (modified)\n                              \"/opt\",\n                              \"/opt/bin\",\n                              \"/opt/local/bin\",\n-                            \"/opt/homebrew/bin\" \"C:\\\\Windows\\\\System32\",\n+                            \"/opt/homebrew/bin\",\n+                            \"C:\\\\Windows\\\\System32\",\n                              \"C:\\\\Program Files\",\n                              \"C:\\\\Program Files (x86)\",\n                          ]",
    "comment": "fix typo in well-known path list (#1109)",
    "language": "diff",
    "repo": "microsoft/markitdown",
    "sha": "2405f201afe5d8f05373ba557b9bd7c1e29f1d82",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/markitdown/src/markitdown/_markitdown.py (modified)\n  import os\n  import re\n  import sys\n+import shutil\n  import tempfile\n  import warnings\n  import traceback\n              self._llm_model = kwargs.get(\"llm_model\")\n              self._exiftool_path = kwargs.get(\"exiftool_path\")\n              self._style_map = kwargs.get(\"style_map\")\n+\n              if self._exiftool_path is None:\n                  self._exiftool_path = os.getenv(\"EXIFTOOL_PATH\")\n+            # Still none? Check well-known paths\n+            if self._exiftool_path is None:\n+                candidate = shutil.which(\"exiftool\")\n+                if candidate:\n+                    candidate = os.path.abspath(candidate)\n+                    if any(\n+                        d == os.path.dirname(candidate)\n\nFile: packages/markitdown/src/markitdown/converters/_exiftool.py (modified)\n  import shutil\n  import os\n  import warnings\n-from typing import BinaryIO, Optional, Any\n+from typing import BinaryIO, Any, Union\n  def exiftool_metadata(\n-    file_stream: BinaryIO, *, exiftool_path: Optional[str] = None\n+    file_stream: BinaryIO,\n+    *,\n+    exiftool_path: Union[str, None],\n  ) -> Any:  # Need a better type for json data\n-    # Check if we have a valid pointer to exiftool\n+    # Nothing to do\n      if not exiftool_path:\n-        which_exiftool = shutil.which(\"exiftool\")\n-        if which_exiftool:\n-            warnings.warn(\n-                f\"\"\"Implicit discovery of 'exiftool' is disabled. If you would like to continue to use exiftool in MarkItDown, please set the exiftool_path parameter in the MarkItDown consructor. E.g., \n-\n-    md = MarkItDown(exiftool_path=\"{which_exiftool}\")\n\nFile: packages/markitdown/tests/test_markitdown.py (modified)\n  import pytest\n  import requests\n-import warnings\n-\n  from markitdown import (\n      MarkItDown,\n      UnsupportedFormatException,\n      reason=\"do not run if exiftool is not installed\",\n  )\n  def test_markitdown_exiftool() -> None:\n-    # Test the automatic discovery of exiftool throws a warning\n-    # and is disabled\n-    try:\n-        warnings.simplefilter(\"default\")\n-        with warnings.catch_warnings(record=True) as w:\n-            markitdown = MarkItDown()\n-            result = markitdown.convert(os.path.join(TEST_FILES_DIR, \"test.jpg\"))\n-            assert len(w) == 1\n-            assert w[0].category is DeprecationWarning\n-            assert result.text_content.strip() == \"\"",
    "comment": "Fix exiftool in well-known paths. (#1106)",
    "language": "diff",
    "repo": "microsoft/markitdown",
    "sha": "99d8e562db89ee00c1fb09598132729d37dcf2e3",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/markitdown/src/markitdown/converters/_pptx_converter.py (modified)\n  import html\n  from typing import BinaryIO, Any\n+from operator import attrgetter\n  from ._html_converter import HtmlConverter\n  from ._llm_caption import llm_caption\n                  # Group Shapes\n                  if shape.shape_type == pptx.enum.shapes.MSO_SHAPE_TYPE.GROUP:\n-                    for subshape in shape.shapes:\n+                    sorted_shapes = sorted(shape.shapes, key=attrgetter(\"top\", \"left\"))\n+                    for subshape in sorted_shapes:\n                          get_shape_content(subshape, **kwargs)\n-            for shape in slide.shapes:\n+            sorted_shapes = sorted(slide.shapes, key=attrgetter(\"top\", \"left\"))\n+            for shape in sorted_shapes:\n                  get_shape_content(shape, **kwargs)\n              md_content = md_content.strip()",
    "comment": "feat: sort pptx shapes to be parsed in top-to-bottom, left-to-right order (#1104)",
    "language": "diff",
    "repo": "microsoft/markitdown",
    "sha": "0229ff6cb75aa63e5ba6582766d0e5b9cba1fdfa",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: js/reveal.js (modified)\n  \t\tif( horizontalSlidesLength && typeof indexh !== 'undefined' ) {\n+\t\t\tconst isOverview = overview.isActive();\n+\n  \t\t\t// The number of steps away from the present slide that will\n  \t\t\t// be visible\n-\t\t\tlet viewDistance = overview.isActive() ? 10 : config.viewDistance;\n+\t\t\tlet viewDistance = isOverview ? 10 : config.viewDistance;\n  \t\t\t// Shorten the view distance on devices that typically have\n  \t\t\t// less resources\n  \t\t\tif( Device.isMobile ) {\n-\t\t\t\tviewDistance = overview.isActive() ? 6 : config.mobileViewDistance;\n+\t\t\t\tviewDistance = isOverview ? 6 : config.mobileViewDistance;\n  \t\t\t}\n  \t\t\t// All slides need to be visible when exporting to PDF\n  \t\t\t\tif( verticalSlidesLength ) {\n-\t\t\t\t\tlet oy = getPreviousVerticalIndex( horizontalSlide );\n+\t\t\t\t\tlet oy = isOverview ? 0 : getPreviousVerticalIndex( horizontalSlide );\n  \t\t\t\t\tfor( let y = 0; y < verticalSlidesLength; y++ ) {\n  \t\t\t\t\t\tlet verticalSlide = verticalSlides[y];",
    "comment": "overview mode: fix missing thumbs in adjacent stacks with over 10 vertical slides (closes #3754)",
    "language": "diff",
    "repo": "hakimel/reveal.js",
    "sha": "eb95b14531a1d52a616553759bd1c383cc1d01fc",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: js/controllers/overlay.js (modified)\n  \t\t// Enable link previews globally\n  \t\tif( this.Reveal.getConfig().previewLinks ) {\n-\t\t\tthis.linkPreviewSelector = 'a[href]:not([data-preview-link=false])';\n+\t\t\tthis.linkPreviewSelector = 'a[href]:not([data-preview-link=false]), [data-preview-link]:not(a):not([data-preview-link=false])';\n  \t\t}\n  \t\t// Enable link previews for individual elements\n  \t\telse {\n  \t\t// Was a link preview clicked?\n  \t\tif( linkTarget ) {\n-\t\t\tlet url = linkTarget.getAttribute( 'href' );\n+\t\t\tlet url = linkTarget.getAttribute( 'href' ) || linkTarget.getAttribute( 'data-preview-link' );\n  \t\t\tif( url ) {\n  \t\t\t\tthis.showIframePreview( url );\n  \t\t\t\tevent.preventDefault();",
    "comment": "add support for data-preview-link on any element type (prev only a)",
    "language": "diff",
    "repo": "hakimel/reveal.js",
    "sha": "94716f9e514ad44dedea103a829adea5be3ac2fc",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: js/reveal.js (modified)\n  } from './utils/constants.js'\n  // The reveal.js version\n-export const VERSION = '5.1.0';\n+export const VERSION = '5.2.0';\n  /**\n   * reveal.js",
    "comment": "update version to 5.2.0",
    "language": "diff",
    "repo": "hakimel/reveal.js",
    "sha": "edb69c840cf3073f98f7bdd7a3148a7e1b176f5f",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: js/config.js (modified)\n  \tmaxScale: 2.0,\n  \t// Display presentation control arrows.\n-\t// This can be a boolean (true / false) or 'speaker-only' to only display\n-\t// the controls on the speaker's screen.\n+\t// - true: Display controls on all screens\n+\t// - false: Hide controls on all screens\n+\t// - \"speaker-only\": Only display controls in the speaker view\n  \tcontrols: true,\n  \t// Help the user learn the controls by providing hints, for example by",
    "comment": "update controls comment to match other prop descriptions",
    "language": "diff",
    "repo": "hakimel/reveal.js",
    "sha": "983c6248f712c69a341d413683bfc3bb4d13973b",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: js/config.js (modified)\n  \tminScale: 0.2,\n  \tmaxScale: 2.0,\n-\t// Display presentation control arrows\n+\t// Display presentation control arrows.\n+\t// This can be a boolean (true / false) or 'speaker-only' to only display\n+\t// the controls on the speaker's screen.\n  \tcontrols: true,\n  \t// Help the user learn the controls by providing hints, for example by\n\nFile: js/controllers/controls.js (modified)\n  \t */\n  \tconfigure( config, oldConfig ) {\n-\t\tthis.element.style.display = config.controls ? 'block' : 'none';\n+\t\tthis.element.style.display = (\n+\t\t\tconfig.controls &&\n+\t\t\t(config.controls !== 'speaker-only' || this.Reveal.isSpeakerNotes())\n+\t\t) ? 'block' : 'none';\n  \t\tthis.element.setAttribute( 'data-controls-layout', config.controlsLayout );\n  \t\tthis.element.setAttribute( 'data-controls-back-arrows', config.controlsBackArrows );",
    "comment": "Add controls: 'speaker-only' option",
    "language": "diff",
    "repo": "hakimel/reveal.js",
    "sha": "37517154144b651522f95669781942a807a93818",
    "quality_score": 0.7,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/algorithms/sorting/bucket-sort/BucketSort.js (added)\n+import RadixSort from '../radix-sort/RadixSort';\n+\n+/**\n+ * Bucket Sort\n+ *\n+ * @param {number[]} arr\n+ * @param {number} bucketsNum\n+ * @return {number[]}\n+ */\n+export default function BucketSort(arr, bucketsNum = 1) {\n+  const buckets = new Array(bucketsNum).fill(null).map(() => []);\n+\n+  const minValue = Math.min(...arr);\n+  const maxValue = Math.max(...arr);\n+\n+  const bucketSize = Math.ceil(Math.max(1, (maxValue - minValue) / bucketsNum));\n+\n+  // Place elements into buckets.\n+  for (let i = 0; i < arr.length; i += 1) {\n+    const currValue = arr[i];\n\nFile: src/algorithms/sorting/bucket-sort/__test__/BucketSort.test.js (added)\n+import BucketSort from '../BucketSort';\n+import {\n+  equalArr,\n+  notSortedArr,\n+  reverseArr,\n+  sortedArr,\n+} from '../../SortTester';\n+\n+describe('BucketSort', () => {\n+  it('should sort the array of numbers with different buckets amounts', () => {\n+    expect(BucketSort(notSortedArr, 4)).toEqual(sortedArr);\n+    expect(BucketSort(equalArr, 4)).toEqual(equalArr);\n+    expect(BucketSort(reverseArr, 4)).toEqual(sortedArr);\n+    expect(BucketSort(sortedArr, 4)).toEqual(sortedArr);\n+\n+    expect(BucketSort(notSortedArr, 10)).toEqual(sortedArr);\n+    expect(BucketSort(equalArr, 10)).toEqual(equalArr);\n+    expect(BucketSort(reverseArr, 10)).toEqual(sortedArr);\n+    expect(BucketSort(sortedArr, 10)).toEqual(sortedArr);\n+",
    "comment": "Add Bucket Sort.",
    "language": "diff",
    "repo": "trekhleb/javascript-algorithms",
    "sha": "1ad60dc5107216e1f6febf7255e5fc915588c94b",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/data-structures/lru-cache/LRUCache.js (modified)\n-/* eslint-disable no-param-reassign */\n-import LinkedListNode from './LinkedListNode';\n+/* eslint-disable no-param-reassign, max-classes-per-file */\n+\n+/**\n+ * Simple implementation of the Doubly-Linked List Node\n+ * that is used in LRUCache class below.\n+ */\n+class LinkedListNode {\n+  /**\n+   * Creates a doubly-linked list node.\n+   * @param {string} key\n+   * @param {any} val\n+   * @param {LinkedListNode} prev\n+   * @param {LinkedListNode} next\n+   */\n+  constructor(key, val, prev = null, next = null) {\n+    this.key = key;\n+    this.val = val;\n+    this.prev = prev;\n\nFile: src/data-structures/lru-cache/LinkedListNode.js (removed)\n-class LinkedListNode {\n-  /**\n-   * Creates a doubly-linked list node.\n-   * @param {string} key\n-   * @param {any} val\n-   * @param {LinkedListNode} prev\n-   * @param {LinkedListNode} next\n-   */\n-  constructor(key, val, prev = null, next = null) {\n-    this.key = key;\n-    this.val = val;\n-    this.prev = prev;\n-    this.next = next;\n-  }\n-}\n-\n-export default LinkedListNode;",
    "comment": "Refactor LRU Cache.",
    "language": "diff",
    "repo": "trekhleb/javascript-algorithms",
    "sha": "69c3a16f75175fa6f49cd936e0049fdc6ede1da4",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: modules/upscaler.py (modified)\n          dest_w = int((img.width * scale) // 8 * 8)\n          dest_h = int((img.height * scale) // 8 * 8)\n-        for _ in range(3):\n-            if img.width >= dest_w and img.height >= dest_h and scale != 1:\n+        for i in range(3):\n+            if img.width >= dest_w and img.height >= dest_h and (i > 0 or scale != 1):\n                  break\n              if shared.state.interrupted:",
    "comment": "fix upscale logic",
    "language": "diff",
    "repo": "AUTOMATIC1111/stable-diffusion-webui",
    "sha": "964fc13a99d47263d023f4e3116ac2c220acec88",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: examples/pytorch/audio-classification/run_audio_classification.py (modified)\n  # See the License for the specific language governing permissions and\n  # limitations under the License.\n+# /// script\n+# dependencies = [\n+#     \"transformers @ git+https://github.com/huggingface/transformers.git\",\n+#     \"datasets[audio]>=1.14.0\",\n+#     \"evaluate\",\n+#     \"librosa\",\n+#     \"torchaudio\",\n+#     \"torch>=1.6\",\n+# ]\n+# ///\n+\n  import logging\n  import os\n  import sys\n\nFile: examples/pytorch/contrastive-image-text/run_clip.py (modified)\n  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  # See the License for the specific language governing permissions and\n  # limitations under the License.\n+\n+# /// script\n+# dependencies = [\n+#     \"transformers @ git+https://github.com/huggingface/transformers.git\",\n+#     \"torch>=1.5.0\",\n+#     \"torchvision>=0.6.0\",\n+#     \"datasets>=1.8.0\",\n+# ]\n+# ///\n+\n  \"\"\"\n  Training a CLIP like dual encoder models using text and vision encoders in the library.\n\nFile: examples/pytorch/image-classification/run_image_classification.py (modified)\n  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  # See the License for the specific language governing permissions and\n+# /// script\n+# dependencies = [\n+#     \"transformers @ git+https://github.com/huggingface/transformers.git\",\n+#     \"accelerate>=0.12.0\",\n+#     \"torch>=1.5.0\",\n+#     \"torchvision>=0.6.0\",\n+#     \"datasets>=2.14.0\",\n+#     \"evaluate\",\n+#     \"scikit-learn\",\n+# ]\n+# ///\n+\n  import logging\n  import os\n  import sys\n\nFile: examples/pytorch/image-classification/run_image_classification_no_trainer.py (modified)\n  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  # See the License for the specific language governing permissions and\n  # limitations under the License.\n+\n+# /// script\n+# dependencies = [\n+#     \"transformers @ git+https://github.com/huggingface/transformers.git\",\n+#     \"accelerate>=0.12.0\",\n+#     \"torch>=1.5.0\",\n+#     \"torchvision>=0.6.0\",\n+#     \"datasets>=2.14.0\",\n+#     \"evaluate\",\n+#     \"scikit-learn\",\n+# ]\n+# ///\n+\n  \"\"\"Finetuning any 🤗 Transformers model for image classification leveraging 🤗 Accelerate.\"\"\"\n  import argparse",
    "comment": "Make pytorch examples UV-compatible (#39635)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "91f591f7bcad6ef4027b5244cab0eb505a491671",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/modeling_flash_attention_utils.py (modified)\n      query = query.contiguous().view(-1, query.size(-2), query.size(-1))\n      key = key.contiguous().view(-1, key.size(-2), key.size(-1))\n      value = value.contiguous().view(-1, value.size(-2), value.size(-1))\n+\n      cu_seqlens_k = torch.cat(\n          [torch.tensor([0], dtype=torch.int32, device=query.device), position_ids[:, -1].cumsum(dim=0) + 1], dim=0\n      )\n      max_k = torch.max(position_ids, dim=1).values.max().item() + 1\n+\n      position_ids = position_ids.flatten()\n      indices_q = torch.arange(position_ids.size(0), device=position_ids.device, dtype=torch.int32)\n      cu_seq_lens = torch.cat(\n          (\n-            torch.tensor([0], device=position_ids.device, dtype=torch.int32),\n+            indices_q[position_ids == 0],\n              torch.tensor(position_ids.size(), device=position_ids.device, dtype=torch.int32),\n          )\n      )",
    "comment": "revert behavior of _prepare_from_posids (#39622)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "5a81d7e0b388fb2b86fc1279cdc07d9dc7e84b4c",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/glm4v/configuration_glm4v.py (modified)\n  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  # See the License for the specific language governing permissions and\n  # limitations under the License.\n-\n  from ...configuration_utils import PretrainedConfig\n  from ...modeling_rope_utils import rope_config_validation\n\nFile: src/transformers/models/glm4v/image_processing_glm4v.py (modified)\n          factor = patch_size * merge_size\n          resized_height, resized_width = smart_resize(\n-            t=self.temporal_patch_size,\n+            num_frames=self.temporal_patch_size,\n              height=height,\n              width=width,\n              factor=factor,\n-            t_factor=self.temporal_patch_size,\n+            temporal_factor=self.temporal_patch_size,\n          )\n          grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n          return grid_h * grid_w\n\nFile: src/transformers/models/glm4v/modeling_glm4v.py (modified)\n  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  # See the License for the specific language governing permissions and\n  # limitations under the License.\n-\n  import itertools\n  from dataclasses import dataclass\n  from typing import Any, Callable, Optional, Union\n              output_attentions=output_attentions,\n              use_cache=use_cache,\n              cache_position=cache_position,\n+            **kwargs,\n          )\n          hidden_states = self.post_self_attn_layernorm(hidden_states)\n\nFile: src/transformers/models/glm4v/modular_glm4v.py (modified)\n  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  # See the License for the specific language governing permissions and\n  # limitations under the License.\n-\n  import itertools\n  from typing import Callable, Optional, Union\n+import numpy as np\n  import torch\n  import torch.nn as nn\n  import torch.nn.functional as F\n              output_attentions=output_attentions,\n              use_cache=use_cache,\n              cache_position=cache_position,\n+            **kwargs,\n          )\n          hidden_states = self.post_self_attn_layernorm(hidden_states)\n      _defaults = {\n          \"text_kwargs\": {\n              \"padding\": False,\n+            \"return_mm_token_type_ids\": False,\n\nFile: src/transformers/models/glm4v/processing_glm4v.py (modified)\n  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  # See the License for the specific language governing permissions and\n  # limitations under the License.\n-\n  from typing import Optional, Union\n+import numpy as np\n+\n  from ...feature_extraction_utils import BatchFeature\n  from ...image_utils import ImageInput\n  from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs\n      _defaults = {\n          \"text_kwargs\": {\n              \"padding\": False,\n+            \"return_mm_token_type_ids\": False,\n          },\n      }\n                  text[i] = text[i].replace(\"<|placeholder|>\", self.image_token)\n          return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n          text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])",
    "comment": "Update recent processors for vLLM backend (#39583)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "947a37e8f5bc50bc0e9a77c0d16b038adcb056d0",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: tests/causal_lm_tester.py (modified)\n                  logits = outputs.hidden_states[-1]\n                  logits_fa = outputs_fa.hidden_states[-1]\n-\n-                assert torch.allclose(logits_fa, logits, atol=2e-3)\n+                torch.testing.assert_close(logits_fa, logits, atol=3e-2, rtol=3e-2)\n\nFile: tests/generation/test_utils.py (modified)\n          set_model_tester_for_less_flaky_test(self)\n          for model_class in self.all_generative_model_classes:\n-            if not getattr(model_class, support_flag[attn_implementation]):\n+            if attn_implementation != \"eager\" and not getattr(model_class, support_flag[attn_implementation]):\n                  self.skipTest(f\"{model_class.__name__} does not support `attn_implementation={attn_implementation}`\")\n              config, original_inputs_dict = self.prepare_config_and_inputs_for_generate()\n\nFile: tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py (modified)\n          }\n          for model_class in self.all_generative_model_classes:\n-            if not getattr(model_class, support_flag[attn_implementation]):\n+            if attn_implementation != \"eager\" and not getattr(model_class, support_flag[attn_implementation]):\n                  self.skipTest(f\"{model_class.__name__} does not support `attn_implementation={attn_implementation}`\")\n              config, original_inputs_dict = self.prepare_config_and_inputs_for_generate()\n\nFile: tests/test_modeling_common.py (modified)\n          config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n          cls = self._torch_compile_train_cls  # e.g. LlamaFroCausalLM\n-        model = cls(config, attn_implementation=\"flash_attention_2\").to(device=torch_device, dtype=torch_dtype)\n+        model = cls._from_config(config, attn_implementation=\"flash_attention_2\").to(\n+            device=torch_device, dtype=torch_dtype\n+        )\n          inputs = {\n              \"input_ids\": torch.randint(low=1, high=model.config.vocab_size, size=(2, 10), device=torch_device),\n          }\n          for model_class in self.all_generative_model_classes:\n-            if not getattr(model_class, support_flag[attn_implementation]):\n+            if attn_implementation != \"eager\" and not getattr(model_class, support_flag[attn_implementation]):\n                  self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n              config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()",
    "comment": "Fix important models CI (#39576)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "ea56eb6bed8a66502e95520a8c6819b0523556e1",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: _site/embed-preview-script.js (modified)\n          if (context.startsWith('@')) {\n              // @mentions - just show the text\n              pill.innerHTML = '<span>' + context + '</span>';\n+        } else if (context.startsWith('http://') || context.startsWith('https://')) {\n+            // Web URLs show world icon\n+            icon = '<svg class=\"w-3 h-3\" viewBox=\"0 0 20 20\" fill=\"currentColor\"><path fill-rule=\"evenodd\" d=\"M10 18a8 8 0 100-16 8 8 0 000 16zM4.332 8.027a6.012 6.012 0 011.912-2.706C6.512 5.73 6.974 6 7.5 6A1.5 1.5 0 019 7.5V8a2 2 0 004 0 2 2 0 011.523-1.943A5.977 5.977 0 0116 10c0 .34-.028.675-.083 1H15a2 2 0 00-2 2v2.197A5.973 5.973 0 0110 16v-2a2 2 0 00-2-2 2 2 0 01-2-2 2 2 0 00-1.668-1.973z\" clip-rule=\"evenodd\"/></svg>';\n+            pill.innerHTML = icon + '<span>' + context + '</span>';\n          } else if (context.startsWith('#')) {\n              // Any hashtag context shows image icon\n              icon = '<svg class=\"w-3 h-3\" viewBox=\"0 0 20 20\" fill=\"currentColor\"><path fill-rule=\"evenodd\" d=\"M4 3a2 2 0 00-2 2v10a2 2 0 002 2h12a2 2 0 002-2V5a2 2 0 00-2-2H4zm12 12H4l4-8 3 6 2-4 3 6z\" clip-rule=\"evenodd\"/></svg>';\n\nFile: embed-preview-script.js (modified)\n          if (context.startsWith('@')) {\n              // @mentions - just show the text\n              pill.innerHTML = '<span>' + context + '</span>';\n+        } else if (context.startsWith('http://') || context.startsWith('https://')) {\n+            // Web URLs show world icon\n+            icon = '<svg class=\"w-3 h-3\" viewBox=\"0 0 20 20\" fill=\"currentColor\"><path fill-rule=\"evenodd\" d=\"M10 18a8 8 0 100-16 8 8 0 000 16zM4.332 8.027a6.012 6.012 0 011.912-2.706C6.512 5.73 6.974 6 7.5 6A1.5 1.5 0 019 7.5V8a2 2 0 004 0 2 2 0 011.523-1.943A5.977 5.977 0 0116 10c0 .34-.028.675-.083 1H15a2 2 0 00-2 2v2.197A5.973 5.973 0 0110 16v-2a2 2 0 00-2-2 2 2 0 01-2-2 2 2 0 00-1.668-1.973z\" clip-rule=\"evenodd\"/></svg>';\n+            pill.innerHTML = icon + '<span>' + context + '</span>';\n          } else if (context.startsWith('#')) {\n              // Any hashtag context shows image icon\n              icon = '<svg class=\"w-3 h-3\" viewBox=\"0 0 20 20\" fill=\"currentColor\"><path fill-rule=\"evenodd\" d=\"M4 3a2 2 0 00-2 2v10a2 2 0 002 2h12a2 2 0 002-2V5a2 2 0 00-2-2H4zm12 12H4l4-8 3 6 2-4 3 6z\" clip-rule=\"evenodd\"/></svg>';",
    "comment": "add embedding",
    "language": "diff",
    "repo": "f/awesome-chatgpt-prompts",
    "sha": "6e5d28e52e33c858f922569e07e75afa12589faf",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: index.d.ts (modified)\n  type Milliseconds = number;\n-type AxiosAdapterName = 'fetch' | 'xhr' | 'http' | string;\n+type AxiosAdapterName = 'fetch' | 'xhr' | 'http' | (string & {});\n  type AxiosAdapterConfig = AxiosAdapter | AxiosAdapterName;",
    "comment": "fix(types): fix autocomplete for adapter config (#6855)",
    "language": "diff",
    "repo": "axios/axios",
    "sha": "e61a8934d8f94dd429a2f309b48c67307c700df0",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: lib/core/Axios.js (modified)\n   */\n  class Axios {\n    constructor(instanceConfig) {\n-    this.defaults = instanceConfig;\n+    this.defaults = instanceConfig || {};\n      this.interceptors = {\n        request: new InterceptorManager(),\n        response: new InterceptorManager()\n\nFile: test/unit/core/Axios.js (modified)\n          }\n        })\n      });\n-  })\n+  });\n+\n+  it('should not throw if the config argument is omitted', () => {\n+    const axios = new Axios();\n+\n+    assert.deepStrictEqual(axios.defaults, {});\n+  });\n  });",
    "comment": "fix(core): fix the Axios constructor implementation to treat the config argument as optional; (#6881)",
    "language": "diff",
    "repo": "axios/axios",
    "sha": "6c5d4cd69286868059c5e52d45085cb9a894a983",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/create-react-app/createReactApp.js (modified)\n        'You can find a list of up-to-date React frameworks on react.dev'\n      );\n      console.log(\n-      chalk.underline('https://react.dev/learn/start-a-new-react-project')\n+      'For more info see:' + chalk.underline('https://react.dev/link/cra')\n      );\n      console.log('');\n      console.log(",
    "comment": "Update deprecation link (#17015)",
    "language": "diff",
    "repo": "facebook/create-react-app",
    "sha": "380066041106963c50f55869904f504822d396af",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/react-scripts/fixtures/kitchensink/template/src/features/env/ExpandEnvVariables.test.js (modified)\n  import React from 'react';\n  import ReactDOMClient from 'react-dom/client';\n  import ExpandEnvVariables from './ExpandEnvVariables';\n+import { flushSync } from 'react-dom';\n  describe('expand .env variables', () => {\n    it('renders without crashing', () => {\n      const div = document.createElement('div');\n-    ReactDOMClient.createRoot(div).render(<ExpandEnvVariables />);\n+    flushSync(() => {\n+      ReactDOMClient.createRoot(div).render(<ExpandEnvVariables />);\n+    });\n    });\n  });\n\nFile: packages/react-scripts/fixtures/kitchensink/template/src/features/env/FileEnvVariables.test.js (modified)\n  import React from 'react';\n  import FileEnvVariables from './FileEnvVariables';\n  import ReactDOMClient from 'react-dom/client';\n+import { flushSync } from 'react-dom';\n  describe('.env variables', () => {\n    it('renders without crashing', () => {\n      const div = document.createElement('div');\n-    ReactDOMClient.createRoot(div).render(<FileEnvVariables />);\n+    flushSync(() => {\n+      ReactDOMClient.createRoot(div).render(<FileEnvVariables />);\n+    });\n    });\n  });\n\nFile: packages/react-scripts/fixtures/kitchensink/template/src/features/env/PublicUrl.test.js (modified)\n  import React from 'react';\n  import PublicUrl from './PublicUrl';\n  import ReactDOMClient from 'react-dom/client';\n+import { flushSync } from 'react-dom';\n  describe('PUBLIC_URL', () => {\n    it('renders without crashing', () => {\n      const div = document.createElement('div');\n-    ReactDOMClient.createRoot(div).render(<PublicUrl />);\n+    flushSync(() => {\n+      ReactDOMClient.createRoot(div).render(<PublicUrl />);\n+    });\n    });\n  });\n\nFile: packages/react-scripts/fixtures/kitchensink/template/src/features/env/ShellEnvVariables.test.js (modified)\n  import React from 'react';\n  import ShellEnvVariables from './ShellEnvVariables';\n  import ReactDOMClient from 'react-dom/client';\n+import { flushSync } from 'react-dom';\n  describe('shell env variables', () => {\n    it('renders without crashing', () => {\n      const div = document.createElement('div');\n-    ReactDOMClient.createRoot(div).render(<ShellEnvVariables />);\n+    flushSync(() => {\n+      ReactDOMClient.createRoot(div).render(<ShellEnvVariables />);\n+    });\n    });\n  });\n\nFile: packages/react-scripts/fixtures/kitchensink/template/src/features/webpack/CssInclusion.test.js (modified)\n  import React from 'react';\n  import CssInclusion from './CssInclusion';\n  import ReactDOMClient from 'react-dom/client';\n+import { flushSync } from 'react-dom';\n  describe('css inclusion', () => {\n    it('renders without crashing', () => {\n      const div = document.createElement('div');\n-    ReactDOMClient.createRoot(div).render(<CssInclusion />);\n+    flushSync(() => {\n+      ReactDOMClient.createRoot(div).render(<CssInclusion />);\n+    });\n    });\n  });",
    "comment": "Update tests",
    "language": "diff",
    "repo": "facebook/create-react-app",
    "sha": "0dc2b2df00ceae62a4014e3f554da1bde9515b81",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/react-scripts/fixtures/kitchensink/template/src/features/config/BaseUrl.test.js (modified)\n   */\n  import React from 'react';\n-import ReactDOM from 'react-dom';\n+import ReactDOMClient from 'react-dom/client';\n  import NodePath from './BaseUrl';\n  describe('BASE_URL', () => {\n    it('renders without crashing', () => {\n      const div = document.createElement('div');\n      return new Promise(resolve => {\n-      ReactDOM.render(<NodePath onReady={resolve} />, div);\n+      ReactDOMClient.createRoot(div).render(<NodePath onReady={resolve} />);\n      });\n    });\n  });\n\nFile: packages/react-scripts/fixtures/kitchensink/template/src/features/env/ExpandEnvVariables.test.js (modified)\n   */\n  import React from 'react';\n-import ReactDOM from 'react-dom';\n+import ReactDOMClient from 'react-dom/client';\n  import ExpandEnvVariables from './ExpandEnvVariables';\n  describe('expand .env variables', () => {\n    it('renders without crashing', () => {\n      const div = document.createElement('div');\n-    ReactDOM.render(<ExpandEnvVariables />, div);\n+    ReactDOMClient.createRoot(div).render(<ExpandEnvVariables />);\n    });\n  });\n\nFile: packages/react-scripts/fixtures/kitchensink/template/src/features/env/FileEnvVariables.test.js (modified)\n   */\n  import React from 'react';\n-import ReactDOM from 'react-dom';\n  import FileEnvVariables from './FileEnvVariables';\n+import ReactDOMClient from 'react-dom/client';\n  describe('.env variables', () => {\n    it('renders without crashing', () => {\n      const div = document.createElement('div');\n-    ReactDOM.render(<FileEnvVariables />, div);\n+    ReactDOMClient.createRoot(div).render(<FileEnvVariables />);\n    });\n  });\n\nFile: packages/react-scripts/fixtures/kitchensink/template/src/features/env/PublicUrl.test.js (modified)\n   */\n  import React from 'react';\n-import ReactDOM from 'react-dom';\n  import PublicUrl from './PublicUrl';\n+import ReactDOMClient from 'react-dom/client';\n  describe('PUBLIC_URL', () => {\n    it('renders without crashing', () => {\n      const div = document.createElement('div');\n-    ReactDOM.render(<PublicUrl />, div);\n+    ReactDOMClient.createRoot(div).render(<PublicUrl />);\n    });\n  });",
    "comment": "Fix e2e tests",
    "language": "diff",
    "repo": "facebook/create-react-app",
    "sha": "9bd1974523d317699ce790ef4f3bffe751598a89",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: inference/model.py (modified)\n              else:\n                  group_scores = scores.topk(2, dim=-1)[0].sum(dim=-1)\n              indices = group_scores.topk(self.topk_groups, dim=-1)[1]\n-            mask = torch.zeros_like(scores[..., 0]).scatter_(1, indices, True)\n-            scores = (scores * mask.unsqueeze(-1)).flatten(1)\n+            mask = scores.new_ones(x.size(0), self.n_groups, dtype=bool).scatter_(1, indices, False)\n+            scores = scores.masked_fill_(mask.unsqueeze(-1), float(\"-inf\")).flatten(1)\n          indices = torch.topk(scores, self.topk, dim=-1)[1]\n          weights = original_scores.gather(1, indices)\n          if self.score_func == \"sigmoid\":",
    "comment": "fix scores mask",
    "language": "diff",
    "repo": "deepseek-ai/DeepSeek-V3",
    "sha": "1398800ebfcd49c048737e8b1aae69dee46ffefc",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: inference/model.py (modified)\n          quantization-aware computations depending on the input parameters.\n      Notes:\n-        - If `weight` is quantized (e.g., `element_size() > 1`), a dequantized version \n+        - If `weight` is quantized (e.g., `element_size() == 1`), a dequantized version \n            is used for computation.\n          - If `gemm_impl == \"bf16\"`, dequantization and a `bf16` GEMM operation are applied.\n          - For other cases, the function applies quantization to `x` and uses `fp8_gemm` for computation.",
    "comment": "fix comment",
    "language": "diff",
    "repo": "deepseek-ai/DeepSeek-V3",
    "sha": "5ee97a83f0457d0d805b862aeb387358e1801e6d",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: whisper/utils.py (modified)\n              if len(subtitle) > 0:\n                  yield subtitle\n-        if \"words\" in result[\"segments\"][0]:\n+        if len(result[\"segments\"]) > 0 and \"words\" in result[\"segments\"][0]:\n              for subtitle in iterate_subtitles():\n                  subtitle_start = self.format_timestamp(subtitle[0][\"start\"])\n                  subtitle_end = self.format_timestamp(subtitle[-1][\"end\"])",
    "comment": "Fix exception when an audio file with no speech is provided (#1396)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "b38a1f20f4b23f3f3099af2c3e0ca95627276ddf",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: scripts/question_utils.py (modified)\n  from typing import List\n  import re\n-p = pathlib.Path(__file__).parent.parent.joinpath(\"README.md\")\n-\n-\n-def get_file_list():\n-    file_list = \"\"\n-    with open(p, \"rb\") as f:\n-        for line in f.readlines():\n-            file_list += line.rstrip().decode()\n-    return file_list\n-\n-\n-def get_question_list(file_list: List[str]) -> list:\n-    file_list = re.findall(\"<details>(.*?)</details>\", file_list)\n-    questions_list = []\n-    for i in file_list:\n-        q = re.findall(r\"<summary>(.*?)</summary>\", i)[0]\n-        questions_list.append(q)\n-    return questions_list",
    "comment": "Refactor question utils for enhanced performance, readability, and extended functionality while preserving backward compatibility (#10551)",
    "language": "diff",
    "repo": "bregman-arie/devops-exercises",
    "sha": "70f382d5b15c852298a3cb74b8e1a31ff14c83fc",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: backend/llm.py (modified)\n      Llm.O1_2024_12_17: \"openai\",\n      Llm.O4_MINI_2025_04_16: \"openai\",\n      Llm.O3_2025_04_16: \"openai\",\n-\n      # Anthropic models\n      Llm.CLAUDE_3_SONNET: \"anthropic\",\n      Llm.CLAUDE_3_OPUS: \"anthropic\",\n      Llm.CLAUDE_3_7_SONNET_2025_02_19: \"anthropic\",\n      Llm.CLAUDE_4_SONNET_2025_05_14: \"anthropic\",\n      Llm.CLAUDE_4_OPUS_2025_05_14: \"anthropic\",\n-\n      # Gemini models\n      Llm.GEMINI_2_0_FLASH_EXP: \"gemini\",\n      Llm.GEMINI_2_0_FLASH: \"gemini\",\n      Llm.GEMINI_2_0_PRO_EXP: \"gemini\",\n      Llm.GEMINI_2_5_FLASH_PREVIEW_05_20: \"gemini\",\n-    Llm.GEMINI_2_5_PRO_PREVIEW_05_06: \"gemini\",\n  }\n  # Convenience sets for membership checks",
    "comment": "Update llm.py",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "4554548089ec210a7837d73afec477f280457222",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: backend/routes/generate_code.py (modified)\n          # Extract prompt content\n          prompt = params.get(\"prompt\", {\"text\": \"\", \"images\": []})\n-        \n+\n          # Extract history (default to empty list)\n          history = params.get(\"history\", [])\n-        \n+\n          # Extract imported code flag\n          is_imported_from_code = params.get(\"isImportedFromCode\", False)\n      ) -> List[Llm]:\n          \"\"\"Simple model cycling that scales with num_variants\"\"\"\n-        # Determine primary Claude model based on generation type\n-        if generation_type == \"create\":\n-            claude_model = Llm.CLAUDE_3_7_SONNET_2025_02_19\n-        else:\n-            claude_model = Llm.CLAUDE_3_5_SONNET_2024_06_20\n+        claude_model = Llm.CLAUDE_3_7_SONNET_2025_02_19\n          # For text input mode, use Claude 4 Sonnet as third option\n          # For other input modes (image/video), use Gemini as third option",
    "comment": "update models",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "326c2d501d94d8e7de8d5e93edc7a9bc372a2bb0",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: frontend/src/tests/qa.test.ts (modified)\n  // Results\n  const RESULTS_DIR = `${TESTS_ROOT_PATH}/results`;\n-describe(\"e2e tests\", () => {\n+describe.skip(\"e2e tests\", () => {\n    let browser: Browser;\n    let page: Page;",
    "comment": "Update qa.test.ts",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "d1cef10127ccbf3bb091d70279b6ed32b4e17763",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: backend/custom_types.py (modified)\n  InputMode = Literal[\n      \"image\",\n      \"video\",\n+    \"text\",\n  ]\n\nFile: backend/prompts/__init__.py (modified)\n  from image_generation.core import create_alt_url_mapping\n  from prompts.imported_code_prompts import IMPORTED_CODE_SYSTEM_PROMPTS\n  from prompts.screenshot_system_prompts import SYSTEM_PROMPTS\n+from prompts.text_prompts import SYSTEM_PROMPTS as TEXT_SYSTEM_PROMPTS\n  from prompts.types import Stack\n  from video.utils import assemble_claude_prompt_video\n              prompt_messages.append(message)\n      else:\n          # Assemble the prompt for non-imported code\n-        if params.get(\"resultImage\"):\n-            prompt_messages = assemble_prompt(\n-                params[\"image\"], stack, params[\"resultImage\"]\n-            )\n+        if input_mode == \"image\":\n+            if params.get(\"resultImage\"):\n+                prompt_messages = assemble_prompt(\n+                    params[\"image\"], stack, params[\"resultImage\"]\n+                )\n+            else:\n+                prompt_messages = assemble_prompt(params[\"image\"], stack)\n\nFile: backend/prompts/types.py (modified)\n  class SystemPrompts(TypedDict):\n      html_css: str\n      html_tailwind: str\n+    html_tailwind_thinking: str\n      react_tailwind: str\n      bootstrap: str\n      ionic_tailwind: str\n+    ionic_react: str\n      vue_tailwind: str\n      svg: str\n  Stack = Literal[\n      \"html_css\",\n      \"html_tailwind\",\n+    \"html_tailwind_thinking\",\n      \"react_tailwind\",\n      \"bootstrap\",\n      \"ionic_tailwind\",\n+    \"ionic_react\",\n      \"vue_tailwind\",\n      \"svg\",\n\nFile: backend/routes/generate_code.py (modified)\n      async def select_models(\n          self,\n          generation_type: Literal[\"create\", \"update\"],\n+        input_mode: InputMode,\n          openai_api_key: str | None,\n          anthropic_api_key: str | None,\n          gemini_api_key: str | None = None,\n          try:\n              variant_models = self._get_variant_models(\n                  generation_type,\n+                input_mode,\n                  NUM_VARIANTS,\n                  openai_api_key,\n                  anthropic_api_key,\n      def _get_variant_models(\n          self,\n          generation_type: Literal[\"create\", \"update\"],\n+        input_mode: InputMode,\n          num_variants: int,\n          openai_api_key: str | None,",
    "comment": "add text generation",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "c8651dd939087af1742b339bb4cb3d1bb6b9bd31",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: docs/conf.py (modified)\n  extlinks = {\n      \"issue\": (\"https://github.com/pallets/flask/issues/%s\", \"#%s\"),\n      \"pr\": (\"https://github.com/pallets/flask/pull/%s\", \"#%s\"),\n+    \"ghsa\": (\"https://github.com/pallets/flask/security/advisories/GHSA-%s\", \"GHSA-%s\"),\n  }\n  intersphinx_mapping = {\n      \"python\": (\"https://docs.python.org/3/\", None),",
    "comment": "add ghsa links",
    "language": "diff",
    "repo": "pallets/flask",
    "sha": "bfffe87d4c2ea255b9a51432bebb3d28741245c4",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: js/controllers/keyboard.js (modified)\n  \t\t\t}\n  \t\t}\n-\t\tif( this.Reveal.isOverlayOpen() && !['Escape', 'f', 'c'].includes(event.key) ) {\n+\t\tif( this.Reveal.isOverlayOpen() && ![\"Escape\", \"f\", \"c\", \"b\", \".\"].includes(event.key) ) {\n  \t\t\treturn false;\n  \t\t}",
    "comment": "fix pause overlay",
    "language": "diff",
    "repo": "hakimel/reveal.js",
    "sha": "1e0a2a7d4a2864a7acbabcf808cf6db78a3955b1",
    "quality_score": 0.6000000000000001,
    "comment_type": "commit_message"
  },
  {
    "code": "File: web_programming/fetch_well_rx_price.py (modified)\n  \"\"\"\n-from urllib.error import HTTPError\n-\n+import httpx\n  from bs4 import BeautifulSoup\n-from requests import exceptions, get\n-BASE_URL = \"https://www.wellrx.com/prescriptions/{0}/{1}/?freshSearch=true\"\n+BASE_URL = \"https://www.wellrx.com/prescriptions/{}/{}/?freshSearch=true\"\n  def fetch_pharmacy_and_price_list(drug_name: str, zip_code: str) -> list | None:\n      \"\"\"[summary]\n      This function will take input of drug name and zipcode,\n      then request to the BASE_URL site.\n-    Get the page data and scrape it to the generate the\n-    list of lowest prices for the prescription drug.\n+    Get the page data and scrape it to generate the\n+    list of the lowest prices for the prescription drug.\n      Args:\n          drug_name (str): [Drug name]\n      Returns:\n          list: [List of pharmacy name and price]",
    "comment": "Update .pre-commit-config.yaml to reflect current linting and formatt… (#12841)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "5a4a6a5497776c564774fa915792924eee36865d",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: maths/test_factorial.py (added)\n+# /// script\n+# requires-python = \">=3.13\"\n+# dependencies = [\n+#     \"pytest\",\n+# ]\n+# ///\n+\n+import pytest\n+\n+from maths.factorial import factorial, factorial_recursive\n+\n+\n+@pytest.mark.parametrize(\"function\", [factorial, factorial_recursive])\n+def test_zero(function):\n+    assert function(0) == 1\n+\n+\n+@pytest.mark.parametrize(\"function\", [factorial, factorial_recursive])\n+def test_positive_integers(function):\n+    assert function(1) == 1\n\nFile: web_programming/current_stock_price.py (modified)\n  def stock_price(symbol: str = \"AAPL\") -> str:\n      \"\"\"\n      >>> stock_price(\"EEEE\")\n-    '- '\n+    'No <fin-streamer> tag with the specified data-testid attribute found.'\n      >>> isinstance(float(stock_price(\"GOOG\")),float)\n      True\n      \"\"\"",
    "comment": "Add unit tests for factorial.py (#12815)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "7665ba5e257a1a42caa36d781925d87968ee1497",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: strings/boyer_moore_search.py (modified)\n  a shift is proposed that moves the entirety of Pattern past\n  the point of mismatch in the text.\n-If there no mismatch then the pattern matches with text block.\n+If there is no mismatch then the pattern matches with text block.\n  Time Complexity : O(n/m)\n      n=length of main string\n      m=length of pattern string\n  \"\"\"\n-from __future__ import annotations\n-\n  class BoyerMooreSearch:\n+    \"\"\"\n+    Example usage:\n+\n+        bms = BoyerMooreSearch(text=\"ABAABA\", pattern=\"AB\")\n+        positions = bms.bad_character_heuristic()\n+\n+    where 'positions' contain the locations where the pattern was matched.\n+    \"\"\"\n+",
    "comment": "Add doctests for the boyer_moore_search algorithm. (#12769)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "c3d4b9e54d65c90422a0863c38b1ca72e6037bc9",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: maths/radix2_fft.py (modified)\n      Print product\r\n      >>> x.product  # 2x + 3x^2 + 8x^3 + 4x^4 + 6x^5\r\n-    [(-0+0j), (2+0j), (3+0j), (8+0j), (6+0j), (8+0j)]\r\n+    [(-0-0j), (2+0j), (3-0j), (8-0j), (6+0j), (8+0j)]\r\n      __str__ test\r\n      >>> print(x)\r\n      A = 0*x^0 + 1*x^1 + 2*x^0 + 3*x^2\r\n      B = 0*x^2 + 1*x^3 + 2*x^4\r\n-    A*B = 0*x^(-0+0j) + 1*x^(2+0j) + 2*x^(3+0j) + 3*x^(8+0j) + 4*x^(6+0j) + 5*x^(8+0j)\r\n+    A*B = 0*x^(-0-0j) + 1*x^(2+0j) + 2*x^(3-0j) + 3*x^(8-0j) + 4*x^(6+0j) + 5*x^(8+0j)\r\n      \"\"\"\r\n      def __init__(self, poly_a=None, poly_b=None):\r\n              inverce_c = new_inverse_c\r\n              next_ncol *= 2\r\n          # Unpack\r\n-        inverce_c = [round(x[0].real, 8) + round(x[0].imag, 8) * 1j for x in inverce_c]\r\n+        inverce_c = [\r\n+            complex(round(x[0].real, 8), round(x[0].imag, 8)) for x in inverce_c\r\n+        ]\r\n          # Remove leading 0's\r",
    "comment": "maths/radix2_fft.py: Fix calculation for Python 3.14 (#12772)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "d0d7f0b18ab5cca2c573448b757c45bc0eee69c6",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: backtracking/sum_of_subsets.py (modified)\n  \"\"\"\n-The sum-of-subsetsproblem states that a set of non-negative integers, and a\n+The sum-of-subsets problem states that a set of non-negative integers, and a\n  value M, determine all possible subsets of the given set whose summation sum\n  equal to given M.\n  Summation of the chosen numbers must be equal to given number M and one number\n  can be used only once.\n  \"\"\"\n-from __future__ import annotations\n+def generate_sum_of_subsets_solutions(nums: list[int], max_sum: int) -> list[list[int]]:\n+    \"\"\"\n+    The main function. For list of numbers 'nums' find the subsets with sum\n+    equal to 'max_sum'\n+\n+    >>> generate_sum_of_subsets_solutions(nums=[3, 34, 4, 12, 5, 2], max_sum=9)\n+    [[3, 4, 2], [4, 5]]\n+    >>> generate_sum_of_subsets_solutions(nums=[3, 34, 4, 12, 5, 2], max_sum=3)\n+    [[3]]\n+    >>> generate_sum_of_subsets_solutions(nums=[3, 34, 4, 12, 5, 2], max_sum=1)\n+    []",
    "comment": "Add tests and cleanup sum_of_subsets algorithm (#12746)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "e1115b5f15afa44deb4752483b9b456457f7e683",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: data_structures/hashing/hash_map.py (modified)\n  VAL = TypeVar(\"VAL\")\n-@dataclass(frozen=True, slots=True)\n+@dataclass(slots=True)\n  class _Item(Generic[KEY, VAL]):\n      key: KEY\n      val: VAL\n          If bucket is empty or key is the same, does insert and return True.\n-        If bucket has another key or deleted placeholder,\n-        that means that we need to check next bucket.\n+        If bucket has another key that means that we need to check next bucket.\n          \"\"\"\n          stored = self._buckets[ind]\n          if not stored:\n+            # A falsy item means that bucket was never used (None)\n+            # or was deleted (_deleted).\n              self._buckets[ind] = _Item(key, val)\n              self._len += 1\n              return True\n          elif stored.key == key:\n-            self._buckets[ind] = _Item(key, val)",
    "comment": "Improve hash map (#12678)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "c81cc269962b346420b9e551782bd836dedb4528",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: searches/quick_select.py (modified)\n      # must be in larger\n      else:\n          return quick_select(larger, index - (m + count))\n+\n+\n+def median(items: list):\n+    \"\"\"\n+    One common application of Quickselect is finding the median, which is\n+    the middle element (or average of the two middle elements) in a sorted dataset.\n+    It works efficiently on unsorted lists by partially sorting the data without\n+    fully sorting the entire list.\n+\n+    >>> median([3, 2, 2, 9, 9])\n+    3\n+\n+    >>> median([2, 2, 9, 9, 9, 3])\n+    6.0\n+    \"\"\"\n+    mid, rem = divmod(len(items), 2)\n+    if rem != 0:",
    "comment": "Add median() function using Quickselect (#12676)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "26ad6891d8bdadc7dd88a2c91cdb28977a7a9dc0",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: physics/orbital_transfer_work.py (added)\n+def orbital_transfer_work(\n+    mass_central: float, mass_object: float, r_initial: float, r_final: float\n+) -> str:\n+    \"\"\"\n+    Calculates the work required to move an object from one orbit to another in a\n+    gravitational field based on the change in total mechanical energy.\n+\n+    The formula used is:\n+        W = (G * M * m / 2) * (1/r_initial - 1/r_final)\n+\n+    where:\n+        W = work done (Joules)\n+        G = gravitational constant (6.67430 * 10^-11 m^3 kg^-1 s^-2)\n+        M = mass of the central body (kg)\n+        m = mass of the orbiting object (kg)\n+        r_initial = initial orbit radius (m)\n+        r_final = final orbit radius (m)\n+\n+    Args:\n+        mass_central (float): Mass of the central body (kg)",
    "comment": "Physics orbital_transfer_work (#12728)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "6e4d1b376546cdf347b6513ff5f493510b956309",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: scripts/validate_solutions.py (modified)\n  #!/usr/bin/env python3\n+\n+# /// script\n+# requires-python = \">=3.13\"\n+# dependencies = [\n+#     \"pytest\",\n+#     \"requests\",\n+# ]\n+# ///\n+\n  import hashlib\n  import importlib.util\n  import json",
    "comment": "Add PEP723 header to scripts/validate_solutions.py (#12731)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "485f688d0683ca026959e1b68f12e5d221724860",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: physics/escape_velocity.py (added)\n+import math\n+\n+\n+def escape_velocity(mass: float, radius: float) -> float:\n+    \"\"\"\n+    Calculates the escape velocity needed to break free from a celestial body's\n+    gravitational field.\n+\n+    The formula used is:\n+        v = sqrt(2 * G * M / R)\n+\n+    where:\n+        v = escape velocity (m/s)\n+        G = gravitational constant (6.67430 * 10^-11 m^3 kg^-1 s^-2)\n+        M = mass of the celestial body (kg)\n+        R = radius from the center of mass (m)\n+\n+    Source:\n+        https://en.wikipedia.org/wiki/Escape_velocity\n+",
    "comment": "Add escape velocity calculator using standard physics formula (#12721)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "95fb181f5a944427fdbc5766cbf4e1cb699d4a6d",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: physics/horizontal_projectile_motion.py (modified)\n  \"\"\"\n  # Importing packages\n-from math import radians as angle_to_radians\n+from math import radians as deg_to_rad\n  from math import sin\n  # Acceleration Constant on Earth (unit m/s^2)\n      # Ensure valid instance\n      if not isinstance(init_velocity, (int, float)):\n-        raise TypeError(\"Invalid velocity. Should be a positive number.\")\n+        raise TypeError(\"Invalid velocity. Should be an integer or float.\")\n      if not isinstance(angle, (int, float)):\n-        raise TypeError(\"Invalid angle. Range is 1-90 degrees.\")\n+        raise TypeError(\"Invalid angle. Should be an integer or float.\")\n      # Ensure valid angle\n      if angle > 90 or angle < 1:\n      ValueError: Invalid angle. Range is 1-90 degrees.\n      \"\"\"\n      check_args(init_velocity, angle)\n-    radians = angle_to_radians(2 * angle)\n+    radians = deg_to_rad(2 * angle)",
    "comment": " Fix error messages for horizontal_projectile_motion.py (#12722)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "131765574f5ccfaff4214a6f848412ce2fe4ab20",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: dynamic_programming/longest_common_substring.py (modified)\n      if not (isinstance(text1, str) and isinstance(text2, str)):\n          raise ValueError(\"longest_common_substring() takes two strings for inputs\")\n+    if not text1 or not text2:\n+        return \"\"\n+\n      text1_length = len(text1)\n      text2_length = len(text2)\n      dp = [[0] * (text2_length + 1) for _ in range(text1_length + 1)]\n-    ans_index = 0\n-    ans_length = 0\n+    end_pos = 0\n+    max_length = 0\n      for i in range(1, text1_length + 1):\n          for j in range(1, text2_length + 1):\n              if text1[i - 1] == text2[j - 1]:\n                  dp[i][j] = 1 + dp[i - 1][j - 1]\n-                if dp[i][j] > ans_length:\n-                    ans_index = i\n-                    ans_length = dp[i][j]\n+                if dp[i][j] > max_length:",
    "comment": "Improve longest_common_substring.py (#12705)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "47a44abe23870ca0f7c8062601278645039b1c70",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: boolean_algebra/and_gate.py (modified)\n  \"\"\"\n-An AND Gate is a logic gate in boolean algebra which results to 1 (True) if both the\n-inputs are 1, and 0 (False) otherwise.\n+An AND Gate is a logic gate in boolean algebra which results to 1 (True) if all the\n+inputs are 1 (True), and 0 (False) otherwise.\n-Following is the truth table of an AND Gate:\n+Following is the truth table of a Two Input AND Gate:\n      ------------------------------\n      | Input 1 | Input 2 | Output |\n      ------------------------------\n      |    1    |    1    |    1   |\n      ------------------------------\n-Refer - https://www.geeksforgeeks.org/logic-gates-in-python/\n+Refer - https://www.geeksforgeeks.org/logic-gates/\n  \"\"\"\n      return int(input_1 and input_2)\n+def n_input_and_gate(inputs: list[int]) -> int:\n+    \"\"\"\n+    Calculate AND of a list of input values\n+",
    "comment": "Add N Input AND Gate (#12717)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "59c3c8bbf384ef05d3cb9862a41bba39bb098fe9",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: data_structures/binary_tree/lowest_common_ancestor.py (modified)\n      (4, 3)\n      >>> swap(67, 12)\n      (12, 67)\n+    >>> swap(3,-4)\n+    (-4, 3)\n      \"\"\"\n      a ^= b\n      b ^= a\n  def create_sparse(max_node: int, parent: list[list[int]]) -> list[list[int]]:\n      \"\"\"\n      creating sparse table which saves each nodes 2^i-th parent\n+    >>> max_node = 6\n+    >>> parent = [[0, 0, 1, 1, 2, 2, 3]] + [[0] * 7 for _ in range(19)]\n+    >>> parent = create_sparse(max_node=max_node, parent=parent)\n+    >>> parent[0]\n+    [0, 0, 1, 1, 2, 2, 3]\n+    >>> parent[1]\n+    [0, 0, 0, 0, 1, 1, 1]\n+    >>> parent[2]\n+    [0, 0, 0, 0, 0, 0, 0]",
    "comment": "Added/Improved doctests for lowest_common_ancestor.py (#12673)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "a728cc96ab4f05248ac3389365a26f01dfaf6f8e",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: project_euler/problem_190/sol1.py (added)\n+\"\"\"\n+Project Euler Problem 190: https://projecteuler.net/problem=190\n+\n+Maximising a Weighted Product\n+\n+Let S_m = (x_1, x_2, ..., x_m) be the m-tuple of positive real numbers with\n+x_1 + x_2 + ... + x_m = m for which P_m = x_1 * x_2^2 * ... * x_m^m is maximised.\n+\n+For example, it can be verified that |_ P_10 _| = 4112\n+(|_ _| is the integer part function).\n+\n+Find Sum_{m=2}^15 = |_ P_m _|.\n+\n+Solution:\n+- Fix x_1 = m - x_2 - ... - x_m.\n+- Calculate partial derivatives of P_m wrt the x_2, ..., x_m. This gives that\n+  x_2 = 2 * x_1, x_3 = 3 * x_1, ..., x_m = m * x_1.\n+- Calculate partial second order derivatives of P_m wrt the x_2, ..., x_m.\n+  By plugging in the values from the previous step, can verify that solution is maximum.\n+\"\"\"",
    "comment": "Add solution for the Euler problem 190 (#12664)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "40f4c510b6047d95d07f03c9915a53bbf84789e4",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: project_euler/problem_164/sol1.py (added)\n+\"\"\"\n+Project Euler Problem 164: https://projecteuler.net/problem=164\n+\n+Three Consecutive Digital Sum Limit\n+\n+How many 20 digit numbers n (without any leading zero) exist such that no three\n+consecutive digits of n have a sum greater than 9?\n+\n+Brute-force recursive solution with caching of intermediate results.\n+\"\"\"\n+\n+\n+def solve(\n+    digit: int, prev1: int, prev2: int, sum_max: int, first: bool, cache: dict[str, int]\n+) -> int:\n+    \"\"\"\n+    Solve for remaining 'digit' digits, with previous 'prev1' digit, and\n+    previous-previous 'prev2' digit, total sum of 'sum_max'.\n+    Pass around 'cache' to store/reuse intermediate results.\n+",
    "comment": "Add solution for the Euler project problem 164. (#12663)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "145879b8b2546c74fc51446ac607823876a0f601",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: data_structures/linked_list/doubly_linked_list_two.py (modified)\n  \"\"\"\n  from dataclasses import dataclass\n-from typing import Self\n+from typing import Self, TypeVar\n+\n+DataType = TypeVar(\"DataType\")\n  @dataclass\n-class Node:\n-    data: int\n+class Node[DataType]:\n+    data: DataType\n      previous: Self | None = None\n      next: Self | None = None\n              current = current.next\n          return \" \".join(str(node) for node in nodes)\n-    def __contains__(self, value: int):\n+    def __contains__(self, value: DataType):\n          current = self.head\n          while current:\n              if current.data == value:",
    "comment": "Generic type hint in DDL (#12677)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "11a61d15dc3aebc69f153adca8568076a25f7110",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: financial/time_and_half_pay.py (added)\n+\"\"\"\n+Calculate time and a half pay\n+\"\"\"\n+\n+\n+def pay(hours_worked: float, pay_rate: float, hours: float = 40) -> float:\n+    \"\"\"\n+    hours_worked = The total hours worked\n+    pay_rate = Amount of money per hour\n+    hours = Number of hours that must be worked before you receive time and a half\n+\n+    >>> pay(41, 1)\n+    41.5\n+    >>> pay(65, 19)\n+    1472.5\n+    >>> pay(10, 1)\n+    10.0\n+    \"\"\"\n+    # Check that all input parameters are float or integer\n+    assert isinstance(hours_worked, (float, int)), (",
    "comment": "Adding time and a half pay calculator algorithm to financial folder (#12662)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "a1aa6313e08657f0e9ae337afa81d6b6f95357c9",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: project_euler/problem_136/sol1.py (added)\n+\"\"\"\n+Project Euler Problem 136: https://projecteuler.net/problem=136\n+\n+Singleton Difference\n+\n+The positive integers, x, y, and z, are consecutive terms of an arithmetic progression.\n+Given that n is a positive integer, the equation, x^2 - y^2 - z^2 = n,\n+has exactly one solution when n = 20:\n+                              13^2 - 10^2 - 7^2 = 20.\n+\n+In fact there are twenty-five values of n below one hundred for which\n+the equation has a unique solution.\n+\n+How many values of n less than fifty million have exactly one solution?\n+\n+By change of variables\n+\n+x = y + delta\n+z = y - delta\n+",
    "comment": "Solution for the Euler Project problem 136 (#12658)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "c585cb122718e219870ea7a2af110939b55e52f9",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: linear_algebra/matrix_inversion.py (added)\n+import numpy as np\n+\n+\n+def invert_matrix(matrix: list[list[float]]) -> list[list[float]]:\n+    \"\"\"\n+    Returns the inverse of a square matrix using NumPy.\n+\n+    Parameters:\n+    matrix (list[list[float]]): A square matrix.\n+\n+    Returns:\n+    list[list[float]]: Inverted matrix if invertible, else raises error.\n+\n+    >>> invert_matrix([[4.0, 7.0], [2.0, 6.0]])\n+    [[0.6000000000000001, -0.7000000000000001], [-0.2, 0.4]]\n+    >>> invert_matrix([[1.0, 2.0], [0.0, 0.0]])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: Matrix is not invertible\n+    \"\"\"",
    "comment": "Add matrix inversion algorithm using NumPy (#12657)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "42820634f3795e7d7397ad2e688d091a79c1eb83",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: project_euler/problem_122/sol1.py (added)\n+\"\"\"\n+Project Euler Problem 122: https://projecteuler.net/problem=122\n+\n+Efficient Exponentiation\n+\n+The most naive way of computing n^15 requires fourteen multiplications:\n+\n+                                               n x n x ... x n = n^15.\n+\n+But using a \"binary\" method you can compute it in six multiplications:\n+\n+                                                         n x n = n^2\n+                                                     n^2 x n^2 = n^4\n+                                                     n^4 x n^4 = n^8\n+                                                     n^8 x n^4 = n^12\n+                                                    n^12 x n^2 = n^14\n+                                                      n^14 x n = n^15\n+\n+However it is yet possible to compute it in only five multiplications:\n+",
    "comment": "Solution for the Euler Project Problem 122 (#12655)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "d123cbc649e0777717b02dc74b4466872941a8d5",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: bit_manipulation/find_unique_number.py (added)\n+def find_unique_number(arr: list[int]) -> int:\n+    \"\"\"\n+    Given a list of integers where every element appears twice except for one,\n+    this function returns the element that appears only once using bitwise XOR.\n+\n+    >>> find_unique_number([1, 1, 2, 2, 3])\n+    3\n+    >>> find_unique_number([4, 5, 4, 6, 6])\n+    5\n+    >>> find_unique_number([7])\n+    7\n+    >>> find_unique_number([10, 20, 10])\n+    20\n+    >>> find_unique_number([])\n+    Traceback (most recent call last):\n+        ...\n+    ValueError: input list must not be empty\n+    >>> find_unique_number([1, 'a', 1])\n+    Traceback (most recent call last):\n+        ...",
    "comment": "Add find_unique_number algorithm to bit manipulation (#12654)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "cc621f1fddac7391389270d7bc38326507b4b495",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: data_structures/linked_list/doubly_linked_list_two.py (modified)\n              self.insert_before_node(self.head, node)\n      def set_tail(self, node: Node) -> None:\n-        if self.head is None:\n-            self.set_head(node)\n+        if self.tail is None:\n+            self.head = node\n+            self.tail = node\n          else:\n              self.insert_after_node(self.tail, node)\n          node.previous = node_to_insert\n-    def insert_after_node(self, node: Node | None, node_to_insert: Node) -> None:\n-        assert node is not None\n-\n+    def insert_after_node(self, node: Node, node_to_insert: Node) -> None:\n          node_to_insert.previous = node\n          node_to_insert.next = node.next\n                  return\n              current_position += 1\n              node = node.next\n-        self.insert_after_node(self.tail, new_node)",
    "comment": "Fix bug for data_structures/linked_list/doubly_linked_list_two.py (#12651)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "0c8cf8e9871a5f91182d767adf173dccf87c2c0f",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: data_structures/linked_list/doubly_linked_list_two.py (modified)\n       Delete operation is more efficient\n  \"\"\"\n+from dataclasses import dataclass\n+from typing import Self\n+\n+@dataclass\n  class Node:\n-    def __init__(self, data: int, previous=None, next_node=None):\n-        self.data = data\n-        self.previous = previous\n-        self.next = next_node\n+    data: int\n+    previous: Self | None = None\n+    next: Self | None = None\n      def __str__(self) -> str:\n          return f\"{self.data}\"\n-    def get_data(self) -> int:\n-        return self.data\n-\n-    def get_next(self):",
    "comment": "doubly linked list: add dataclass and typing (#12647)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "baab802965c37fa1740054a559cad8c119b2ee35",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: data_structures/stacks/prefix_evaluation.py (modified)\n  \"\"\"\n-Python3 program to evaluate a prefix expression.\n+Program to evaluate a prefix expression.\n+https://en.wikipedia.org/wiki/Polish_notation\n  \"\"\"\n-calc = {\n+operators = {\n      \"+\": lambda x, y: x + y,\n      \"-\": lambda x, y: x - y,\n      \"*\": lambda x, y: x * y,\n      21\n      >>> evaluate(\"/ * 10 2 + 4 1 \")\n      4.0\n+    >>> evaluate(\"2\")\n+    2\n+    >>> evaluate(\"+ * 2 3 / 8 4\")\n+    8.0\n      \"\"\"\n      stack = []\n              # push the result onto the stack again",
    "comment": "prefix_evaluation: Add alternative recursive implementation (#12646)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "f10a5cbfccc5ee9ddb5ddd9906591ecaad58f672",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: genetic_algorithm/basic_string.py (modified)\n          # Random population created. Now it's time to evaluate.\n-        # Adding a bit of concurrency can make everything faster,\n+        # (Option 1) Adding a bit of concurrency can make everything faster,\n          #\n          # import concurrent.futures\n          # population_score: list[tuple[str, float]] = []\n          # with concurrent.futures.ThreadPoolExecutor(\n          #                                   max_workers=NUM_WORKERS) as executor:\n-        #     futures = {executor.submit(evaluate, item) for item in population}\n+        #     futures = {executor.submit(evaluate, item, target) for item in population}\n          #     concurrent.futures.wait(futures)\n          #     population_score = [item.result() for item in futures]\n          #\n          # but with a simple algorithm like this, it will probably be slower.\n-        # We just need to call evaluate for every item inside the population.\n+        # (Option 2) We just need to call evaluate for every item inside the population.\n          population_score = [evaluate(item, target) for item in population]\n          # Check if there is a matching evolution.",
    "comment": "Genetic Algorithm: Fix bug in multi-threading (#12644)",
    "language": "diff",
    "repo": "TheAlgorithms/Python",
    "sha": "74b540ad73bd3b1187ed6e3c89bb8f309ef543fd",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/data-structures/disjoint-set/DisjointSetAdhoc.js (added)\n+/**\n+ * The minimalistic (ad hoc) version of a DisjointSet (or a UnionFind) data structure\n+ * that doesn't have external dependencies and that is easy to copy-paste and\n+ * use during the coding interview if allowed by the interviewer (since many\n+ * data structures in JS are missing).\n+ *\n+ * Time Complexity:\n+ *\n+ * - Constructor: O(N)\n+ * - Find: O(α(N))\n+ * - Union: O(α(N))\n+ * - Connected: O(α(N))\n+ *\n+ * Where N is the number of vertices in the graph.\n+ * α refers to the Inverse Ackermann function.\n+ * In practice, we assume it's a constant.\n+ * In other words, O(α(N)) is regarded as O(1) on average.\n+ */\n+class DisjointSetAdhoc {\n+  /**\n\nFile: src/data-structures/disjoint-set/__test__/DisjointSetAdhoc.test.js (added)\n+import DisjointSetAdhoc from '../DisjointSetAdhoc';\n+\n+describe('DisjointSetAdhoc', () => {\n+  it('should create unions and find connected elements', () => {\n+    const set = new DisjointSetAdhoc(10);\n+\n+    // 1-2-5-6-7 3-8-9 4\n+    set.union(1, 2);\n+    set.union(2, 5);\n+    set.union(5, 6);\n+    set.union(6, 7);\n+\n+    set.union(3, 8);\n+    set.union(8, 9);\n+\n+    expect(set.connected(1, 5)).toBe(true);\n+    expect(set.connected(5, 7)).toBe(true);\n+    expect(set.connected(3, 8)).toBe(true);\n+\n+    expect(set.connected(4, 9)).toBe(false);",
    "comment": "Ad hoc versions of MinHeap, MaxHeap, and DisjointSet (#1117)",
    "language": "diff",
    "repo": "trekhleb/javascript-algorithms",
    "sha": "2c67b48c21eed86aafbb4d09065ffe391b4fc7e4",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/data-structures/lru-cache/LinkedListNode.js (added)\n+class LinkedListNode {\n+  /**\n+   * Creates a doubly-linked list node.\n+   * @param {string} key\n+   * @param {any} val\n+   * @param {LinkedListNode} prev\n+   * @param {LinkedListNode} next\n+   */\n+  constructor(key, val, prev = null, next = null) {\n+    this.key = key;\n+    this.val = val;\n+    this.prev = prev;\n+    this.next = next;\n+  }\n+}\n+\n+export default LinkedListNode;",
    "comment": "Add an example of the LRU (Least Recently Used) Cache implementation (#980)",
    "language": "diff",
    "repo": "trekhleb/javascript-algorithms",
    "sha": "e4f2ccdbece78d44d3f4bd908c828408b1c0c791",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/algorithms/sets/power-set/__test__/caPowerSet.test.js (added)\n+import caPowerSet from '../caPowerSet';\n+\n+describe('caPowerSet', () => {\n+  it('should calculate power set of given set using cascading approach', () => {\n+    expect(caPowerSet([1])).toEqual([\n+      [],\n+      [1],\n+    ]);\n+\n+    expect(caPowerSet([1, 2])).toEqual([\n+      [],\n+      [1],\n+      [2],\n+      [1, 2],\n+    ]);\n+\n+    expect(caPowerSet([1, 2, 3])).toEqual([\n+      [],\n+      [1],\n+      [2],\n\nFile: src/algorithms/sets/power-set/caPowerSet.js (added)\n+/**\n+ * Find power-set of a set using CASCADING approach.\n+ *\n+ * @param {*[]} originalSet\n+ * @return {*[][]}\n+ */\n+export default function caPowerSet(originalSet) {\n+  // Let's start with an empty set.\n+  const sets = [[]];\n+\n+  /*\n+    Now, let's say:\n+    originalSet = [1, 2, 3].\n+\n+    Let's add the first element from the originalSet to all existing sets:\n+    [[]] ← 1 = [[], [1]]\n+\n+    Adding the 2nd element to all existing sets:\n+    [[], [1]] ← 2 = [[], [1], [2], [1, 2]]\n+",
    "comment": "Adding a simple cascading solution to generate a Power Set (#975)",
    "language": "diff",
    "repo": "trekhleb/javascript-algorithms",
    "sha": "65e4a7c8b35914e70bfc2b7cbc58c5a9f654db53",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: third_party/xla/build_tools/ci/build.py (modified)\n  Build(\n      type_=BuildType.XLA_LINUX_X86_GPU_ONEAPI_GITHUB_ACTIONS,\n      repo=\"openxla/xla\",\n-    configs=(\"sycl\", \"sycl_hermetic\"),\n+    configs=(\"sycl\", \"sycl_hermetic\", \"icpx_clang\"),\n      target_patterns=_XLA_ONEAPI_TARGET_PATTERNS,\n      build_tag_filters=oneapi_build_tag_filter,\n      test_tag_filters=oneapi_test_tag_filter,\n\nFile: third_party/xla/build_tools/configure/configure.py (modified)\n    HIPCC = enum.auto()\n+class SyclCompiler(ArgparseableEnum):\n+  ICPX = enum.auto()\n+\n+\n  class OS(ArgparseableEnum):\n    \"\"\"Modeled after the values returned by `platform.system()`.\"\"\"\n    LINUX = enum.auto()\n    # ROCM specific\n    rocm_compiler: RocmCompiler\n+  # SYCL specific\n+  sycl_compiler: SyclCompiler\n+\n    def to_bazelrc_lines(\n        self,\n        dpav: DiscoverablePathsAndVersions,\n        build_and_test_tag_filters.append(\"-rocm-only\")\n        build_and_test_tag_filters.append(\"-no-oneapi\")\n-      rc.append(\"build --config sycl\")\n+      compiler_pair = self.sycl_compiler, self.host_compiler\n\nFile: third_party/xla/build_tools/configure/configure_test.py (modified)\n  HostCompiler = configure.HostCompiler\n  CudaCompiler = configure.CudaCompiler\n  RocmCompiler = configure.RocmCompiler\n+SyclCompiler = configure.SyclCompiler\n  OS = configure.OS\n  _PYTHON_BIN_PATH = \"/usr/bin/python3\"\n          cuda_compiler=CudaCompiler.NVCC,\n          using_nccl=False,\n          rocm_compiler=RocmCompiler.HIPCC,\n+        sycl_compiler=SyclCompiler.ICPX,\n      )\n      bazelrc_lines = config.to_bazelrc_lines(\n          cuda_compiler=CudaCompiler.NVCC,\n          using_nccl=False,\n          rocm_compiler=RocmCompiler.HIPCC,\n+        sycl_compiler=SyclCompiler.ICPX,\n      )\n      bazelrc_lines = config.to_bazelrc_lines(\n          cuda_compiler=CudaCompiler.CLANG,\n          using_nccl=False,",
    "comment": "PR #27904: [XLA:GPU][oneAPI] Enable Clang compiler as the host compiler",
    "language": "diff",
    "repo": "tensorflow/tensorflow",
    "sha": "71166cd2810b46cfca423efe31a65dcb609325f4",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/utils/auto_docstring.py (modified)\n              if place_holder_value is not None:\n                  if isinstance(place_holder_value, (list, tuple)):\n                      place_holder_value = place_holder_value[0]\n-                placeholders_dict[placeholder] = place_holder_value\n+                placeholders_dict[placeholder] = place_holder_value if place_holder_value is not None else placeholder\n              else:\n                  placeholders_dict[placeholder] = placeholder",
    "comment": "Fix auto_docstring crashing when dependencies are missing (#39564)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "a26f0fabb8654131157a763ac6e742fc424cf8db",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/__init__.py (modified)\n      from .decision_transformer import *\n      from .deepseek_v2 import *\n      from .deepseek_v3 import *\n+    from .deepseek_vl import *\n+    from .deepseek_vl_hybrid import *\n      from .deformable_detr import *\n      from .deit import *\n      from .deprecated import *\n\nFile: src/transformers/models/auto/configuration_auto.py (modified)\n          (\"decision_transformer\", \"DecisionTransformerConfig\"),\n          (\"deepseek_v2\", \"DeepseekV2Config\"),\n          (\"deepseek_v3\", \"DeepseekV3Config\"),\n+        (\"deepseek_vl\", \"DeepseekVLConfig\"),\n+        (\"deepseek_vl_hybrid\", \"DeepseekVLHybridConfig\"),\n          (\"deformable_detr\", \"DeformableDetrConfig\"),\n          (\"deit\", \"DeiTConfig\"),\n          (\"depth_anything\", \"DepthAnythingConfig\"),\n          (\"decision_transformer\", \"Decision Transformer\"),\n          (\"deepseek_v2\", \"DeepSeek-V2\"),\n          (\"deepseek_v3\", \"DeepSeek-V3\"),\n+        (\"deepseek_vl\", \"DeepseekVL\"),\n+        (\"deepseek_vl_hybrid\", \"DeepseekVLHybrid\"),\n          (\"deformable_detr\", \"Deformable DETR\"),\n          (\"deit\", \"DeiT\"),\n          (\"deplot\", \"DePlot\"),",
    "comment": "Add support for DeepseekAI's DeepseekVL (#36248)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "69cff312f5c8026fea13029bb45b139385a88b4c",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/__init__.py (modified)\n      from .encoder_decoder import *\n      from .ernie import *\n      from .esm import *\n+    from .evolla import *\n      from .falcon import *\n      from .falcon_h1 import *\n      from .falcon_mamba import *\n\nFile: src/transformers/models/auto/configuration_auto.py (modified)\n          (\"ernie4_5_moe\", \"Ernie4_5_MoeConfig\"),\n          (\"ernie_m\", \"ErnieMConfig\"),\n          (\"esm\", \"EsmConfig\"),\n+        (\"evolla\", \"EvollaConfig\"),\n          (\"falcon\", \"FalconConfig\"),\n          (\"falcon_h1\", \"FalconH1Config\"),\n          (\"falcon_mamba\", \"FalconMambaConfig\"),\n          (\"ernie4_5_moe\", \"Ernie4_5_MoE\"),\n          (\"ernie_m\", \"ErnieM\"),\n          (\"esm\", \"ESM\"),\n+        (\"evolla\", \"Evolla\"),\n          (\"falcon\", \"Falcon\"),\n          (\"falcon3\", \"Falcon3\"),\n          (\"falcon_h1\", \"FalconH1\"),\n\nFile: src/transformers/models/auto/modeling_auto.py (modified)\n          (\"ernie4_5_moe\", \"Ernie4_5_MoeModel\"),\n          (\"ernie_m\", \"ErnieMModel\"),\n          (\"esm\", \"EsmModel\"),\n+        (\"evolla\", \"EvollaModel\"),\n          (\"falcon\", \"FalconModel\"),\n          (\"falcon_h1\", \"FalconH1Model\"),\n          (\"falcon_mamba\", \"FalconMambaModel\"),\n          (\"distilbert\", \"DistilBertForMaskedLM\"),\n          (\"electra\", \"ElectraForPreTraining\"),\n          (\"ernie\", \"ErnieForPreTraining\"),\n+        (\"evolla\", \"EvollaForProteinText2Text\"),\n          (\"falcon_mamba\", \"FalconMambaForCausalLM\"),\n          (\"flaubert\", \"FlaubertWithLMHeadModel\"),\n          (\"flava\", \"FlavaForPreTraining\"),\n          (\"blip-2\", \"Blip2ForConditionalGeneration\"),\n          (\"chameleon\", \"ChameleonForConditionalGeneration\"),\n          (\"emu3\", \"Emu3ForConditionalGeneration\"),\n+        (\"evolla\", \"EvollaForProteinText2Text\"),\n          (\"fuyu\", \"FuyuForCausalLM\"),\n          (\"gemma3\", \"Gemma3ForConditionalGeneration\"),",
    "comment": "Add evolla rebase main (#36232)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "45c7bfb1571160d2c06b880073a5c73e6bfa3677",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: tests/models/whisper/test_modeling_whisper.py (modified)\n      def test_whisper_longform_multi_batch_hard_prev_cond(self):\n          # fmt: off\n          EXPECTED_TEXT = [\n-            \" Folks, if you watch the show, you know I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories, developing the central headline pawns, definitely maneuvering an oh-so-topical night to F6, faming of classic Sicilian, named or variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a Fisher shows in lip-nitsky attack that culminates in the elegant lethal slow-played all-pass on checkmate that is my nightly monologue, but sometimes sometimes, sometimes folks I sometimes I start a little wake-up side down in the monkey bars of a condemned playground on a super fun site, get all hept up on goofballs, rummage that would discard a tag bag of defective toys, yank out a fistball of disembodied doll limbs, toss them on a stain kid's place mad from a defunct denies, set up a table inside a rusty cargo container down by the warf and challenge toothless drifters to the godless bughouse blitz of tournament that is my segment.\",\n-            \" Folks, I spent a lot of time right over there night after night, actually. Carefully selecting for you the day's newsiest, most aerodynamic headlines, stress testing on those topical anti-lock breaks and power steering, painstakingly stitching, leather seating, so soft, it would make JD power and her associates blush. To create the luxury sedan that is my nightly monologue, but sometimes I just sometimes folks, I lurched to consciousness in the back of an abandoned school bus and slapped myself awake with a crusty floor mat. Before using a mouse-bitten timing belt to strap some old plywood to a couple of discarded oil drums, then by the light of a heathen-moon render a gas tank out of an empty big gulp, filled with white claw and de-natured alcohol, then light a match, letter-rip, and the dis-mented one-man soapbox derby of news that is my segment. Meanwhile.\",\n+            \" Folks, if you watch the show, you know I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories, developing the central headline pawns, definitely maneuvering an oh-so-topical night to F6, faming of classic Sicilian, named or variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a fisher shows in lip-nitsky attack that culminates in the elegant lethal slow-played all-pass on checkmate that is my nightly monologue, but sometimes sometimes, sometimes folks I sometimes I start a little wake-up side down in the monkey bars of a condemned play ground on a super fun site, get all hept up on goofballs, rummage that would discard a tag bag of defective toys, yank out a fistball of disembodied doll limbs, toss them on a stain kid's place mad from a defunct denies, set up a table inside a rusty cargo container down by the warf and challenge toothless drifters to the godless bughouse blitz of tournament that is my segment. Meanwhile!\",\n+            \" Folks, I spent a lot of time right over there night after night, actually. Carefully selecting for you the day's newsiest, most aerodynamic headlines, stress testing on those topical anti-lock breaks and power steering, painstakingly stitching, leather-seeding, so soft, it would make JD power and her associates blush. To create the luxury sedan that is my nightly monologue, but sometimes I'm just sometimes folks. I lurched to consciousness in the back of an abandoned school bus and slapped myself awake with a crusty floor mat. Before using a mouse-bitten timing belt to strap some old plywood to a couple of discarded oil drums, then by the light of a heathen-moon render a gas tank out of an empty big gulp, filled with white claw and de-natured alcohol, then light a match, and letter-rip, and the dis-mented one-man, soapbox derby of news that is my segment. Meanwhile.\",\n              \" Ladies and gentlemen, you know, I spent a lot of time right over there, raising the finest hosting news cattle firmly, yet tenderly milking the latest headlines from their jokes, swollen teats, churning the daily stories into the decadent Provincil style triple cream-breed. It is my nightly monologue, but sometimes sometimes I stagger home hungry after being released by the police and root around in the neighbor's trash can for an old milk carton scraped out the blooming dairy residue into the remains of a wet cheese rod I won from a rat in a pre-dawn street fight. Put it in a discarded paint can to leave it to ferment next to a trash fire than a hunker down in hallucinate while eating the Listeria latent demon custard of news that is my segment.\",\n-            \" Folks, you watched this show. You know I spend most of my time right over there, carefully sorting through the days, big stories, and selecting only the most subtle and unblemished ostrich and crocodile news leather, which I then entrust to artisan graduates of the Icol Greg Waferandi, who carefully die them in a pallet of bright, zesty shades, and adorn them in the finest, most topical inlay work, using hand tools and double magnifying glasses, then assemble them according to now classic and elegant geometry using our signature saddle stitching, and line it with bees, wax, coated linen, finally attach a mallet hammered strap, pearl hardware, and close-shet to create for you the one-of-a-kind, hout-cout-tour, earned me his burkin bag that is my monologue, but sometimes, sometimes, folks. Sometimes, sometimes, sometimes, sometimes I wake up in the last car of an abandoned roller coaster at Coney Island, where I'm hiding from the triads, I huff some engine lubricants out of a safe way bag, and staggered down the shore to tear the sail off a beach skoener, then I ripped the coaxial cable out of an RV and elderly couple from Utah, Hank, and Mabel, lovely folks. And use it to stitch the sail into a loose pouch-like rock sack, and I stow in the back of a garbage truck to the junkyard, where I pick through to the debris for only the broken toys that make me the saddest, until I have loaded for you. The hobo fugitives bug out bindle of news that is my segment. Meanwhile.\",\n-            \" You know, folks, I spent a lot of time crafting for you a bespoke playlist of the day's big stories right over there. meticulously selecting the most topical chakra affirming scented candles, using Feng Shui, to perfectly align the joke energy in the exclusive boutique yoga retreat that is my monologue, but sometimes just sometimes, I go to the dumpster behind the waffle house at three in the morning, take off my shirt, cover myself and use fry oil, wrap my hands and some old duct tape I stole from a broken car window, pound a six pack of blueberry hardcelser and a sack of pills I stole from a parked ambulance, then arm wrestle a raccoon in the back alley vision quest of news that is my segment.\",\n+            \" Folks, you watched this show, you know I spend most of my time right over there, carefully sorting through the days, big stories, and selecting only the most subtle and unblemished ostrich and crocodile news leather, which I then entrust to artisan graduates of the Ickel Greg Waferandi, who carefully died them in a pallet of bright, zesty shades, and adorn them in the finest most topical inlay work, using hand tools and double magnifying glasses, then assemble them according to now classic and elegant geometry using our signature saddle stitching, and line it with bees, wax, coated linen, and finally attach a mallet hammered strap, purled hardware, and close-shet to create for you the one of a kind hot couture, earn-may's burkin bag that is my monologue, but sometimes, sometimes folks, sometimes. Sometimes I wake up in the last car of an abandoned roller coaster at Coney Island where I'm hiding from the triads, I huff some engine lubricants out of a safe way bag and staggered down the shore to tear the sail off a beach skoener, then I ripped the coaxial cable out of an RV and elderly couple from Utah, Hank, and Mabel, lovely folks, and use it to stitch the sail into a loose pouch-like rock sack, and I stow in the back of a garbage truck to the junkyard, where I pick through to the debris for only the broken toys that make me the saddest, until I have loaded for you the hobo fugitives bug out bindle of news that is my segment.\",\n+            \" You know, folks, I spent a lot of time crafting for you a bespoke playlist of the day's big stories right over there. meticulously selecting the most topical chakra affirming scented candles, using Feng Shui, to perfectly align the joke energy in the exclusive boutique yoga retreat that is my monologue, but sometimes just sometimes, I go to the dumpster behind the waffle house at three in the morning, take off my shirt, cover myself and use fry oil, wrap my hands and some old duct tape I stole from a broken car window, pound a six pack of blueberry hard-seller and a second pill, as I stole from a parked ambulance, then arm wrestle a raccoon in the back alley vision quest of news that is my segment.\",\n              ' You know, folks, I spend most of my time right over there. Mining the days, biggest, most important stories, collecting the finest, most topical iron or hand hammering it into joke panels, then I craft sheets of bronze and blazing with patterns that tell an epic tale of conquest and glory. Then, using the Germanic tradition press, black process, I place thin sheets of foil against the scenes and by hammering or otherwise applying pressure from the back, I project these scenes into a pair of cheat cards and a face plate, and finally using fluted strips of white alloyed molding I divide the designs into framed panels and hold it all together using bronze rivets to create the beautiful and intimidating Anglo-Saxon battle helm that is my nightly monologue. Sometimes, sometimes, folks. Sometimes, just sometimes, I come to my senses fully naked on the deck of a pirate, besieged, melee, container ship that picked me up floating on the detached door of a port of potty in the Indian Ocean. Then, after a sunstroke induced realization of the crew of this ship plans to sell me and exchange for a bag of oranges to fight off scurvy, I lead a mutiny using only a PVC pipe and a pool chain that accepting my new role as captain and declaring myself King of the Windark Seas. I grab a dirty mop bucket covered in barnacles and adorn it with the teeth of the vanquished to create these shopping wet pirate crown of news that is my segment. Meanwhile, young man.',\n-            \" Folks, if you watch this show, you know I spend most of my time right over there carefully blending for you the day's newsiest, most topical flower eggs, milk and butter. And straining into a fine batter to make delicate and informative comedy pancakes, then I glaze them in the juice and zest of the most relevant midnight valencio oranges. And doubts at all, and I find delimane de voyage cognac, before from bang and basting them tables, I deserve you the James Beard Award worthy creeps to ZET. That is my nightly monologue, but sometimes sometimes folks, I wake up in the baggage hole of Greyhound bus. It's being hoisted by the scrapyard claw toward the burn pit. Escape to a nearby abandoned price chopper where I scrounge for old bread scraps, busted open bags of starfruit candies and expired eggs. Chuck it all on a dirty hubcap and slap it over a tire fire before using the legs of a strained pair of sweatpants. As ovenmets to extract and serve the Demented Transience pound cake of news that is my segment.\",\n-            \" Folks, if you watch the show and I hope you do, I spend a lot of time right over there. Tirelessly studying the lineage of the day's most important thoroughbred stories and whole-stiner headlines, working with the best trainers money can buy to rear their comedy offspring with a hand that is stern yet gentle into the triple crown winning equine specimen that is my nightly monologue. But sometimes sometimes folks I break into an unincorporated veterinary genetics lab. And grab whatever test tubes I can find and then under a grow light I got from it a discarded chia pet. I mixed the pill for DNA of a horse and whatever was in a tube labeled Keith Cole and extra. Slering the concoction with caffeine pills and a microwave bread bowl, I screamed sing a prayer to Janice initiator of human life and God of transformation as a half horse, half man freak, seizes to life before me and the hideous collection of loose animal parts and corrupted men tissue that is my segment. Meanwhile.\",\n+            \" Folks, if you watch this show, you know I spend most of my time right over there, carefully blending for you the day's newsiest, most topical flower eggs, milk and butter. And straining into a fine batter to make delicate and informative comedy pancakes, then I glaze them in the juice and zest of the most relevant midnight valencio oranges. And doubts at all, and I find delimane de voyage cognac, before from bang and basting them tables, I deserve you the James Beard Award worthy creeps to ZET. That is my nightly monologue, but sometimes sometimes folks, I wake up in the baggage hole of Greyhound bus. It's being hoisted by the scrapyard claw toward the burn pit. Escape to a nearby abandoned price chopper where I scrounge for old bread scraps, busted open bags of starfruit candies and expired eggs. Chuck it all on a dirty hubcap and slap it over a tire fire before using the legs of a strained pair of sweat pants. As ovenmets to extract and serve the demented transience pound cake of news that is my segment.\",\n+            \" Folks, if you watch the show and I hope you do, I spend a lot of time right over there. Tirelessly studying the lineage of the day's most important thoroughbred stories and whole-stiner headlines, working with the best trainers money can buy to rear their comedy offspring with a hand that is stern yet gentle into the triple crown winning equine specimen that is my nightly monologue. But sometimes sometimes folks I break into an unincorporated veterinary genetics lab. And grab whatever test tubes I can find and then under a grow light I got from a discarded chia pet. I mixed the pill for DNA of a horse and whatever was in a tube labelled Keith Cole and extra. Slurring the concoction with caffeine pills and a microwave bread bowl, I screamed sing a prayer to Janice initiator of human life and God of transformation as a half horse, half man freak, seizes to life before me and the hideous collection of loose animal parts and corrupted men tissue that is my segment. Meanwhile!\",\n          ]\n          # fmt: on",
    "comment": "update expected outputs for whisper after #38778 (#39304)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "2670da66ce08a21654b9700212355d9763d3c503",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py (modified)\n          ])\n          # fmt: on\n-        torch.testing.assert_close(out.cpu(), EXPECTED_TOKENS)\n+        # See https://github.com/huggingface/transformers/pull/39416\n+        EXPECTED_TOKENS_2 = torch.clone(EXPECTED_TOKENS)\n+        EXPECTED_TOKENS_2[2, 159:162] = torch.tensor([3, 0, 269])\n+\n+        try:\n+            torch.testing.assert_close(out.cpu(), EXPECTED_TOKENS)\n+        except AssertionError:\n+            torch.testing.assert_close(out.cpu(), EXPECTED_TOKENS_2)",
    "comment": "fix `kyutai` tests (#39416)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "4b125e29939e3425947f319aa9e4924ad74d619e",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/utils/generic.py (modified)\n          def make_capture_wrapper(module, orig_forward, key, index):\n              @wraps(orig_forward)\n              def wrapped_forward(*args, **kwargs):\n+                if key == \"hidden_states\" and len(collected_outputs[key]) == 0:\n+                    collected_outputs[key] += (args[0],)\n                  output = orig_forward(*args, **kwargs)\n                  if not isinstance(output, tuple):\n                      collected_outputs[key] += (output,)\n                          monkey_patched_layers.append((module, original_forward))\n          outputs = func(self, *args, **kwargs)\n-\n          # Restore original forward methods\n          for module, original_forward in monkey_patched_layers:\n              module.forward = original_forward\n          # Inject collected outputs into model output\n          for key in collected_outputs:\n              if key == \"hidden_states\":\n+                collected_outputs[key] = collected_outputs[key][:-1]\n                  if hasattr(outputs, \"vision_hidden_states\"):\n                      collected_outputs[key] += (outputs.vision_hidden_states,)",
    "comment": "Fixes the BC (#39636)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "4f17bf0572fbeb380db0b4e95c54b28e7b4bc52b",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/auto/image_processing_auto.py (modified)\n  logger = logging.get_logger(__name__)\n+FORCE_FAST_IMAGE_PROCESSOR = [\"Qwen2VLImageProcessor\"]\n+\n+\n  if TYPE_CHECKING:\n      # This significantly improves completion suggestion performance when\n      # the transformers package is used with Microsoft's Pylance language server.\n              # if use_fast is not set and the processor was saved with a fast processor, we use it, otherwise we use the slow processor.\n              if use_fast is None:\n                  use_fast = image_processor_type.endswith(\"Fast\")\n+                if not use_fast and image_processor_type in FORCE_FAST_IMAGE_PROCESSOR and is_torchvision_available():\n+                    use_fast = True\n+                    logger.warning_once(\n+                        f\"The image processor of type `{image_processor_type}` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. \"\n+                        \"This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \"\n+                        \"Note that this behavior will be extended to all models in a future release.\"\n+                    )\n                  if not use_fast:\n                      logger.warning_once(\n                          \"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. \"\n\nFile: src/transformers/models/colqwen2/modular_colqwen2.py (modified)\n          query_prefix (`str`, *optional*): A prefix to be used for the query.\n      \"\"\"\n-    image_processor_class = \"Qwen2VLImageProcessor\"\n+    image_processor_class = \"AutoImageProcessor\"\n      tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n      def __init__(\n\nFile: src/transformers/models/colqwen2/processing_colqwen2.py (modified)\n      attributes = [\"image_processor\", \"tokenizer\"]\n-    image_processor_class = \"Qwen2VLImageProcessor\"\n+    image_processor_class = \"AutoImageProcessor\"\n      tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n      def __init__(\n\nFile: src/transformers/models/glm4v/image_processing_glm4v_fast.py (modified)\n          processed_images_grouped = {}\n          processed_grids = {}\n          for shape, stacked_images in grouped_images.items():\n+            resized_height, resized_width = stacked_images.shape[-2:]\n              # Fused rescale and normalize\n              stacked_images = self.rescale_and_normalize(\n                  stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n          images: ImageInput,\n          **kwargs: Unpack[Glm4vFastImageProcessorKwargs],\n      ) -> BatchFeature:\n-        \"\"\"\n-        Preprocess an image or batch of images.\n-        \"\"\"\n          return super().preprocess(images, **kwargs)",
    "comment": "🚨[Fast Image Processor] Force Fast Image Processor for Qwen2_VL/2_5_VL + Refactor (#39591)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "17f02102c5c9d3dd8f83db92efafc71fad291090",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/voxtral/processing_voxtral.py (modified)\n  # limitations under the License.\n  import io\n+import warnings\n  from typing import Optional, Union\n  from ...utils import is_mistral_common_available, is_soundfile_available, is_torch_available, logging\n          the text. Please refer to the docstring of the above methods for more information.\n          This methods does not support audio. To prepare the audio, please use:\n          1. `apply_chat_template` [`~VoxtralProcessor.apply_chat_template`] method.\n-        2. `apply_transcrition_request` [`~VoxtralProcessor.apply_transcrition_request`] method.\n+        2. `apply_transcription_request` [`~VoxtralProcessor.apply_transcription_request`] method.\n          Args:\n              text (`str`, `list[str]`, `list[list[str]]`):\n          return BatchFeature(data=out, tensor_type=common_kwargs.pop(\"return_tensors\", None))\n      # TODO: @eustlb, this should be moved to mistral_common + testing\n-    def apply_transcrition_request(\n+    def apply_transcription_request(\n          self,\n          language: Union[str, list[str]],\n          audio: Union[str, list[str], AudioInput],\n          language = \"en\"\n\nFile: tests/models/voxtral/test_modeling_voxtral.py (modified)\n          model = VoxtralForConditionalGeneration.from_pretrained(\n              self.checkpoint_name, torch_dtype=self.dtype, device_map=torch_device\n          )\n-        inputs = self.processor.apply_transcrition_request(\n+        inputs = self.processor.apply_transcription_request(\n              language=\"en\",\n              audio=\"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\",\n              model_id=self.checkpoint_name,",
    "comment": "fix(voxtral): correct typo in apply_transcription_request (#39572)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "3b3f9c0c46ea2dd1de519cc428e6f27dd2ef4b97",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/bark/modeling_bark.py (modified)\n      def set_input_embeddings(self, new_embeddings):\n          self.input_embeds_layer = new_embeddings\n-    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, cache_position=None, **kwargs):\n-        # Overwritten -- bark has a model-specific hack\n-        input_embeds = kwargs.get(\"input_embeds\", None)\n-\n-        attention_mask = kwargs.get(\"attention_mask\", None)\n-        position_ids = kwargs.get(\"position_ids\", None)\n-\n-        if cache_position[0] != 0:\n-            # Omit tokens covered by past_key_values\n-            seq_len = input_ids.shape[1]\n-            past_length = past_key_values.get_seq_length()\n-\n-            # Some generation methods already pass only the last input ID\n-            if input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = input_ids.shape[1] - 1\n\nFile: tests/models/chameleon/test_modeling_chameleon.py (modified)\n          input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n          input_ids[input_ids == self.image_token_id] = self.pad_token_id\n          input_ids[:, : self.image_seq_length] = self.image_token_id\n-        attention_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n+        attention_mask = input_ids.ne(self.pad_token_id).to(torch_device)\n          pixel_values = floats_tensor([self.batch_size, 3, self.image_size, self.image_size])\n          config = self.get_config()\n      def test_model_is_small(self):\n          pass\n+    @unittest.skip(\"Chameleon applies key/query norm which doesn't work with packing\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Chameleon applies key/query norm which doesn't work with packing\")\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n      def test_mismatching_num_image_tokens(self):\n          \"\"\"\n          Tests that VLMs through an error with explicit message saying what is wrong\n\nFile: tests/models/emu3/test_modeling_emu3.py (modified)\n      def prepare_config_and_inputs(self):\n          input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-        attention_mask = input_ids.ne(1).to(torch_device)\n+        attention_mask = input_ids.ne(self.pad_token_id).to(torch_device)\n          config = self.get_config()\n          config = self.get_config()\n          input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size)\n-        attention_mask = input_ids.ne(1).to(torch_device)\n          input_ids[input_ids == self.image_token_id] = self.pad_token_id\n          input_ids[:, : self.image_seq_length] = self.image_token_id\n+        attention_mask = input_ids.ne(self.pad_token_id).to(torch_device)\n          pixel_values = floats_tensor(\n              [\n\nFile: tests/models/fuyu/test_modeling_fuyu.py (modified)\n      def test_generate_continue_from_inputs_embeds():\n          pass\n+    @unittest.skip(\"Persimmon backbone applies key/query norm which doesn't work with packing\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Persimmon backbone applies key/query norm which doesn't work with packing\")\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n  @slow\n  @require_torch_accelerator",
    "comment": "[attention] fix test for packed padfree masking (#39582)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "c392d47c9b40a9866663ea3bed97904cec27658b",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/auto/image_processing_auto.py (modified)\n              (\"nat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n              (\"nougat\", (\"NougatImageProcessor\", \"NougatImageProcessorFast\")),\n              (\"oneformer\", (\"OneFormerImageProcessor\", \"OneFormerImageProcessorFast\")),\n-            (\"owlv2\", (\"Owlv2ImageProcessor\",)),\n+            (\"owlv2\", (\"Owlv2ImageProcessor\", \"Owlv2ImageProcessorFast\")),\n              (\"owlvit\", (\"OwlViTImageProcessor\", \"OwlViTImageProcessorFast\")),\n              (\"paligemma\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n              (\"perceiver\", (\"PerceiverImageProcessor\", \"PerceiverImageProcessorFast\")),\n\nFile: src/transformers/models/owlv2/__init__.py (modified)\n  if TYPE_CHECKING:\n      from .configuration_owlv2 import *\n      from .image_processing_owlv2 import *\n+    from .image_processing_owlv2_fast import *\n      from .modeling_owlv2 import *\n      from .processing_owlv2 import *\n  else:",
    "comment": "Add owlv2 fast processor (#39041)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "565c035a2eb610de9a208ee598c47c75cd21fc70",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/encodec/configuration_encodec.py (modified)\n          else:\n              return max(1, int((1.0 - self.overlap) * self.chunk_length))\n+    @property\n+    def hop_length(self) -> int:\n+        return int(np.prod(self.upsampling_ratios))\n+\n+    @property\n+    def codebook_nbits(self) -> int:\n+        return math.ceil(math.log2(self.codebook_size))\n+\n      @property\n      def frame_rate(self) -> int:\n-        hop_length = np.prod(self.upsampling_ratios)\n-        return math.ceil(self.sampling_rate / hop_length)\n+        return math.ceil(self.sampling_rate / self.hop_length)\n      @property\n      def num_quantizers(self) -> int:\n-        return int(1000 * self.target_bandwidths[-1] // (self.frame_rate * 10))\n+        return int(1000 * self.target_bandwidths[-1] // (self.frame_rate * self.codebook_nbits))\n  __all__ = [\"EncodecConfig\"]",
    "comment": "🔴 Fix EnCodec internals and integration tests (#39431)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "c5a80dd6c459d7e6b0a8c31182a248de9a6c2d57",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/dac/convert_dac_checkpoint.py (modified)\n  import fnmatch\n  import re\n+import numpy as np\n  import torch\n  import torch.nn as nn\n      config.upsampling_ratios = metadata[\"decoder_rates\"]\n      config.quantizer_dropout = float(metadata[\"quantizer_dropout\"])\n      config.sampling_rate = sample_rate\n+    config.hop_length = int(np.prod(config.downsampling_ratios))\n      model = DacModel(config)\n      feature_extractor = DacFeatureExtractor()",
    "comment": "Fix DAC integration tests and checkpoint conversion. (#39313)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "7a4e2e7868272af012a5418660ae97257505264f",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/commands/chat.py (modified)\n          # 2. c. [no processing needed] lists are lists of ints because `generate` doesn't take lists of strings :)\n          # We also mention in the help message that we only accept lists of ints for now.\n-        # 3. Join the the result into a comma separated string\n+        # 3. Join the result into a comma separated string\n          generate_flags_string = \", \".join([f\"{k}: {v}\" for k, v in generate_flags_as_dict.items()])\n          # 4. Add the opening/closing brackets\n\nFile: src/transformers/trainer.py (modified)\n                  kernel_config = self.args.liger_kernel_config if self.args.liger_kernel_config is not None else {}\n                  if isinstance(model, PreTrainedModel):\n-                    # Patch the model with liger kernels. Use the the specified or default kernel configurations.\n+                    # Patch the model with liger kernels. Use the specified or default kernel configurations.\n                      _apply_liger_kernel_to_instance(model=model, **kernel_config)\n                  elif hasattr(model, \"get_base_model\") and isinstance(model.get_base_model(), PreTrainedModel):\n                      # Patch the base model with liger kernels where model is a PeftModel. Use the specified or default kernel configurations.",
    "comment": "Fix typos and grammar issues in documentation and code (#39598)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "0fe03afeb82e1a435a75704d1f434c47e49a0bbb",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/modeling_utils.py (modified)\n                  A torch tensor parallel degree. If not provided would default to world size.\n              device_mesh (`torch.distributed.DeviceMesh`, *optional*):\n                  A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.\n+                If provided, it has to contain dimension named `\"tp\"` which will be used for tensor parallelism\n              offload_folder (`str` or `os.PathLike`, *optional*):\n                  If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n              offload_state_dict (`bool`, *optional*):\n          # We need to correctly dispatch the model on the current process device. The easiest way for this is to use a simple\n          # `device_map` pointing to the correct device\n          if tp_plan is not None:\n-            if device_mesh is None and tp_plan is not None:\n+            if device_mesh is None:\n                  tp_plan, device_map, device_mesh = initialize_tensor_parallelism(tp_plan, tp_size=None)\n              else:\n-                # TODO: make device_mesh support multiple dimensions\n-                if device_mesh.ndim != 1:\n-                    raise ValueError(\"device_mesh must be 1 dimensional and will be used for TP\")\n-                device_map = torch.device(device_mesh.device_type, int(os.environ[\"LOCAL_RANK\"]))\n+                if \"tp\" not in device_mesh.mesh_dim_names:\n+                    raise ValueError(",
    "comment": "Allow `device_mesh` have multiple dim  (#38949)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "82603b6cc284dbdf2b7a7cf070feb6a2c3bb53cf",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/quantizers/quantizer_awq.py (modified)\n          if not is_accelerate_available():\n              raise ImportError(\"Loading an AWQ quantized model requires accelerate (`pip install accelerate`)\")\n-        if self.quantization_config.version == AWQLinearVersion.GEMM and not torch.cuda.is_available():\n-            logger.warning_once(\"No CUDA found, replace GEMM with IPEX version to support non-cuda AWQ model.\")\n+        if (\n+            self.quantization_config.version == AWQLinearVersion.GEMM\n+            and not torch.cuda.is_available()\n+            and not torch.xpu.is_available()\n+        ):\n+            logger.warning_once(\"No CUDA or XPU found, consider switching to the IPEX version for CPU-only execution.\")\n              self.quantization_config.version = AWQLinearVersion.IPEX\n          if self.quantization_config.version == AWQLinearVersion.IPEX:\n                      \" This is not supported. Please make sure only cpu and xpu in the device_map.\"\n                  )\n          else:\n-            if not torch.cuda.is_available():\n+            if not torch.cuda.is_available() and not torch.xpu.is_available():\n                  raise RuntimeError(\n                      \"GPU is required to run AWQ quantized model. You can use IPEX version AWQ if you have an Intel CPU\"\n                  )",
    "comment": "enable triton backend on awq xpu (#39443)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "10c990f7e290816221fbcc05ed2e01a66587bdbe",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py (modified)\n              # router_logits: (batch * sequence_length, n_experts)\n              router_logits = self.gate(hidden_states.float())\n-            # NOTE: we are using the original code base at\n-            # https://github.com/PaddlePaddle/Paddle/blob/9b40438ce0f6d76b4f08a7837dd1e28b26cf8ee6/python/paddle/incubate/nn/functional/moe_gate_dispatch.py#L109-L116\n-            # this might differ from the remote version regarding the bias (see `Ernie4_5_MoEStatics`)\n              routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-            routing_weights = self.moe_statics(routing_weights)\n-            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n+            _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n+            routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n              routing_weights = routing_weights / torch.clamp(\n                  routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n              )\n\nFile: src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py (modified)\n              # router_logits: (batch * sequence_length, n_experts)\n              router_logits = self.gate(hidden_states.float())\n-            # NOTE: we are using the original code base at\n-            # https://github.com/PaddlePaddle/Paddle/blob/9b40438ce0f6d76b4f08a7837dd1e28b26cf8ee6/python/paddle/incubate/nn/functional/moe_gate_dispatch.py#L109-L116\n-            # this might differ from the remote version regarding the bias (see `Ernie4_5_MoEStatics`)\n              routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-            routing_weights = self.moe_statics(routing_weights)\n-            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n+            _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n+            routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n              routing_weights = routing_weights / torch.clamp(\n                  routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n              )\n\nFile: tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py (modified)\n      @require_bitsandbytes\n      @slow\n      def test_model_21b_a3b_generation(self):\n-        EXPECTED_TEXT_COMPLETION = \"User: Hey, are you conscious? Can you talk to me?\\nAssistant: Yes, I am conscious and I can communicate with you. How can I assist you with any questions or information you need?\"  # fmt: skip\n+        EXPECTED_TEXT_COMPLETION = \"User: Hey, are you conscious? Can you talk to me?\\nAssistant:  I don't have consciousness in the way humans do. I'm a text-based AI created to process and generate responses based on patterns in data.\"  # fmt: skip\n          model = self.get_model()\n          tokenizer = AutoTokenizer.from_pretrained(\"baidu/ERNIE-4.5-21B-A3B-PT\", revision=\"refs/pr/11\")",
    "comment": "fix moe routing_weights (#39581)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "a62f65a989b10bf1130e098bb62f16a5b3994ee8",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: examples/modular-transformers/modeling_my_new_model2.py (modified)\n      _supports_flex_attn = True\n      _supports_cache_class = True\n      _supports_quantized_cache = True\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n      _supports_attention_backend = True\n      _can_record_outputs = {\n          \"hidden_states\": MyNewModel2DecoderLayer,\n\nFile: examples/modular-transformers/modeling_new_task_model.py (modified)\n      _skip_keys_device_placement = \"past_key_values\"\n      _supports_cache_class = True\n      _supports_quantized_cache = True\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n      _supports_flash_attn = True\n      _supports_sdpa = True\n      _supports_flex_attn = True\n\nFile: examples/modular-transformers/modeling_super.py (modified)\n      _supports_flex_attn = True\n      _supports_cache_class = True\n      _supports_quantized_cache = True\n-    _supports_static_cache = True\n+    _can_compile_fullgraph = True\n      _supports_attention_backend = True\n      _can_record_outputs = {\n          \"hidden_states\": SuperDecoderLayer,\n\nFile: src/transformers/generation/utils.py (modified)\n          )\n          if generation_config.cache_implementation is not None:\n              if generation_config.cache_implementation in NEED_SETUP_CACHE_CLASSES_MAPPING:\n-                if generation_config.cache_implementation == \"static\" and not self._supports_static_cache:\n+                if generation_config.cache_implementation == \"static\" and not self._can_compile_fullgraph:\n                      raise ValueError(\n                          \"This model does not support `cache_implementation='static'`. Please check the following \"\n                          \"issue: https://github.com/huggingface/transformers/issues/28981\"\n          using_compilable_cache = (\n              isinstance(model_kwargs.get(\"past_key_values\"), Cache) and model_kwargs[\"past_key_values\"].is_compileable\n          )\n-        can_compile = valid_hardware and using_compilable_cache and self._supports_static_cache\n+        # TODO @raushan `self._can_compile_fullgraph` can be removed and inferred from model arch (e.g. MoE doesn't support compile)\n+        can_compile = valid_hardware and using_compilable_cache and self._can_compile_fullgraph\n          # Exception 1: Some quantization methods do not support compilation\n          if getattr(self, \"hf_quantizer\", None) is not None:\n\nFile: src/transformers/modeling_utils.py (modified)\n      # Flex Attention support\n      _supports_flex_attn = False\n-    # Has support `torch.compile(fullgraph=True)`\n-    _supports_static_cache = False\n+    _can_compile_fullgraph = False\n      # A tensor parallel plan to be applied to the model when TP is enabled. For\n      # top-level models, this attribute is currently defined in respective model",
    "comment": "Rename `supports_static_cache` to `can_compile_fullgraph` (#39505)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "eb1a007f7f0bcff45b0b6d43759c583246946f91",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/aria/modeling_aria.py (modified)\n          **kwargs: Unpack[TransformersKwargs],\n      ) -> CausalLMOutputWithPast:\n          r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n          Example:\n          ```python\n\nFile: src/transformers/models/deepseek_v3/modeling_deepseek_v3.py (modified)\n          **kwargs: Unpack[TransformersKwargs],\n      ) -> CausalLMOutputWithPast:\n          r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n          Example:\n          ```python",
    "comment": "Generic task-specific base classes (#39584)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "019b74977d7469af3a86d3f5bbbb02ffe62af94d",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/bamba/modeling_bamba.py (modified)\n  from transformers.activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import Cache, DynamicCache, DynamicLayer\n  from ...generation import GenerationMixin\n  from ...integrations import use_kernel_forward_from_hub\n  from ...modeling_attn_mask_utils import AttentionMaskConverter\n      is_compileable = False\n      def __init__(self, config: BambaConfig, batch_size, dtype=torch.float16, device=None):\n-        super().__init__()\n+        super().__init__(layer_classes=DynamicLayer)\n          self.layers_block_type = config.layers_block_type\n          self.has_previous_state = False  # only used by mamba\n          conv_kernel_size = config.mamba_d_conv\n\nFile: src/transformers/models/bamba/modular_bamba.py (modified)\n      segment_sum,\n  )\n-from ...cache_utils import Cache\n+from ...cache_utils import DynamicLayer\n  from ...modeling_attn_mask_utils import AttentionMaskConverter\n  from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n  from ...modeling_utils import PreTrainedModel\n  # Adapted from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache for the v2 mixer\n-class HybridMambaAttentionDynamicCache(HybridMambaAttentionDynamicCache, Cache):\n+class HybridMambaAttentionDynamicCache(HybridMambaAttentionDynamicCache):\n      \"\"\"\n      A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n      (which has a constant shape regardless of seq_len).\n      \"\"\"\n      def __init__(self, config: BambaConfig, batch_size, dtype=torch.float16, device=None):\n-        Cache.__init__()\n+        HybridMambaAttentionDynamicCache.__init__(layer_classes=DynamicLayer)\n          self.layers_block_type = config.layers_block_type\n          self.has_previous_state = False  # only used by mamba\n          conv_kernel_size = config.mamba_d_conv\n\nFile: src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py (modified)\n  from transformers.activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import Cache, DynamicCache, DynamicLayer\n  from ...generation import GenerationMixin\n  from ...modeling_attn_mask_utils import AttentionMaskConverter\n  from ...modeling_layers import GradientCheckpointingLayer\n      is_compileable = False\n      def __init__(self, config: GraniteMoeHybridConfig, batch_size, dtype=torch.float16, device=None):\n-        super().__init__()\n+        super().__init__(layer_classes=DynamicLayer)\n          self.layers_block_type = config.layers_block_type\n          self.has_previous_state = False  # only used by mamba\n          conv_kernel_size = config.mamba_d_conv\n\nFile: src/transformers/models/jamba/modeling_jamba.py (modified)\n  from torch import nn\n  from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import Cache, DynamicCache, DynamicLayer\n  from ...generation import GenerationMixin\n  from ...modeling_attn_mask_utils import AttentionMaskConverter\n  from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n      is_compileable = False\n      def __init__(self, config, batch_size, dtype=torch.float16, device=None):\n-        super().__init__()\n+        super().__init__(layer_classes=DynamicLayer)\n          self.dtype = dtype\n          self.layers_block_type = config.layers_block_type\n          self.has_previous_state = False  # only used by mamba",
    "comment": "Fix DynamicCache and simplify Cache classes a bit (#39590)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "5dba4bc7b2c1ef517ed44bba76bb70b59001c737",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/auto/image_processing_auto.py (modified)\n              (\"llava_next\", (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")),\n              (\"llava_next_video\", (\"LlavaNextVideoImageProcessor\",)),\n              (\"llava_onevision\", (\"LlavaOnevisionImageProcessor\", \"LlavaOnevisionImageProcessorFast\")),\n-            (\"mask2former\", (\"Mask2FormerImageProcessor\",)),\n-            (\"maskformer\", (\"MaskFormerImageProcessor\",)),\n+            (\"mask2former\", (\"Mask2FormerImageProcessor\", \"Mask2FormerImageProcessorFast\")),\n+            (\"maskformer\", (\"MaskFormerImageProcessor\", \"MaskFormerImageProcessorFast\")),\n              (\"mgp-str\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n              (\"mistral3\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n              (\"mlcd\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n\nFile: src/transformers/models/mask2former/__init__.py (modified)\n  if TYPE_CHECKING:\n      from .configuration_mask2former import *\n      from .image_processing_mask2former import *\n+    from .image_processing_mask2former_fast import *\n      from .modeling_mask2former import *\n  else:\n      import sys\n\nFile: src/transformers/models/mask2former/image_processing_mask2former.py (modified)\n      PILImageResampling,\n      get_image_size,\n      infer_channel_dimension_format,\n-    is_batched,\n      is_scaled_image,\n+    make_list_of_images,\n      to_numpy_array,\n      valid_images,\n      validate_preprocess_arguments,\n      from torch import nn\n+# Copied from transformers.models.detr.image_processing_detr.get_size_with_aspect_ratio\n+def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n+    \"\"\"\n+    Computes the output image size given the input image size and the desired output size.\n+\n+    Args:\n+        image_size (`tuple[int, int]`):\n+            The input image size.\n+        size (`int`):\n+            The desired output size.",
    "comment": "Mask2former & Maskformer Fast Image Processor (#35685)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "d9b35c635eb16e837a7b563c519c5231d4e09265",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/__init__.py (modified)\n          \"is_sigopt_available\",\n          \"is_swanlab_available\",\n          \"is_tensorboard_available\",\n+        \"is_trackio_available\",\n          \"is_wandb_available\",\n      ],\n      \"loss\": [],\n          is_sigopt_available,\n          is_swanlab_available,\n          is_tensorboard_available,\n+        is_trackio_available,\n          is_wandb_available,\n      )\n      from .integrations.executorch import (\n\nFile: src/transformers/integrations/__init__.py (modified)\n          \"NeptuneMissingConfiguration\",\n          \"SwanLabCallback\",\n          \"TensorBoardCallback\",\n+        \"TrackioCallback\",\n          \"WandbCallback\",\n          \"get_available_reporting_integrations\",\n          \"get_reporting_integration_callbacks\",\n          \"is_sigopt_available\",\n          \"is_swanlab_available\",\n          \"is_tensorboard_available\",\n+        \"is_trackio_available\",\n          \"is_wandb_available\",\n          \"rewrite_logs\",\n          \"run_hp_search_optuna\",\n          NeptuneMissingConfiguration,\n          SwanLabCallback,\n          TensorBoardCallback,\n+        TrackioCallback,\n          WandbCallback,\n          get_available_reporting_integrations,\n\nFile: src/transformers/testing_utils.py (modified)\n      is_sigopt_available,\n      is_swanlab_available,\n      is_tensorboard_available,\n+    is_trackio_available,\n      is_wandb_available,\n  )\n  from .integrations.deepspeed import is_deepspeed_available\n      return unittest.skipUnless(is_swanlab_available(), \"test requires swanlab\")(test_case)\n+def require_trackio(test_case):\n+    \"\"\"\n+    Decorator marking a test that requires trackio.\n+\n+    These tests are skipped when trackio isn't installed.\n+\n+    \"\"\"\n+    return unittest.skipUnless(is_trackio_available(), \"test requires trackio\")(test_case)\n+\n+\n  def require_wandb(test_case):\n      \"\"\"",
    "comment": "🎯 Trackio integration (#38814)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "6e9972962fbc80d218234bfbd8c9b2843ef02b2b",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/transformers/models/auto/image_processing_auto.py (modified)\n              (\"mobilevitv2\", (\"MobileViTImageProcessor\", \"MobileViTImageProcessorFast\")),\n              (\"nat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n              (\"nougat\", (\"NougatImageProcessor\", \"NougatImageProcessorFast\")),\n-            (\"oneformer\", (\"OneFormerImageProcessor\",)),\n+            (\"oneformer\", (\"OneFormerImageProcessor\", \"OneFormerImageProcessorFast\")),\n              (\"owlv2\", (\"Owlv2ImageProcessor\",)),\n              (\"owlvit\", (\"OwlViTImageProcessor\", \"OwlViTImageProcessorFast\")),\n              (\"paligemma\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n\nFile: src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py (modified)\n                  The target size of the image, as returned by the preprocessing `resize` step.\n              threshold (`float`, *optional*, defaults to 0.5):\n                  The threshold used to binarize the segmentation masks.\n-            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST`):\n+            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST_EXACT`):\n                  The resampling filter to use when resizing the masks.\n          \"\"\"\n-        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n          ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n          new_annotation = {}\n\nFile: src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py (modified)\n                  The target size of the image, as returned by the preprocessing `resize` step.\n              threshold (`float`, *optional*, defaults to 0.5):\n                  The threshold used to binarize the segmentation masks.\n-            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST`):\n+            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST_EXACT`):\n                  The resampling filter to use when resizing the masks.\n          \"\"\"\n-        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n          ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n          new_annotation = {}\n\nFile: src/transformers/models/detr/image_processing_detr_fast.py (modified)\n                  The target size of the image, as returned by the preprocessing `resize` step.\n              threshold (`float`, *optional*, defaults to 0.5):\n                  The threshold used to binarize the segmentation masks.\n-            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST`):\n+            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST_EXACT`):\n                  The resampling filter to use when resizing the masks.\n          \"\"\"\n-        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n          ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n          new_annotation = {}",
    "comment": "[WIP] Add OneformerFastImageProcessor (#38343)",
    "language": "diff",
    "repo": "huggingface/transformers",
    "sha": "c6d0500d15b9eedc33e9131a6bec6db56282b875",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: index.d.ts (modified)\n    <T = any, R = AxiosResponse<T>, D = any>(config: AxiosRequestConfig<D>): Promise<R>;\n    <T = any, R = AxiosResponse<T>, D = any>(url: string, config?: AxiosRequestConfig<D>): Promise<R>;\n+  create(config?: CreateAxiosDefaults): AxiosInstance;\n    defaults: Omit<AxiosDefaults, 'headers'> & {\n      headers: HeadersDefaults & {\n        [key: string]: AxiosHeaderValue\n  export function mergeConfig<D = any>(config1: AxiosRequestConfig<D>, config2: AxiosRequestConfig<D>): AxiosRequestConfig<D>;\n  export interface AxiosStatic extends AxiosInstance {\n-  create(config?: CreateAxiosDefaults): AxiosInstance;\n    Cancel: CancelStatic;\n    CancelToken: CancelTokenStatic;\n    Axios: typeof Axios;\n\nFile: test/module/typings/esm/index.ts (modified)\n  // Instances\n  const instance1: AxiosInstance = axios.create();\n-const instance2: AxiosInstance = axios.create(config);\n+const instance2: AxiosInstance = instance1.create(config);\n  instance1(config)\n      .then(handleResponse)",
    "comment": "chore(types): move AxiosStatic#create to AxiosInstance#create (#5096)",
    "language": "diff",
    "repo": "axios/axios",
    "sha": "d1178cad4c4fe21be3d36a5a89c1590f474b4d89",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: index.d.ts (modified)\n    getAuthorization(matcher?: AxiosHeaderMatcher): AxiosHeaderValue;\n    hasAuthorization(matcher?: AxiosHeaderMatcher): boolean;\n+  getSetCookie(): string[];\n+\n    [Symbol.iterator](): IterableIterator<[string, AxiosHeaderValue]>;\n  }\n\nFile: lib/core/AxiosHeaders.js (modified)\n      return Object.entries(this.toJSON()).map(([header, value]) => header + ': ' + value).join('\\n');\n    }\n+  getSetCookie() {\n+    return this[\"set-cookie\"] || [];\n+  }\n+\n    get [Symbol.toStringTag]() {\n      return 'AxiosHeaders';\n    }\n\nFile: test/unit/core/AxiosHeaders.js (modified)\n        assert.deepStrictEqual(new AxiosHeaders({x:1, y:2}).toString(), 'x: 1\\ny: 2');\n      });\n    });\n+\n+  describe('getSetCookie', function () {\n+    it('should return set-cookie', function () {\n+      const headers = new AxiosHeaders(\n+        'Set-Cookie: key=val;\\n' +\n+        'Set-Cookie: key2=val2;\\n'\n+      );\n+\n+      assert.deepStrictEqual(headers.getSetCookie(), ['key=val;', 'key2=val2;']);\n+    });\n+\n+    it('should return empty set-cookie', function () {\n+      assert.deepStrictEqual(new AxiosHeaders().getSetCookie(), []);\n+    });\n+  });\n  });",
    "comment": "feat(AxiosHeaders): add getSetCookie method to retrieve set-cookie headers values (#5707)",
    "language": "diff",
    "repo": "axios/axios",
    "sha": "80ea756e72bcf53110fa792f5d7ab76e8b11c996",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: docusaurus/website/docusaurus.config.js (modified)\n  const siteConfig = {\n    title: 'Create React App',\n-  tagline: 'Set up a modern web app by running one command.',\n+  tagline:\n+    'Create React App has been deprecated. Please visit react.dev for modern options.',\n    url: 'https://create-react-app.dev',\n    baseUrl: '/',\n    projectName: 'create-react-app',\n    themeConfig: {\n      image: 'img/logo-og.png',\n      announcementBar: {\n-      id: 'support_ukraine',\n+      id: 'deprecated',\n        content:\n-        'Support Ukraine 🇺🇦 <a target=\"_blank\" rel=\"noopener noreferrer\" \\\n-        href=\"https://opensource.facebook.com/support-ukraine\"> Help Provide Humanitarian Aid to Ukraine</a>.',\n+        'Create React App is deprecated. <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://react.dev/link/cra\">Read more here</a>.',\n        backgroundColor: '#20232a',\n        textColor: '#fff',\n        isCloseable: false,\n\nFile: docusaurus/website/src/pages/index.js (modified)\n  import React from 'react';\n  import Link from '@docusaurus/Link';\n+import Head from '@docusaurus/Head';\n  import useDocusaurusContext from '@docusaurus/useDocusaurusContext';\n  import useBaseUrl from '@docusaurus/useBaseUrl';\n        permalink={'/'}\n        description={'Set up a modern web app by running one command.'}\n      >\n+      <Head>\n+        <meta name=\"robots\" content=\"noindex\" />\n+        <title>Create React App is deprecated.</title>\n+        <meta\n+          name=\"description\"\n+          content=\"Create React App is deprecated. Please see react.dev for modern options.\"\n+        />\n+        <meta property=\"og:title\" content=\"Create React App is deprecated.\" />\n+        <meta\n+          property=\"og:description\"\n+          content=\"Create React App is deprecated. Please see react.dev for modern options.\"\n+        />",
    "comment": "Add deprecation to website (#17008)",
    "language": "diff",
    "repo": "facebook/create-react-app",
    "sha": "b961be37929c9dd340aff94cb92debf700a9aeba",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/create-react-app/createReactApp.js (modified)\n    return (process.env.npm_config_user_agent || '').indexOf('yarn') === 0;\n  }\n+function hasGivenWarning() {\n+  const localWarningFilePath = path.join(\n+    __dirname,\n+    'given-deprecation-warning'\n+  );\n+  return fs.existsSync(localWarningFilePath);\n+}\n+\n+function writeWarningFile() {\n+  const localWarningFilePath = path.join(\n+    __dirname,\n+    'given-deprecation-warning'\n+  );\n+  fs.writeFileSync(localWarningFilePath, 'true');\n+}\n+\n  let projectName;\n  function init() {",
    "comment": "Deprecate Create React App officially by changing the README, and adding a message on init (#17003)",
    "language": "diff",
    "repo": "facebook/create-react-app",
    "sha": "b532a58792ea79ba7b0e85c1cb42b2a36dafe276",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: setup.py (modified)\n                    ':python_version>\"2.7\"': ['decorator', 'pyte'],\n                    \":sys_platform=='win32'\": ['win_unicode_console']}\n+if sys.platform == \"win32\":\n+    scripts = ['scripts\\\\fuck.bat', 'scripts\\\\fuck.ps1']\n+    entry_points = {'console_scripts': [\n+                  'thefuck = thefuck.entrypoints.main:main',\n+                  'thefuck_firstuse = thefuck.entrypoints.not_configured:main']}\n+else:\n+    scripts = []\n+    entry_points = {'console_scripts': [\n+                  'thefuck = thefuck.entrypoints.main:main',\n+                  'fuck = thefuck.entrypoints.not_configured:main']}\n+\n  setup(name='thefuck',\n        version=VERSION,\n        description=\"Magnificent app which corrects your previous console command\",\n        python_requires='>=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*',\n        install_requires=install_requires,\n        extras_require=extras_require,\n-      entry_points={'console_scripts': [",
    "comment": "#1329: Add support for Windows `CMD` and easier setup for Powershell",
    "language": "diff",
    "repo": "nvbn/thefuck",
    "sha": "3cd187a3bb47351890ac7308464e1a2780507220",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: docs/source/conf.py (modified)\n      \"probs_to_logits\",\n      \"tril_matrix_to_vec\",\n      \"vec_to_tril_matrix\",\n-    # torch.functional\n-    \"align_tensors\",\n-    \"atleast_1d\",\n-    \"atleast_2d\",\n-    \"atleast_3d\",\n-    \"block_diag\",\n-    \"broadcast_shapes\",\n-    \"broadcast_tensors\",\n-    \"cartesian_prod\",\n-    \"cdist\",\n-    \"chain_matmul\",\n-    \"einsum\",\n-    \"lu\",\n-    \"meshgrid\",\n-    \"norm\",\n-    \"split\",\n-    \"stft\",",
    "comment": "Remove torch.functional entries from the doc ignore list (#158581)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "316c188a5e700c276ed2c4ec1e15e5b4eee6d1a8",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: test/profiler/test_execution_trace.py (modified)\n                          assert len(n[\"outputs\"][\"values\"]) == 0\n          assert found_captured_triton_kernel_node\n+    @unittest.skipIf(IS_WINDOWS, \"torch.compile does not support WINDOWS\")\n+    @unittest.skipIf(\n+        (not has_triton()) or (not TEST_CUDA and not TEST_XPU),\n+        \"need triton and device(CUDA or XPU) availability to run\",\n+    )\n+    @skipCPUIf(True, \"skip CPU device for testing profiling triton\")\n+    def test_triton_fx_graph_with_et(self, device):\n+        import os\n+\n+        os.environ[\"ENABLE_PYTORCH_EXECUTION_TRACE\"] = \"1\"\n+        os.environ[\"ENABLE_PYTORCH_EXECUTION_TRACE_EXTRAS\"] = \"1\"\n+\n+        @torchdynamo.optimize(\"inductor\")\n+        def fn(a, b, c):\n+            x = torch.nn.functional.linear(a, b)\n+            x = x.sin()\n+            x = x.t() + c * 1111\n+            return x.cos()\n\nFile: torch/_inductor/utils.py (modified)\n  import torch\n  from torch._inductor.analysis.device_info import datasheet_tops\n  from torch._inductor.runtime.hints import DeviceProperties\n+from torch.utils._dtype_abbrs import dtype_abbrs\n  from torch.utils._ordered_set import OrderedSet\n  from torch.utils._pytree import tree_flatten, tree_map_only\n      node_schedule: Union[Sequence[BaseSchedulerNode], ExternKernel],\n      wrapper: PythonWrapperCodegen,\n  ) -> tuple[str, str]:\n+    \"\"\"\n+    Retrieves metadata information for a kernel.\n+    Args:\n+        node_schedule (Union[Sequence[BaseSchedulerNode], ExternKernel]):\n+            Either a sequence of BaseSchedulerNode objects or an ExternKernel instance.\n+        wrapper (PythonWrapperCodegen):\n+            An instance of PythonWrapperCodegen, used to define the code comment format.\n+    Returns:\n+        tuple[str, str]:\n+            A tuple containing two strings:\n+                - The first string represents the kernel's metadata.",
    "comment": "Add inputs and outputs in Triton Kernel FX Graph segment (#158174)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "74f64d3c84df9abeaa02c3624bec40cf1d9174e2",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: test/test_custom_ops.py (modified)\n  class TestCustomOp(CustomOpTestCaseBase):\n      test_ns = \"_test_custom_op\"\n+    def test_deploy_interaction(self):\n+        # run in a different process to avoid parallel issues when we monkeypatch torch._running_with_deploy\n+        script = \"\"\"\n+import torch\n+torch._running_with_deploy = lambda: True\n+\n+# creating the library is a no-op, so you can DEF multiple times\n+m1 = torch.library.Library(\"mylib4392\", \"DEF\")  # noqa: TOR901\n+m2 = torch.library.Library(\"mylib4392\", \"DEF\")  # noqa: TOR901\n+\n+m = torch.library.Library(\"aten\", \"FRAGMENT\")  # noqa: TOR901\n+\n+# define is a no-op\n+m.define(\"foobarbaz9996(Tensor x) -> Tensor\")\n+assert not hasattr(torch.ops.aten, \"foobarbaz9996\"), \"m.define should have been a noop\"\n+\n+def sin_override(x):\n+    raise AssertionError(\"m.impl should have been a noop\")\n\nFile: test/test_sparse_csr.py (modified)\n      @onlyCUDA\n      @dtypes(torch.half, torch.bfloat16, torch.float)\n      @dtypesIfCUDA(torch.half, *[torch.bfloat16] if SM80OrLater else [], torch.float)\n-    @unittest.skipIf((not TEST_WITH_TORCHINDUCTOR) or (IS_FBCODE and IS_REMOTE_GPU),\n-                     \"Skipped for internal with remote GPUs\")\n+    @unittest.skipIf((not TEST_WITH_TORCHINDUCTOR) or (IS_FBCODE and IS_REMOTE_GPU) or torch._running_with_deploy(),\n+                     \"Skipped for deploy and internal with remote GPUs\")\n      def test_triton_bsr_dense_bmm(self, device, dtype, index_dtype, block_size):\n          from functools import partial\n          from torch.sparse._triton_ops import bsr_dense_mm\n      @onlyCUDA\n      @dtypes(torch.half)\n-    @unittest.skipIf(IS_FBCODE and IS_REMOTE_GPU,\n-                     \"Skipped for internal with remote GPUs\")\n+    @unittest.skipIf(IS_FBCODE and IS_REMOTE_GPU or torch._running_with_deploy(),\n+                     \"Skipped for deploy and internal with remote GPUs\")\n      def test_triton_bsr_dense_bmm_error_messages(self, device, dtype):\n          from torch.sparse._triton_ops import bsr_dense_mm\n\nFile: torch/__init__.py (modified)\n  )\n  from typing_extensions import ParamSpec as _ParamSpec, TypeIs as _TypeIs\n+from . import version\n+\n  if TYPE_CHECKING:\n      from .types import Device, IntLikeType\n+\n+# multipy/deploy is setting this import before importing torch, this is the most  # codespell:ignore multipy\n+# reliable way we have to detect if we're running within deploy.\n+# https://github.com/pytorch/multipy/blob/d60f34ad38c371e441fe7ffdb77a3c3dda5a5d19/multipy/runtime/interpreter/interpreter_impl.cpp#L134-L137  # codespell:ignore multipy # noqa: B950\n+def _running_with_deploy() -> builtins.bool:\n+    return sys.modules.get(\"torch._meta_registrations\", None) is object\n+\n+\n  from torch._utils import (\n      _functionalize_sync as _sync,\n      _import_dotted_name,\n      USE_GLOBAL_DEPS,\n      USE_RTLD_GLOBAL_WITH_LIBTORCH,\n  )\n\nFile: torch/_dynamo/_trace_wrapped_higher_order_op.py (modified)\n  __all__ = [\"trace_wrapped\"]\n-@torch.library.custom_op(\"flex_lib::zeros_and_scatter\", mutates_args=())  # type: ignore[misc]\n-def zeros_and_scatter(\n-    shape: list[int],\n-    indices: list[Tensor],\n-    vals: Tensor,\n-) -> Tensor:\n-    \"\"\"Custom Op so that we can register a custom lowering for the new_output + scatter in the backwards pass\"\"\"\n-    grad = torch.zeros(shape, device=vals.device, dtype=vals.dtype)\n-    return torch.ops.aten.index_put(grad, indices, vals, accumulate=True)\n-\n-\n-@zeros_and_scatter.register_fake  # type: ignore[misc]\n-def _(\n-    shape: list[int],\n-    indices: list[Tensor],\n-    vals: Tensor,\n-) -> Tensor:\n-    return vals.new_empty(shape)\n-\n\nFile: torch/_dynamo/trace_rules.py (modified)\n          \"torch._lowrank.svd_lowrank\",\n          \"torch._preload_cuda_deps\",\n          \"torch._register_device_module\",\n+        \"torch._running_with_deploy\",\n          \"torch._utils._dummy_type\",\n          \"torch._utils._flatten_dense_tensors\",\n          \"torch._utils._unflatten_dense_tensors\",",
    "comment": "Revert \"[BE] remove torch deploy - conditionals (#158288)\"",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "f8fafdc7a6d260cea6c145643f4cf73631c81460",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: test/test_deploy.py (added)\n+# Owner(s): [\"oncall: package/deploy\"]\n+\n+import textwrap\n+import types\n+\n+from torch.testing._internal.common_utils import run_tests, TestCase\n+from torch.utils._freeze import Freezer, PATH_MARKER\n+\n+\n+class TestFreezer(TestCase):\n+    \"\"\"Tests the freeze.py script\"\"\"\n+\n+    def test_compile_string(self):\n+        freezer = Freezer(True)\n+        code_str = textwrap.dedent(\n+            \"\"\"\n+            class MyCls:\n+                def __init__(self) -> None:\n+                    pass\n+            \"\"\"\n\nFile: tools/lldb/deploy_debugger.py (added)\n+import lldb  # type: ignore[import]\n+\n+\n+# load into lldb instance with:\n+#   command script import tools/lldb/deploy_debugger.py\n+\n+target = lldb.debugger.GetSelectedTarget()\n+bp = target.BreakpointCreateByRegex(\"__deploy_register_code\")\n+bp.SetScriptCallbackBody(\n+    \"\"\"\\\n+process = frame.thread.GetProcess()\n+target = process.target\n+symbol_addr = frame.module.FindSymbol(\"__deploy_module_info\").GetStartAddress()\n+info_addr = symbol_addr.GetLoadAddress(target)\n+e = lldb.SBError()\n+ptr_size = 8\n+str_addr = process.ReadPointerFromMemory(info_addr, e)\n+file_addr = process.ReadPointerFromMemory(info_addr + ptr_size, e)\n+file_size = process.ReadPointerFromMemory(info_addr + 2*ptr_size, e)\n+load_bias = process.ReadPointerFromMemory(info_addr + 3*ptr_size, e)",
    "comment": "Revert \"[BE] Remove torch deploy | remove torch deploy specific files (#158290)\"",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "c8316d0e79cfcdf3b0b2c450a391b58af1b9f29c",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: docs/source/conf.py (modified)\n      \"z3op\",\n      \"z3str\",\n      # torch.fx.graph_module\n+    \"reduce_deploy_graph_module\",\n      \"reduce_graph_module\",\n      \"reduce_package_graph_module\",\n      # torch.fx.node\n\nFile: torch/_dynamo/trace_rules.py (modified)\n      \"torch._custom_op\",\n      \"torch._custom_ops\",\n      \"torch._decomp\",\n+    \"torch._deploy\",\n      \"torch._dispatch\",\n      \"torch._dynamo\",\n      \"torch._export\",\n\nFile: torch/fx/_lazy_graph_module.py (modified)\n      forward = _lazy_forward\n+    # TODO: we should handle __reduce_deploy__ the same way as __reduce_package__,\n+    # or __reduce__ by calling _real_recompile. But I don't find a good way\n+    # to test __reduce_deploy__ out. Also it's very unlikely that LazyGraphModule\n+    # will be used in torch::deploy. So it's skipped for now.\n+\n      def __reduce_package__(self, exporter: PackageExporter):\n          \"\"\"\n          Follow GraphModule.__reduce__ but call 'self._real_recompile' rather\n\nFile: torch/fx/graph_module.py (modified)\n  __all__ = [\n      \"reduce_graph_module\",\n      \"reduce_package_graph_module\",\n+    \"reduce_deploy_graph_module\",\n      \"GraphModule\",\n  ]\n      return _deserialize_graph_module(forward, body)\n+@compatibility(is_backward_compatible=True)\n+def reduce_deploy_graph_module(\n+    importer: PackageImporter, body: dict[Any, Any], import_block: str\n+) -> torch.nn.Module:\n+    ns = {}\n+    ns[\"__builtins__\"] = importer.patched_builtins\n+    fn_src = body.get(\"_code\")\n+    assert fn_src is not None\n+    forward = _forward_from_src(import_block + fn_src, ns)\n+    return _deserialize_graph_module(forward, body)\n+\n+\n  # We create a dummy class here because symbolic_trace pulls the forward()",
    "comment": "Revert \"[BE] Remove __reduce_deploy__ (#158291)\"",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "a9f6770edd84c85706a641d4f5b4e8a8c5d0776c",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: test/dynamo/test_generator.py (modified)\n          ctx = whoo()\n          next(ctx)\n          with self.assertRaisesRegex(\n-            Unsupported, \"Generator as graph argument is not supported\"\n+            Unsupported, \"Detected a method call to a user-defined generator object.\"\n          ):\n              fn(t, ctx)\n          ctx = whoo(t)\n          next(ctx)\n          with self.assertRaisesRegex(\n-            Unsupported, \"Generator as graph argument is not supported\"\n+            Unsupported, \"Detected a method call to a user-defined generator object.\"\n          ):\n              fn(t, ctx)\n          t = torch.randn(2)\n          ctx = whoo()\n          with self.assertRaisesRegex(\n-            Unsupported, \"Generator as graph argument is not supported\"\n+            Unsupported, \"Detected a method call to a user-defined generator object.\"\n          ):",
    "comment": "[dynamo] unimplemented -> unimplemented_v2 for user_defined.py (#156652)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "6fcb2b44137ed59482fb9b5f18f7ee6b0ba5e2d6",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: test/functorch/test_aotdispatch.py (modified)\n          inps = [torch.randn(2, 2), torch.ones(2)]\n          gm, _ = aot_export_module(M(), inps, trace_joint=False, pre_dispatch=True)\n          self.assertExpectedInline(\n-            normalize_gm(gm.print_readable(False)),\n+            normalize_gm(gm.print_readable(False, expanded_def=True)),\n              \"\"\"\\\n  class <lambda>(torch.nn.Module):\n-    def forward(self, arg0_1: \"f32[2, 2]\", arg1_1: \"f32[2]\"):\n+    def forward(\n+        self,\n+        arg0_1: \"f32[2, 2]\",  # PlainAOTInput(idx=0)\n+        arg1_1: \"f32[2]\",  # PlainAOTInput(idx=1)\n+    ):\n          sum_1: \"f32[]\" = torch.ops.aten.sum.default(arg0_1)\n          gt: \"b8[]\" = torch.ops.aten.gt.Scalar(sum_1, 4);  sum_1 = None\n          add: \"f32[2, 2]\" = torch.ops.aten.add.Tensor(getitem, 3)\n          add_1: \"f32[2, 2]\" = torch.ops.aten.add.Tensor(getitem, 4);  getitem = None\n-        return (add, add_1)\n+        return (\n+            add,  # PlainAOTOutput(idx=0)\n\nFile: torch/_functorch/_aot_autograd/descriptors.py (modified)\n      def expr(self) -> str:\n          return f\"__dummy{self.idx}\"\n+\n+\n+@dataclasses.dataclass(frozen=True)\n+class SavedForBackwardsAOTOutput(AOTOutput):\n+    idx: int\n+\n+    def expr(self) -> str:\n+        return f\"__saved_for_backwards_{self.idx}\"\n\nFile: torch/_functorch/_aot_autograd/graph_capture.py (modified)\n  from torchgen.utils import dataclass_repr\n  from .. import config\n-from .descriptors import AOTInput\n+from .descriptors import AOTInput, BackwardTokenAOTInput\n  from .functional_utils import (\n      assert_functional_graph,\n      propagate_input_mutation_stacktraces,\n  from .utils import (\n      call_and_expect_output_descs,\n      copy_fwd_metadata_to_bw_nodes,\n+    fn_wrappers,\n      register_buffer_assignment_hook,\n      root_module_when_exporting_non_strict,\n      unlift_tokens,\n          @functools.wraps(f)\n          def inner_f(*args):\n              nonlocal out_descs\n+            assert out_descs is None\n              out, out_descs = call_and_expect_output_descs(f, args)\n              return out",
    "comment": "Add expanded_def option for FX printing, render descriptor, update tests (#158708)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "204eb4da5eac199125ed6b78c957aeffcfc7df54",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: torch/_functorch/_aot_autograd/collect_metadata_analysis.py (modified)\n      transform_subclass,\n  )\n+from .descriptors import (\n+    AOTInput,\n+    AOTOutput,\n+    InputMutationAOTOutput,\n+    IntermediateBaseAOTOutput,\n+    PlainAOTOutput,\n+    TangentAOTInput,\n+)\n  from .functional_utils import (\n      are_all_mutations_hidden_from_autograd,\n      are_all_mutations_under_no_grad_or_inference_mode,\n  def run_functionalized_fw_and_collect_metadata(\n      f,\n      *,\n+    flat_args_descs: list[AOTInput],\n      keep_input_mutations: bool,\n      # TODO: refactor to kill this flag\n      is_train: bool = False,\n\nFile: torch/_functorch/_aot_autograd/graph_compile.py (modified)\n  from contextlib import nullcontext\n  from typing import Any, Callable, Optional, TYPE_CHECKING, Union\n+\n+if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+\n  import torch\n  import torch.utils._pytree as pytree\n  import torch.utils.dlpack\n      should_bundle_autograd_cache,\n      should_use_remote_autograd_cache,\n  )\n+from .descriptors import AOTOutput, PlainAOTOutput\n  from .graph_capture import aot_dispatch_autograd_graph, aot_dispatch_base_graph\n  from .logging_utils import track_graph_compiling\n  from .runtime_wrappers import (\n      AOTConfig,\n      AOTGraphCapture,\n      AOTState,\n+    FlatFn,",
    "comment": "Track descriptors for all inputs/outputs of AOTAutograd traced graph (#158624)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "bf311141d6bbcd634df92bf4e48177dbb3f3e94c",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: test/inductor/test_torchinductor.py (modified)\n          cf(x, 1e-5)\n          cf(x, 1e-6)\n+    def test_div_presicion_accuracy(self):\n+        # fix https://github.com/pytorch/pytorch/issues/157959\n+        def forward(x, y):\n+            return (x / y).sum()\n+\n+        x = torch.rand((5, 5))\n+        y = 101\n+        self.common(forward, (x, y))\n+\n      def test_mul_softmax_symfloat(self):\n          def forward(x, y):\n              z = x.mul(y * x.shape[-1])\n\nFile: torch/_inductor/lowering.py (modified)\n      if is_integral:\n          return truncdiv(a, b)\n-    if (divisor := get_constant_value(b)) is not None:\n+    # Disable CPU optimization to avoid precision issues.\n+    # see https://github.com/pytorch/pytorch/issues/157959\n+    if (divisor := get_constant_value(b)) is not None and a.get_device().type != \"cpu\":\n          # Replace divide by constant with multiply by reciprocal\n          if divisor.value == 0:\n              reciprocal = math.copysign(float(\"inf\"), divisor.value)",
    "comment": "[inductor][cpu] Stop lowering div to reciprocal multiplication to preserve precision when the divisor is a scalar and device is on cpu (#158231)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "92e93bb580f31d405a72ee58f30fe82908bbeacf",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: torch/_subclasses/fake_tensor.py (modified)\n          # If there's a Python meta, prefer that over the decomposition\n          from torch._decomp import meta_table as meta_table\n-        if func not in meta_table and not self.cpp_meta_supports_symint(func):\n+        if (\n+            func not in meta_table\n+            and not self.cpp_meta_supports_symint(func)\n+            and not (has_symbolic_sizes and func in self._view_fake_tensor_impl_ops)\n+        ):\n              from torch._decomp import decomposition_table\n              # Prefer Python decompositions over C++ ones\n          aten._sparse_coo_tensor_with_dims_and_tensors.default,\n      )\n+    _view_fake_tensor_impl_ops = ordered_set(\n+        aten.view.default, aten._unsafe_view.default\n+    )\n+\n      def cpp_meta_supports_symint(self, func: OpOverload) -> bool:\n          if torch.Tag.view_copy in func.tags:\n              return True",
    "comment": "move view_meta to fake impl (#158406)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "aaa384b2d4d6be11b5cfc168780694da60217c65",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: torch/_inductor/utils.py (modified)\n                  # Let's not fail if we can't clean up the temp dir. Also note that for\n                  # Windows, we can't delete the loaded modules because the module binaries\n                  # are open.\n+                ignore_errors=is_windows(),\n                  onerror=lambda func, path, exc_info: log.warning(\n                      \"Failed to remove temporary cache dir at %s\",\n                      inductor_cache_dir,",
    "comment": "[inductor] add missing ignore_errors parameter for Windows. (#159025)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "e38a2b3d0f85656adbce979f476af937ed8f5b3b",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: test/distributed/checkpoint/_experimental/test_builder.py (modified)\n          # Create async checkpointer using factory function with default parameters\n          config: CheckpointerConfig = CheckpointerConfig()\n          config.staging_config = CheckpointStagerConfig(\n-            use_cuda_non_blocking_copy=torch.cuda.is_available(),\n-            use_pinned_memory=torch.cuda.is_available(),\n+            use_non_blocking_copy=torch.accelerator.is_available(),\n+            use_pinned_memory=torch.accelerator.is_available(),\n          )\n          checkpointer = make_async_checkpointer(config=config, rank_info=self.rank_info)\n\nFile: test/distributed/checkpoint/_experimental/test_staging.py (modified)\n          if torch.cuda.is_available():\n              self.skipTest(\"CUDA is available, cannot test CUDA unavailable scenario\")\n-        options = CheckpointStagerConfig(use_cuda_non_blocking_copy=True)\n+        options = CheckpointStagerConfig(use_non_blocking_copy=True)\n          with self.assertRaises(AssertionError):\n              DefaultStager(options)\n                  use_pinned_memory=False,\n                  use_shared_memory=False,\n                  use_async_staging=False,\n-                use_cuda_non_blocking_copy=False,\n+                use_non_blocking_copy=False,\n              ),\n              # Only pinned memory\n              CheckpointStagerConfig(\n                  use_pinned_memory=True,\n                  use_shared_memory=False,\n                  use_async_staging=False,\n-                use_cuda_non_blocking_copy=False,\n+                use_non_blocking_copy=False,\n              ),\n\nFile: test/distributed/checkpoint/e2e/test_e2e_save_and_load.py (modified)\n                      use_async_staging=zoc,\n                      use_shared_memory=use_shared_memory,\n                      use_pinned_memory=zoc,\n-                    use_cuda_non_blocking_copy=zoc,\n+                    use_non_blocking_copy=zoc,\n                  )\n                  stager = DefaultStager(staging_options)\n              async_save_response_or_future = saver.async_save(\n\nFile: torch/distributed/checkpoint/_experimental/staging.py (modified)\n          use_async_staging (bool): Enable asynchronous staging using a\n              background thread pool. Allows overlapping computation with\n              staging operations. Requires CUDA. Default: True\n-        use_cuda_non_blocking_copy (bool): Use non-blocking CUDA memory\n+        use_non_blocking_copy (bool): Use non-blocking device memory\n              copies with stream synchronization. Improves performance by\n              allowing CPU work to continue during GPU transfers. Default: True\n      use_pinned_memory: bool = True\n      use_shared_memory: bool = True\n      use_async_staging: bool = True\n-    use_cuda_non_blocking_copy: bool = True\n+    use_non_blocking_copy: bool = True\n  class DefaultStager(CheckpointStager):\n          if self._config.use_async_staging:\n              self._staging_executor = ThreadPoolExecutor(max_workers=1)\n-            if torch.cuda.is_available():\n+            if torch.accelerator.is_available():\n                  # Note: stream needs to be initialized on the main thread after default cuda\n                  # stream is setup/used to avoid the risk of accidentally reusing the main\n                  # compute stream or in other cases kernels actually launching from the\n\nFile: torch/distributed/checkpoint/staging.py (modified)\n          use_async_staging (bool): Enable asynchronous staging using a\n              background thread pool. Allows overlapping computation with\n              staging operations. Requires CUDA. Default: True\n-        use_cuda_non_blocking_copy (bool): Use non-blocking CUDA memory\n+        use_non_blocking_copy (bool): Use non-blocking device memory\n              copies with stream synchronization. Improves performance by\n              allowing CPU work to continue during GPU transfers. Default: True\n      use_pinned_memory: bool = True\n      use_shared_memory: bool = True\n      use_async_staging: bool = True\n-    use_cuda_non_blocking_copy: bool = True\n+    use_non_blocking_copy: bool = True\n  class DefaultStager(AsyncStager):\n          self._staging_stream = None\n          if self._config.use_async_staging:\n              self._staging_executor = ThreadPoolExecutor(max_workers=1)\n-            if torch.cuda.is_available():\n+            if torch.accelerator.is_available():\n                  # Note: stream needs to be initialized on the main thread after default cuda\n                  # stream is setup/used to avoid the risk of accidentally reusing the main",
    "comment": "Device agnostic for DCP (#158337)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "02ca965560e74e44dfd446d97b77d84eb35b2de2",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: torch/_tensor_docs.py (modified)\n  Args:\n      src (Tensor): the source tensor to copy from\n-    non_blocking (bool): if ``True`` and this copy is between CPU and GPU,\n+    non_blocking (bool, optional): if ``True`` and this copy is between CPU and GPU,\n          the copy may occur asynchronously with respect to the host. For other\n-        cases, this argument has no effect.\n+        cases, this argument has no effect. Default: ``False``\n  \"\"\",\n  )\n  then no copy is performed and the original object is returned.\n  Args:\n-    device (:class:`torch.device`): The destination GPU device.\n+    device (:class:`torch.device`, optional): The destination GPU device.\n          Defaults to the current CUDA device.\n-    non_blocking (bool): If ``True`` and the source is in pinned memory,\n+    non_blocking (bool, optional): If ``True`` and the source is in pinned memory,\n          the copy will be asynchronous with respect to the host.\n          Otherwise, the argument has no effect. Default: ``False``.\n      {memory_format}\n  then no copy is performed and the original object is returned.",
    "comment": "Add missing optional for tensor ops (#159028)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "9685fc36d4dc4b3e6c52a47791403a21888948f8",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: benchmarks/dynamo/genai_layers/utils.py (modified)\n                  print(\n                      f\"Failed to run {backend} backend on {self.name} kernel for {setting} due to {e}\"\n                  )\n-                self.available_backends.remove(backend)\n+                self.available_backends.remove(backend)  # noqa: B909\n                  continue\n              mem_bytes = self.get_memory_bytes(args_ref, kwargs_ref)\n              perf = Performance(setting, avg_time, mem_bytes)\n\nFile: docs/source/conf.py (modified)\n  theme_variables = pytorch_sphinx_theme2.get_theme_variables()\n  html_context = {\n-    \"theme_variables\": theme_variables,\n      \"github_url\": \"https://github.com\",\n      \"github_user\": \"pytorch\",\n      \"github_repo\": \"pytorch\",\n      \"feedback_url\": \"https://github.com/pytorch/pytorch\",\n      \"github_version\": \"main\",\n      \"pytorch_project\": \"docs\",\n      \"doc_path\": \"docs/source\",\n-    \"theme_variables\": theme_variables,  # noqa: F601\n+    \"theme_variables\": theme_variables,\n      # library links are defined in\n      # pytorch_sphinx_theme2/pytorch_sphinx_theme2/links.json\n      \"library_links\": theme_variables.get(\"library_links\", []),\n\nFile: test/distributed/optim/test_zero_redundancy_optimizer.py (modified)\n                  # Increased tolerances are needed to pass when using TF32\n                  # See: https://github.com/pytorch/pytorch/issues/67764\n-                (\n-                    torch.testing.assert_close(\n-                        local_loss.cpu(),\n-                        ddp_loss.cpu(),\n-                        rtol=1e-03,\n-                        atol=1e-08,\n-                    ),\n-                    \"Losses differ between local optimizer and ZeRO\",\n+                torch.testing.assert_close(\n+                    local_loss.cpu(),\n+                    ddp_loss.cpu(),\n+                    rtol=1e-03,\n+                    atol=1e-08,\n+                    msg=\"Losses differ between local optimizer and ZeRO\",\n                  )\n                  for local_p, ddp_p in zip(\n                      local_model.parameters(), ddp_model.parameters()\n                  ):\n\nFile: test/distributions/test_distributions.py (modified)\n              except NotImplementedError:\n                  pass\n              self.assertNotIn(\"probs\", dist.__dict__, msg=message)\n-            dist.batch_shape, dist.event_shape\n+            _ = (dist.batch_shape, dist.event_shape)\n              self.assertNotIn(\"probs\", dist.__dict__, msg=message)\n      def test_lazy_probs_initialization(self):\n              except NotImplementedError:\n                  pass\n              self.assertNotIn(\"logits\", dist.__dict__, msg=message)\n-            dist.batch_shape, dist.event_shape\n+            _ = (dist.batch_shape, dist.event_shape)\n              self.assertNotIn(\"logits\", dist.__dict__, msg=message)",
    "comment": "[BE] fix remaining flake8 v7 warnings (#159044)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "f5e2de928bc669240d742695f62e3aa392c3f4d7",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: benchmarks/instruction_counts/worker/main.py (modified)\n          # Runner process sent SIGINT.\n          sys.exit()\n-    except BaseException:\n+    except BaseException:  # noqa: B036\n          trace_f = io.StringIO()\n          traceback.print_exc(file=trace_f)\n          result = WorkerFailure(failure_trace=trace_f.getvalue())\n\nFile: functorch/benchmarks/chrome_trace_parser.py (modified)\n                  filenames, total_length\n              )\n              print(f\"{modelname}, {utilization}, {mm_conv_utilization}\")\n-        except BaseException:\n+        except BaseException:  # noqa: B036\n              log.exception(\"%s, ERROR\", filename)\n              print(f\"{filename}, ERROR\")\n\nFile: test/distributed/test_c10d_functional_native.py (modified)\n                  try:\n                      func(arg)\n                      compiled(arg)\n-                except BaseException as exc:\n+                except BaseException as exc:  # noqa: B036\n                      self.exc = exc\n              def join(self):\n\nFile: test/dynamo/test_exceptions.py (modified)\n          def cm():\n              try:\n                  yield\n-            except BaseException:\n+            except BaseException:  # noqa: B036\n                  raise ValueError  # noqa: B904\n          @contextlib.contextmanager\n                  for x, y in args:\n                      try:\n                          fn(x, y)\n-                    except BaseException:\n+                    except BaseException:  # noqa: B036\n                          new_exc = sys.exc_info()\n                          fix_exc_context(frame_exc[1], new_exc[1], prev_exc[1])\n                          prev_exc = new_exc\n                  try:\n                      fixed_ctx = prev_exc[1].__context__\n                      raise prev_exc[1]\n-                except BaseException:\n+                except BaseException:  # noqa: B036\n\nFile: test/test_cuda.py (modified)\n              try:\n                  with torch.cuda.stream(stream):\n                      mem = torch.cuda.caching_allocator_alloc(1024)\n-            except BaseException:\n+            except BaseException:  # noqa: B036\n                  if mem is None:\n                      return\n              try:\n                  torch.cuda.caching_allocator_delete(mem)\n                  mem = None\n                  return None\n-            except BaseException:\n+            except BaseException:  # noqa: B036\n                  pass\n          def throws_on_cuda_event(capture_error_mode):",
    "comment": "[BE] add noqa for flake8 rule B036: found `except BaseException` without re-raising (#159043)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "f903bc475cc0b649155d2c1f113d4a857667e7f5",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: test/cpp_extensions/open_registration_extension/torch_openreg/setup.py (modified)\n  from setuptools import Extension, find_packages, setup\n-PACKAGE_NAME = \"torch_openreg\"\n  BASE_DIR = os.path.dirname(os.path.realpath(__file__))\n+RUN_BUILD_DEPS = any(arg in {\"clean\", \"dist_info\"} for arg in sys.argv)\n  def get_pytorch_dir():\n  class BuildClean(clean):\n      def run(self):\n-        for i in [\"build\", \"install\", \"torch_openreg.egg-info\", \"torch_openreg/lib\"]:\n+        for i in [\"build\", \"install\", \"torch_openreg/lib\"]:\n              dirs = os.path.join(BASE_DIR, i)\n              if os.path.exists(dirs) and os.path.isdir(dirs):\n                  shutil.rmtree(dirs)\n                      os.remove(os.path.join(dirpath, filename))\n-RUN_BUILD_DEPS = any(arg == \"clean\" for arg in sys.argv)\n-\n-\n  def main():\n      if not RUN_BUILD_DEPS:\n          build_deps()\n          Extension(\n\nFile: test/cpp_extensions/open_registration_extension/torch_openreg/torch_openreg/__init__.py (modified)\n  import torch\n+\n  import torch_openreg._C  # type: ignore[misc]\n  import torch_openreg.openreg",
    "comment": "[OpenReg] add pyproject.toml for openreg (#158440)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "b635359e4c4003b62fbd0d6239e78e0489eec019",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: test/test_openreg.py (modified)\n  import torch\n  from torch.serialization import safe_globals\n  from torch.testing._internal.common_utils import (\n-    IS_LINUX,\n      run_tests,\n      skipIfTorchDynamo,\n      skipIfXpu,\n  if __name__ == \"__main__\":\n-    if IS_LINUX:\n-        run_tests()\n+    run_tests()",
    "comment": "[OpenReg] Improve README.md and optimize some codes for OpenReg (#158415)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "f1a1aa9490cb24d74717b8369c29e82eb319a47e",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: test/test_custom_ops.py (modified)\n          loss.backward()\n          self.assertEqual(x.grad, temp)\n+    # Using a non-existent DSO is a quick way to trigger an OSError,\n+    # which can be used to not break BC.\n+    def test_load_library(self):\n+        with self.assertRaisesRegex(\n+            OSError, \"Could not load this library: .*libnoexist.so\"\n+        ):\n+            torch.ops.load_library(\"libnoexist.so\")\n+\n  def op_with_incorrect_schema(testcase, name):\n      lib = testcase.lib()",
    "comment": "Add tests for torch.ops.load_library (#158838)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "c60d382870b9d56861bc951304ebbf3a35866798",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: test/dynamo/imports_non_circular_repro.py (added)\n+# Owner(s): [\"module: dynamo\"]\n+\"\"\"\n+This file is aimed at providing a simple testcase to reproduce\n+https://github.com/pytorch/pytorch/issues/158120\n+\n+This means that we cannot rely on torch.dynamo before importing\n+torch.export, so we can't add this to a file that is a dynamo testcase\n+\"\"\"\n+\n+import unittest\n+\n+import torch\n+\n+\n+class TestImports(unittest.TestCase):\n+    def test_circular_import_with_export_meta(self):\n+        from torch.export import export\n+\n+        conv = torch.nn.Conv2d(3, 64, 3, padding=1)\n+        # Note: we want to validate that export within\n\nFile: test/dynamo/test_modes.py (modified)\n      # Needs larger cache size since we recompile for each op\n      @patch.object(torch._dynamo.config, \"recompile_limit\", 48)\n      def test_builtin_equivalent_funcs(self):\n+        from torch._dynamo.variables.builtin import (\n+            BUILTIN_TO_TENSOR_FN_MAP,\n+            BUILTIN_TO_TENSOR_RFN_MAP,\n+        )\n          from torch._dynamo.variables.torch_function import (\n              bin_int_ops,\n              bin_ops,\n-            BUILTIN_TO_TENSOR_FN_MAP,\n-            BUILTIN_TO_TENSOR_RFN_MAP,\n              tensor_and_int_ops,\n              un_int_ops,\n              un_ops,\n\nFile: torch/_dynamo/variables/torch_function.py (modified)\n  from torch._guards import Source\n  from torch.overrides import (\n      _get_overloaded_args,\n-    BaseTorchFunctionMode,\n      get_default_nowrap_functions,\n      TorchFunctionMode,\n  )\n      operator.length_hint,\n  ]\n-BUILTIN_TO_TENSOR_FN_MAP = {}\n-\n-# These functions represent the r* versions of the above ops\n-# Basically, if __add__(1, Tensor) is called, it is translated\n-# to __radd__(Tensor, 1).\n-# In the builtin var, we check if there is a tensor in the first args position,\n-# if not, we swap the args and use the r* version of the op.\n-BUILTIN_TO_TENSOR_RFN_MAP = {}\n-\n-\n-def populate_builtin_to_tensor_fn_map():",
    "comment": "[Bugfix] Fix circular import between export and dynamo from tensor fn map (#158931)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "efc810c7d02f492eed5f1393ffd8c0b35b52e46f",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: test/profiler/test_python_tracer.py (removed)\n-# Owner(s): [\"oncall: profiler\"]\n-\n-import json\n-import sys\n-import time\n-\n-from torch.profiler import profile, ProfilerActivity\n-from torch.testing._internal.common_utils import (\n-    run_tests,\n-    skipIfPythonVersionMismatch,\n-    TemporaryFileName,\n-    TestCase,\n-)\n-\n-\n-class TestPythonTracer(TestCase):\n-    @skipIfPythonVersionMismatch(lambda major, minor, micro: major == 3 and minor == 12)\n-    def test_method_with_c_function(self):\n-        class A:\n-            method_with_c_function = classmethod(repr)\n\nFile: torch/testing/_internal/common_utils.py (modified)\n              torch.backends.cuda.matmul.fp32_precision = old_cuda_matmul_p\n      return recover()(fn)\n-\n-def skipIfPythonVersionMismatch(predicate):\n-    vi = sys.version_info\n-\n-    def dec_fn(fn):\n-        @wraps(fn)\n-        def wrap_fn(self, *args, **kwargs):\n-            if predicate(vi.major, vi.minor, vi.micro):\n-                return fn(self, *args, **kwargs)\n-            else:\n-                raise unittest.SkipTest(\"Python version mismatch\")\n-        return wrap_fn\n-    return dec_fn",
    "comment": "Revert \"[Profiler] Fix lost C call events problem in Python 3.12.0-3.12.4 (#155446)\"",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "b533f121200bb1f8669dd367f8ba8b4564b8fb7d",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: test/dynamo/test_error_messages.py (modified)\n    Hint: Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled.\n    Hint: It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues.\n-  Developer debug context: raised exception ExceptionVariable(<class 'RuntimeError'>)\n+  Developer debug context: raised exception RuntimeError([ConstantVariable(str: 'test')])\n   For more details about this graph break, please visit: https://pytorch-labs.github.io/compile-graph-break-site/gb/gb0088.html\n\nFile: torch/_dynamo/symbolic_convert.py (modified)\n          self.call_function(fn, [typ, val, tb], {})\n      def exception_handler(self, raised_exception):\n+        def bubble_exception_to_interpreter():\n+            # Bubble the exception to the interpreter\n+            curr_exc = self.exn_vt_stack.get_current_exception()\n+            dynamo_exc = exc.get_dynamo_observed_exception(curr_exc.python_type())\n+            assert isinstance(raised_exception, dynamo_exc)  # sanity check\n+            unimplemented_v2(\n+                gb_type=\"Observed exception\",\n+                context=f\"raised exception {curr_exc.python_type_name()}({curr_exc.args})\",\n+                explanation=observed_exn_gb_explanation,\n+                hints=[\n+                    *graph_break_hints.USER_ERROR,\n+                    *graph_break_hints.SUPPORTABLE,\n+                ],\n+            )\n+\n          observed_exn_gb_explanation = (\n              \"Dynamo found no exception handler at the top-level compiled function \"\n              \"when encountering an exception. Exception will propagate outside the compiled region.\"\n\nFile: torch/_dynamo/variables/user_defined.py (modified)\n      def __context__(self):\n          return self.exc_vt.__context__\n+    @property\n+    def args(self):\n+        return self.exc_vt.args\n+\n      def set_context(self, context: \"variables.ExceptionVariable\"):\n          return self.exc_vt.set_context(context)",
    "comment": "Update context  in `unimplemented_v2` when exception bubbles up to the interpreter (#158924)",
    "language": "diff",
    "repo": "pytorch/pytorch",
    "sha": "de85ee73ae1594ee234064e33bbde1682f432234",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: whisper/triton_ops.py (modified)\n          tl.store(y_ptr + offsets, MIDDLE_ROW_HERE, mask=mask)  # noqa: F821\n      kernel = triton.JITFunction(kernel.fn)\n-    kernel.src = kernel.src.replace(\n+    new_kernel = kernel.src.replace(\n          \"    LOAD_ALL_ROWS_HERE\",\n          \"\\n\".join(\n              [\n              ]\n          ),\n      )\n-    kernel.src = kernel.src.replace(\n+\n+    new_kernel = new_kernel.replace(\n          \"    BUBBLESORT_HERE\",\n          \"\\n\\n\".join(\n              [\n              ]\n          ),\n      )\n-    kernel.src = kernel.src.replace(\"MIDDLE_ROW_HERE\", f\"row{filter_width // 2}\")",
    "comment": "Fixed triton kernel update to support latest triton versions (#2588)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "86899243e9fd1047a04a0e3991ef4b239c639d56",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: whisper/normalizers/basic.py (modified)\n      and drop any diacritics (category 'Mn' and some manual mappings)\n      \"\"\"\n      return \"\".join(\n-        c\n-        if c in keep\n-        else ADDITIONAL_DIACRITICS[c]\n-        if c in ADDITIONAL_DIACRITICS\n-        else \"\"\n-        if unicodedata.category(c) == \"Mn\"\n-        else \" \"\n-        if unicodedata.category(c)[0] in \"MSP\"\n-        else c\n+        (\n+            c\n+            if c in keep\n+            else (\n+                ADDITIONAL_DIACRITICS[c]\n+                if c in ADDITIONAL_DIACRITICS\n+                else (\n+                    \"\"\n\nFile: whisper/utils.py (modified)\n                          yield start, end, \"\".join(\n                              [\n-                                re.sub(r\"^(\\s*)(.*)$\", r\"\\1<u>\\2</u>\", word)\n-                                if j == i\n-                                else word\n+                                (\n+                                    re.sub(r\"^(\\s*)(.*)$\", r\"\\1<u>\\2</u>\", word)\n+                                    if j == i\n+                                    else word\n+                                )\n                                  for j, word in enumerate(all_words)\n                              ]\n                          )",
    "comment": "pre-commit autoupdate && pre-commit run --all-files (#2484)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "26a7cacc83c2cfbbf743022da8331b29702ceedc",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: whisper/transcribe.py (modified)\n              if (\n                  no_speech_threshold is not None\n                  and decode_result.no_speech_prob > no_speech_threshold\n+                and logprob_threshold is not None\n+                and decode_result.avg_logprob < logprob_threshold\n              ):\n                  needs_fallback = False  # silence\n              if not needs_fallback:",
    "comment": "Bugfix: Illogical \"Avoid computing higher temperatures on no_speech\" (#1903)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "90db0de1896c23cbfaf0c58bc2d30665f709f170",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: whisper/transcribe.py (modified)\n      no_speech_threshold: Optional[float] = 0.6,\n      condition_on_previous_text: bool = True,\n      initial_prompt: Optional[str] = None,\n+    carry_initial_prompt: bool = False,\n      word_timestamps: bool = False,\n      prepend_punctuations: str = \"\\\"'“¿([{-\",\n      append_punctuations: str = \"\\\"'.。,，!！?？:：”)]}、\",\n          \"prompt-engineer\" a context for transcription, e.g. custom vocabularies or proper nouns\n          to make it more likely to predict those word correctly.\n+    carry_initial_prompt: bool\n+        If carry_initial_prompt is True, `initial_prompt` is prepended to the prompt of each internal\n+        `decode()` call. If there is not enough context space at the start of the prompt, it is\n+        left-sliced to make space.\n+\n      decode_options: dict\n          Keyword arguments to construct `DecodingOptions` instances\n      all_segments = []\n      prompt_reset_since = 0\n+    remaining_prompt_length = model.dims.n_text_ctx // 2 - 1\n      if initial_prompt is not None:",
    "comment": "Add option to carry initial_prompt with the sliding window (#2343)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "5979f03701209bb035a0a466f14131aeb1116cbb",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: setup.py (modified)\n  requirements = []\n  if sys.platform.startswith(\"linux\") and platform.machine() == \"x86_64\":\n-    requirements.append(\"triton>=2.0.0,<3\")\n+    requirements.append(\"triton>=2.0.0\")\n  setup(\n      name=\"openai-whisper\",",
    "comment": "Relax triton requirements for compatibility with pytorch 2.4 and newer (#2307)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "32d55d5d76c9ecbe2dfa3e6735896c648156ab63",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: whisper/timing.py (modified)\n      word_durations = np.array([t.end - t.start for t in alignment])\n      word_durations = word_durations[word_durations.nonzero()]\n      median_duration = np.median(word_durations) if len(word_durations) > 0 else 0.0\n+    median_duration = min(0.7, float(median_duration))\n      max_duration = median_duration * 2\n      # hack: truncate long words at sentence boundaries.\n\nFile: whisper/utils.py (modified)\n  import re\n  import sys\n  import zlib\n-from typing import Callable, Optional, TextIO\n+from typing import Callable, List, Optional, TextIO\n  system_encoding = sys.getdefaultencoding()\n      )\n+def get_start(segments: List[dict]) -> Optional[float]:\n+    return next(\n+        (w[\"start\"] for s in segments for w in s[\"words\"]),\n+        segments[0][\"start\"] if segments else None,\n+    )\n+\n+\n+def get_end(segments: List[dict]) -> Optional[float]:\n+    return next(\n+        (w[\"end\"] for s in reversed(segments) for w in reversed(s[\"words\"])),\n+        segments[-1][\"end\"] if segments else None,\n+    )\n+",
    "comment": "Skip silence around hallucinations (#1838)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: tests/test_transcribe.py (modified)\n      assert \"your country\" in transcription\n      assert \"do for you\" in transcription\n-    tokenizer = get_tokenizer(model.is_multilingual)\n+    tokenizer = get_tokenizer(model.is_multilingual, num_languages=model.num_languages)\n      all_tokens = [t for s in result[\"segments\"] for t in s[\"tokens\"]]\n      assert tokenizer.decode(all_tokens) == result[\"text\"]\n      assert tokenizer.decode_with_timestamps(all_tokens).startswith(\"<|0.00|>\")\n\nFile: whisper/__init__.py (modified)\n      \"medium\": \"https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt\",\n      \"large-v1\": \"https://openaipublic.azureedge.net/main/whisper/models/e4b87e7e0bf463eb8e6956e646f1e277e901512310def2c24bf0e11bd3c28e9a/large-v1.pt\",\n      \"large-v2\": \"https://openaipublic.azureedge.net/main/whisper/models/81f7c96c852ee8fc832187b0132e569d6c3065a3252ed18e56effd0b6a73e524/large-v2.pt\",\n-    \"large\": \"https://openaipublic.azureedge.net/main/whisper/models/81f7c96c852ee8fc832187b0132e569d6c3065a3252ed18e56effd0b6a73e524/large-v2.pt\",\n+    \"large-v3\": \"https://openaipublic.azureedge.net/main/whisper/models/e5b1a55b89c1367dacf97e3e19bfd829a01529dbfdeefa8caeb59b3f1b81dadb/large-v3.pt\",\n+    \"large\": \"https://openaipublic.azureedge.net/main/whisper/models/e5b1a55b89c1367dacf97e3e19bfd829a01529dbfdeefa8caeb59b3f1b81dadb/large-v3.pt\",\n  }\n  # base85-encoded (n_layers, n_heads) boolean arrays indicating the cross-attention heads that are\n      \"medium\": b\"ABzY8B0Jh+0{>%R7}kK1fFL7w6%<-Pf*t^=N)Qr&0RR9\",\n      \"large-v1\": b\"ABzY8r9j$a0{>%R7#4sLmoOs{s)o3~84-RPdcFk!JR<kSfC2yj\",\n      \"large-v2\": b\"ABzY8zd+h!0{>%R7=D0pU<_bnWW*tkYAhobTNnu$jnkEkXqp)j;w1Tzk)UH3X%SZd&fFZ2fC2yj\",\n-    \"large\": b\"ABzY8zd+h!0{>%R7=D0pU<_bnWW*tkYAhobTNnu$jnkEkXqp)j;w1Tzk)UH3X%SZd&fFZ2fC2yj\",\n+    \"large-v3\": b\"ABzY8gWO1E0{>%R7(9S+Kn!D~%ngiGaR?*L!iJG9p-nab0JQ=-{D1-g00\",\n+    \"large\": b\"ABzY8gWO1E0{>%R7(9S+Kn!D~%ngiGaR?*L!iJG9p-nab0JQ=-{D1-g00\",\n  }",
    "comment": "large-v3 (#1761)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "c5d42560760a05584c1c79546a098287e5a771eb",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: whisper/audio.py (modified)\n          )\n      \"\"\"\n      assert n_mels == 80, f\"Unsupported n_mels: {n_mels}\"\n-    with np.load(\n-        os.path.join(os.path.dirname(__file__), \"assets\", \"mel_filters.npz\")\n-    ) as f:\n+\n+    filters_path = os.path.join(os.path.dirname(__file__), \"assets\", \"mel_filters.npz\")\n+    with np.load(filters_path, allow_pickle=False) as f:\n          return torch.from_numpy(f[f\"mel_{n_mels}\"]).to(device)",
    "comment": "allow_pickle=False while loading of mel matrix IN audio.py (#1511)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "7dfcd56304a7025f949a3d69bd38eb0916622453",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: whisper/transcribe.py (modified)\n  def cli():\n      from . import available_models\n+    def valid_model_name(name):\n+        if name in available_models() or os.path.exists(name):\n+            return name\n+        raise ValueError(\n+            f\"model should be one of {available_models()} or path to a model checkpoint\"\n+        )\n+\n      # fmt: off\n      parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n      parser.add_argument(\"audio\", nargs=\"+\", type=str, help=\"audio file(s) to transcribe\")\n-    parser.add_argument(\"--model\", default=\"small\", choices=available_models(), help=\"name of the Whisper model to use\")\n+    parser.add_argument(\"--model\", default=\"small\", type=valid_model_name, help=\"name of the Whisper model to use\")\n      parser.add_argument(\"--model_dir\", type=str, default=None, help=\"the path to save model files; uses ~/.cache/whisper by default\")\n      parser.add_argument(\"--device\", default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"device to use for PyTorch inference\")\n      parser.add_argument(\"--output_dir\", \"-o\", type=str, default=\".\", help=\"directory to save the outputs\")\n      parser.add_argument(\"--highlight_words\", type=str2bool, default=False, help=\"(requires --word_timestamps True) underline each word as it is spoken in srt and vtt\")\n      parser.add_argument(\"--max_line_width\", type=optional_int, default=None, help=\"(requires --word_timestamps True) the maximum number of characters in a line before breaking the line\")\n      parser.add_argument(\"--max_line_count\", type=optional_int, default=None, help=\"(requires --word_timestamps True) the maximum number of lines in a segment\")",
    "comment": "Add new option to generate subtitles by a specific number of words (#1729)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "6ed314fe4109eed40b79500eaf465935905e1aa7",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: whisper/timing.py (modified)\n          hook.remove()\n      # heads * tokens * frames\n-    weights = torch.stack([QKs[l][h] for l, h in model.alignment_heads.indices().T])\n+    weights = torch.stack([QKs[_l][_h] for _l, _h in model.alignment_heads.indices().T])\n      weights = weights[:, :, : num_frames // 2]\n      weights = (weights * qk_scale).softmax(dim=-1)\n      std, mean = torch.std_mean(weights, dim=-2, keepdim=True, unbiased=False)\n\nFile: whisper/tokenizer.py (modified)\n      @cached_property\n      def all_language_codes(self) -> Tuple[str]:\n-        return tuple(self.decode([l]).strip(\"<|>\") for l in self.all_language_tokens)\n+        return tuple(self.decode([_l]).strip(\"<|>\") for _l in self.all_language_tokens)\n      @cached_property\n      def sot_sequence_including_notimestamps(self) -> Tuple[int]:",
    "comment": "Add .pre-commit-config.yaml (#1528)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "8b330df0961ff64499609d70f1ba7c6a8a2f9b1a",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: whisper/model.py (modified)\n          \"\"\"\n          x : torch.LongTensor, shape = (batch_size, <= n_ctx)\n              the text tokens\n-        xa : torch.Tensor, shape = (batch_size, n_mels, n_audio_ctx)\n+        xa : torch.Tensor, shape = (batch_size, n_audio_ctx, n_audio_state)\n              the encoded audio features to be attended on\n          \"\"\"\n          offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0",
    "comment": "fix doc of TextDecoder (#1526)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "21010ef454fb25954b0914785180311fb077add9",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: whisper/decoding.py (modified)\n          self.kv_cache = {}\n          self.hooks = []\n+        key_modules = [block.attn.key for block in self.model.decoder.blocks]\n+        value_modules = [block.attn.value for block in self.model.decoder.blocks]\n+        self.kv_modules = key_modules + value_modules\n+\n      def logits(self, tokens: Tensor, audio_features: Tensor) -> Tensor:\n          if not self.kv_cache:\n              self.kv_cache, self.hooks = self.model.install_kv_cache_hooks()\n          self.hooks = []\n      def rearrange_kv_cache(self, source_indices):\n-        for module, tensor in self.kv_cache.items():\n-            # update the key/value cache to contain the selected sequences\n-            self.kv_cache[module] = tensor[source_indices].detach()\n+        if source_indices != list(range(len(source_indices))):\n+            for module in self.kv_modules:\n+                # update the key/value cache to contain the selected sequences\n+                self.kv_cache[module] = self.kv_cache[module][source_indices].detach()\n  class SequenceRanker:\n          return languages, lang_probs",
    "comment": "Avoid rearranging all caches (#1483)",
    "language": "diff",
    "repo": "openai/whisper",
    "sha": "b91c907694f96a3fb9da03d4bbdc83fbcd3a40a4",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: coding/python/binary_search.py (modified)\n  #!/usr/bin/env python\n  import random\n-from typing import List\n+from typing import List, Optional\n-def binary_search(arr: List[int], lb: int, ub: int, target: int) -> int:\n+def binary_search(arr: List[int], lb: int, ub: int, target: int) -> Optional[int]:\n      \"\"\"\n      A Binary Search Example which has O(log n) time complexity.\n      \"\"\"\n-    if lb <= ub:\n-        mid: int = lb + (ub - lb) // 2\n+    while lb <= ub:\n+        mid = lb + (ub - lb) // 2\n          if arr[mid] == target:\n              return mid\n          elif arr[mid] < target:\n-            return binary_search(arr, mid + 1, ub, target)\n+            lb = mid + 1\n          else:\n-            return binary_search(arr, lb, mid - 1, target)",
    "comment": "Convert binary search to iterative, add helper functions and main with complexity docstring while preserving functionality (#10552)",
    "language": "diff",
    "repo": "bregman-arie/devops-exercises",
    "sha": "689a09a3b1dc3e2205513d92fc897d9c0e42c0ea",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: scripts/aws s3 event triggering/s3-lambda/s3-lambda.py (modified)\n  import boto3\n  import json\n-def lambda_handler(event, context):\n-  # i want to know that event thing\n-  print(event)\n+def lambda_handler(event, context):\n+    # i want to know that event thing\n+    print(event)\n-  # extract relevant information from the s3 event trigger\n-  bucket_name=event['Records'][0]['s3']['bucket']['name']\n-  object_key=event['Records'][0]['s3']['object']['key']\n+    # extract relevant information from the s3 event trigger\n+    bucket_name = event['Records'][0]['s3']['bucket']['name']\n+    object_key = event['Records'][0]['s3']['object']['key']\n-  # perform desired operations with the upload file\n-  print(f\"File '{object_key}' was uploaded to bucket '{bucket_name}'\")\n+    # perform desired operations with the uploaded file\n+    print(f\"File '{object_key}' was uploaded to bucket '{bucket_name}'\")\n-  # example: send a notification via sns\n-  sns_client=boto3.client('sns')",
    "comment": "Broken PR build fix (#10556)",
    "language": "diff",
    "repo": "bregman-arie/devops-exercises",
    "sha": "51f7b8948f7f1b321edc4f730d349a6e0b8e5781",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: db/knex_migrations/2025-07-17-0000-mqtt-websocket-path.js (added)\n+exports.up = function (knex) {\n+    // Add new column monitor.mqtt_websocket_path\n+    return knex.schema\n+        .alterTable(\"monitor\", function (table) {\n+            table.string(\"mqtt_websocket_path\", 255).nullable();\n+        });\n+};\n+\n+exports.down = function (knex) {\n+    // Drop column monitor.mqtt_websocket_path\n+    return knex.schema\n+        .alterTable(\"monitor\", function (table) {\n+            table.dropColumn(\"mqtt_websocket_path\");\n+        });\n+};\n\nFile: server/model/monitor.js (modified)\n                  radiusSecret: this.radiusSecret,\n                  mqttUsername: this.mqttUsername,\n                  mqttPassword: this.mqttPassword,\n+                mqttWebsocketPath: this.mqttWebsocketPath,\n                  authWorkstation: this.authWorkstation,\n                  authDomain: this.authDomain,\n                  tlsCa: this.tlsCa,\n\nFile: server/monitor-types/mqtt.js (modified)\n              username: monitor.mqttUsername,\n              password: monitor.mqttPassword,\n              interval: monitor.interval,\n+            websocketPath: monitor.mqttWebsocketPath,\n          });\n          if (monitor.mqttCheckType == null || monitor.mqttCheckType === \"\") {\n       * @param {string} hostname Hostname / address of machine to test\n       * @param {string} topic MQTT topic\n       * @param {object} options MQTT options. Contains port, username,\n-     * password and interval (interval defaults to 20)\n+     * password, websocketPath and interval (interval defaults to 20)\n       * @returns {Promise<string>} Received MQTT message\n       */\n      mqttAsync(hostname, topic, options = {}) {\n          return new Promise((resolve, reject) => {\n-            const { port, username, password, interval = 20 } = options;\n+            const { port, username, password, websocketPath, interval = 20 } = options;\n              // Adds MQTT protocol to the hostname if not already present\n              if (!/^(?:http|mqtt|ws)s?:\\/\\//.test(hostname)) {\n                  reject(new Error(\"Timeout, Message not received\"));\n\nFile: server/server.js (modified)\n                  bean.mqttTopic = monitor.mqttTopic;\n                  bean.mqttSuccessMessage = monitor.mqttSuccessMessage;\n                  bean.mqttCheckType = monitor.mqttCheckType;\n+                bean.mqttWebsocketPath = monitor.mqttWebsocketPath;\n                  bean.databaseConnectionString = monitor.databaseConnectionString;\n                  bean.databaseQuery = monitor.databaseQuery;\n                  bean.authMethod = monitor.authMethod;",
    "comment": "Add Websocket path to mqtt monitor for WebSocket connection (#6009)",
    "language": "diff",
    "repo": "louislam/uptime-kuma",
    "sha": "2a6d9b4acda128d2958d5c7684fbbc46838bf8be",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: backend/tests/test_prompt_summary.py (modified)\n-from utils import format_prompt_summary\n+import io\n+import sys\n+from utils import format_prompt_summary, print_prompt_summary\n  def test_format_prompt_summary():\n      ]\n      summary = format_prompt_summary(messages)\n-    assert \"system: lorem ipsum\" in summary\n+    assert \"SYSTEM: lorem ipsum\" in summary\n      assert \"[2 images]\" in summary\n+\n+\n+def test_print_prompt_summary():\n+    messages = [\n+        {\"role\": \"system\", \"content\": \"short message\"},\n+        {\"role\": \"user\", \"content\": \"hello\"},\n+    ]\n+\n+    # Capture stdout\n+    captured_output = io.StringIO()\n\nFile: backend/utils.py (modified)\n              text = text[:40] + \"...\"\n          img_part = f\" + [{image_count} images]\" if image_count else \"\"\n-        parts.append(f\"{role}: {text}{img_part}\")\n+        parts.append(f\"  {role.upper()}: {text}{img_part}\")\n-    return \" / \".join(parts)\n+    return \"\\n\".join(parts)\n  def print_prompt_summary(prompt_messages: List[ChatCompletionMessageParam]):\n-    print(\"-\" * 20)\n-    print(\"Prompt summary:\")\n-    print(format_prompt_summary(prompt_messages))\n-    print(\"-\" * 20)\n-    print(\"\")\n+    summary = format_prompt_summary(prompt_messages)\n+    lines = summary.split('\\n')\n+    \n+    # Find the maximum line length, with a minimum of 20 and maximum of 80\n+    max_length = max(len(line) for line in lines) if lines else 20\n+    max_length = max(20, min(80, max_length))\n+    \n+    # Ensure title fits",
    "comment": "improve: enhance prompt summary formatting for better terminal display",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "a1c2793e0eeed87c2f917455e42d28e999c0ec1d",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: frontend/src/components/UrlInputSection.tsx (modified)\n          placeholder=\"Enter URL\"\n          onChange={(e) => setReferenceUrl(e.target.value)}\n          value={referenceUrl}\n+        onKeyDown={(e) => {\n+          if (e.key === \"Enter\" && !isLoading) {\n+            takeScreenshot();\n+          }\n+        }}\n        />\n        <Button\n          onClick={takeScreenshot}",
    "comment": "Add Enter key support for URL screenshot capture",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "b94cf77cdedd6ce07fc6a4a107d76db8df153650",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: backend/tests/test_screenshot.py (added)\n+import pytest\n+from routes.screenshot import normalize_url\n+\n+\n+class TestNormalizeUrl:\n+    \"\"\"Test cases for URL normalization functionality.\"\"\"\n+    \n+    def test_url_without_protocol(self):\n+        \"\"\"Test that URLs without protocol get https:// added.\"\"\"\n+        assert normalize_url(\"example.com\") == \"https://example.com\"\n+        assert normalize_url(\"www.example.com\") == \"https://www.example.com\"\n+        assert normalize_url(\"subdomain.example.com\") == \"https://subdomain.example.com\"\n+    \n+    def test_url_with_http_protocol(self):\n+        \"\"\"Test that existing http protocol is preserved.\"\"\"\n+        assert normalize_url(\"http://example.com\") == \"http://example.com\"\n+        assert normalize_url(\"http://www.example.com\") == \"http://www.example.com\"\n+    \n+    def test_url_with_https_protocol(self):\n+        \"\"\"Test that existing https protocol is preserved.\"\"\"",
    "comment": "Add pytest setup and remove standalone test script",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "825552761fdb62f8a5520987d35c2087ca64c67b",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: backend/routes/screenshot.py (modified)\n      # Parse the URL\n      parsed = urlparse(url)\n-    # If no scheme (protocol) is provided, add https://\n+    # Check if we have a scheme\n      if not parsed.scheme:\n-        # Handle cases like \"example.com\" or \"www.example.com\"\n+        # No scheme, add https://\n          url = f\"https://{url}\"\n-    elif parsed.scheme not in ['http', 'https']:\n-        # Only allow http and https protocols\n-        raise ValueError(f\"Unsupported protocol: {parsed.scheme}\")\n+    elif parsed.scheme in ['http', 'https']:\n+        # Valid scheme, keep as is\n+        pass\n+    else:\n+        # Check if this might be a domain with port (like example.com:8080)\n+        # urlparse treats this as scheme:netloc, but we want to handle it as domain:port\n+        if ':' in url and not url.startswith(('http://', 'https://', 'ftp://', 'file://')):\n+            # Likely a domain:port without protocol\n+            url = f\"https://{url}\"",
    "comment": "Fix URL normalization for domain:port combinations and add tests",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "6cb274ef407df86209d1aac01d7b047801a018e6",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: backend/routes/screenshot.py (modified)\n  import base64\n-from fastapi import APIRouter\n+from fastapi import APIRouter, HTTPException\n  from pydantic import BaseModel\n  import httpx\n+from urllib.parse import urlparse\n  router = APIRouter()\n+def normalize_url(url: str) -> str:\n+    \"\"\"\n+    Normalize URL to ensure it has a proper protocol.\n+    If no protocol is specified, default to https://\n+    \"\"\"\n+    url = url.strip()\n+    \n+    # Parse the URL\n+    parsed = urlparse(url)\n+    \n+    # If no scheme (protocol) is provided, add https://\n+    if not parsed.scheme:\n+        # Handle cases like \"example.com\" or \"www.example.com\"",
    "comment": "Add URL normalization to screenshot endpoint",
    "language": "diff",
    "repo": "abi/screenshot-to-code",
    "sha": "7ca63f85326f2db1aeabbfcccf75385d795af318",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: src/flask/sessions.py (modified)\n          if not app.secret_key:\n              return None\n-        keys: list[str | bytes] = [app.secret_key]\n+        keys: list[str | bytes] = []\n          if fallbacks := app.config[\"SECRET_KEY_FALLBACKS\"]:\n              keys.extend(fallbacks)\n+        keys.append(app.secret_key)  # itsdangerous expects current key at top\n          return URLSafeTimedSerializer(\n              keys,  # type: ignore[arg-type]\n              salt=self.salt,\n\nFile: tests/test_basic.py (modified)\n      def get_session() -> dict[str, t.Any]:\n          return dict(flask.session)\n-    # Set session with initial secret key\n+    # Set session with initial secret key, and two valid expiring keys\n+    app.secret_key, app.config[\"SECRET_KEY_FALLBACKS\"] = (\n+        \"0 key\",\n+        [\"-1 key\", \"-2 key\"],\n+    )\n      client.post()\n      assert client.get().json == {\"a\": 1}\n      # Change secret key, session can't be loaded and appears empty\n-    app.secret_key = \"new test key\"\n+    app.secret_key = \"? key\"\n      assert client.get().json == {}\n-    # Add initial secret key as fallback, session can be loaded\n-    app.config[\"SECRET_KEY_FALLBACKS\"] = [\"test key\"]\n+    # Rotate the valid keys, session can be loaded\n+    app.secret_key, app.config[\"SECRET_KEY_FALLBACKS\"] = (\n+        \"+1 key\",\n+        [\"0 key\", \"-1 key\"],",
    "comment": "secret key rotation: fix key list ordering",
    "language": "diff",
    "repo": "pallets/flask",
    "sha": "fb54159861708558b5f5658ebdc14709d984361c",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/markitdown-mcp/src/markitdown_mcp/__main__.py (modified)\n  import contextlib\n  import sys\n+import os\n  from collections.abc import AsyncIterator\n  from mcp.server.fastmcp import FastMCP\n  from starlette.applications import Starlette\n  @mcp.tool()\n  async def convert_to_markdown(uri: str) -> str:\n      \"\"\"Convert a resource described by an http:, https:, file: or data: URI to markdown\"\"\"\n-    return MarkItDown().convert_uri(uri).markdown\n+    return MarkItDown(enable_plugins=check_plugins_enabled()).convert_uri(uri).markdown\n+\n+\n+def check_plugins_enabled() -> bool:\n+    return os.getenv(\"MARKITDOWN_ENABLE_PLUGINS\", \"false\").strip().lower() in (\n+        \"true\",\n+        \"1\",\n+        \"yes\",\n+    )\n  def create_starlette_app(mcp_server: Server, *, debug: bool = False) -> Starlette:",
    "comment": "Have the MarkItDown MCP server read MARKITDOWN_ENABLE_PLUGINS from ENV (#1273)",
    "language": "diff",
    "repo": "microsoft/markitdown",
    "sha": "3bfb821c0962bebbc157a37bc22919dc3c5220bf",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/markitdown/src/markitdown/converters/_youtube_converter.py (modified)\n              params = parse_qs(parsed_url.query)  # type: ignore\n              if \"v\" in params and params[\"v\"][0]:\n                  video_id = str(params[\"v\"][0])\n+                transcript_list = ytt_api.list(video_id)\n+                languages = [\"en\"]\n+                for transcript in transcript_list:\n+                    languages.append(transcript.language_code)\n+                    break\n                  try:\n                      youtube_transcript_languages = kwargs.get(\n-                        \"youtube_transcript_languages\", (\"en\",)\n+                        \"youtube_transcript_languages\", languages\n                      )\n                      # Retry the transcript fetching operation\n                      transcript = self._retry_operation(\n                          retries=3,  # Retry 3 times\n                          delay=2,  # 2 seconds delay between retries\n                      )\n+\n                      if transcript:",
    "comment": "FIX YouTube transcript errors (#1241)",
    "language": "diff",
    "repo": "microsoft/markitdown",
    "sha": "56f7579ce2f74eff2b75c5337b7ad85d64baa480",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/markitdown/src/markitdown/_markitdown.py (modified)\n      ZipConverter,\n      EpubConverter,\n      DocumentIntelligenceConverter,\n+    CsvConverter,\n  )\n  from ._base_converter import DocumentConverter, DocumentConverterResult\n              self.register_converter(PdfConverter())\n              self.register_converter(OutlookMsgConverter())\n              self.register_converter(EpubConverter())\n+            self.register_converter(CsvConverter())\n              # Register Document Intelligence converter at the top of the stack if endpoint is provided\n              docintel_endpoint = kwargs.get(\"docintel_endpoint\")\n\nFile: packages/markitdown/src/markitdown/converters/__init__.py (modified)\n      DocumentIntelligenceFileType,\n  )\n  from ._epub_converter import EpubConverter\n+from ._csv_converter import CsvConverter\n  __all__ = [\n      \"PlainTextConverter\",\n      \"DocumentIntelligenceConverter\",\n      \"DocumentIntelligenceFileType\",\n      \"EpubConverter\",\n+    \"CsvConverter\",\n  ]\n\nFile: packages/markitdown/src/markitdown/converters/_csv_converter.py (added)\n+import sys\n+import csv\n+import io\n+from typing import BinaryIO, Any\n+from charset_normalizer import from_bytes\n+from ._html_converter import HtmlConverter\n+from .._base_converter import DocumentConverter, DocumentConverterResult\n+from .._stream_info import StreamInfo\n+\n+ACCEPTED_MIME_TYPE_PREFIXES = [\n+    \"text/csv\",\n+    \"application/csv\",\n+]\n+ACCEPTED_FILE_EXTENSIONS = [\".csv\"]\n+\n+\n+class CsvConverter(DocumentConverter):\n+    \"\"\"\n+    Converts CSV files to Markdown tables.\n+    \"\"\"\n\nFile: packages/markitdown/src/markitdown/converters/_doc_intel_converter.py (modified)\n  import sys\n  import re\n  import os\n-\n-from typing import BinaryIO, Any, List\n+from typing import BinaryIO, Any, List, Optional, Union\n  from enum import Enum\n  from ._html_converter import HtmlConverter\n      # Preserve the error and stack trace for later\n      _dependency_exc_info = sys.exc_info()\n+    # Define these types for type hinting when the package is not available\n+    class AzureKeyCredential:\n+        pass\n+\n+    class TokenCredential:\n+        pass\n+\n+    class DocumentIntelligenceClient:\n+        pass\n+\n\nFile: packages/markitdown/tests/_test_vectors.py (modified)\n          charset=\"cp932\",\n          url=None,\n          must_include=[\n-            \"名前,年齢,住所\",\n-            \"佐藤太郎,30,東京\",\n-            \"三木英子,25,大阪\",\n-            \"髙橋淳,35,名古屋\",\n+            \"| 名前 | 年齢 | 住所 |\",\n+            \"| --- | --- | --- |\",\n+            \"| 佐藤太郎 | 30 | 東京 |\",\n+            \"| 三木英子 | 25 | 大阪 |\",\n+            \"| 髙橋淳 | 35 | 名古屋 |\",\n          ],\n          must_not_include=[],\n      ),",
    "comment": "Add CSV to Markdown table conversion - fixes #1144 (#1176)",
    "language": "diff",
    "repo": "microsoft/markitdown",
    "sha": "8576f1d9153d43d7d7d8f3854ed771bcee281175",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/markitdown/src/markitdown/__main__.py (modified)\n          help=\"List installed 3rd-party plugins. Plugins are loaded when using the -p or --use-plugin option.\",\n      )\n+    parser.add_argument(\n+        \"--keep-data-uris\",\n+        action=\"store_true\",\n+        help=\"Keep data URIs (like base64-encoded images) in the output. By default, data URIs are truncated.\",\n+    )\n+\n      parser.add_argument(\"filename\", nargs=\"?\")\n      args = parser.parse_args()\n          markitdown = MarkItDown(enable_plugins=args.use_plugins)\n      if args.filename is None:\n-        result = markitdown.convert_stream(sys.stdin.buffer, stream_info=stream_info)\n+        result = markitdown.convert_stream(\n+            sys.stdin.buffer,\n+            stream_info=stream_info,\n+            keep_data_uris=args.keep_data_uris,\n+        )\n      else:\n-        result = markitdown.convert(args.filename, stream_info=stream_info)\n\nFile: packages/markitdown/src/markitdown/converters/_bing_serp_converter.py (modified)\n              slug.extract()\n          # Parse the algorithmic results\n-        _markdownify = _CustomMarkdownify()\n+        _markdownify = _CustomMarkdownify(**kwargs)\n          results = list()\n          for result in soup.find_all(class_=\"b_algo\"):\n              if not hasattr(result, \"find_all\"):\n\nFile: packages/markitdown/src/markitdown/converters/_docx_converter.py (modified)\n          style_map = kwargs.get(\"style_map\", None)\n          return self._html_converter.convert_string(\n-            mammoth.convert_to_html(file_stream, style_map=style_map).value\n+            mammoth.convert_to_html(file_stream, style_map=style_map).value, **kwargs\n          )\n\nFile: packages/markitdown/src/markitdown/converters/_html_converter.py (modified)\n          body_elm = soup.find(\"body\")\n          webpage_text = \"\"\n          if body_elm:\n-            webpage_text = _CustomMarkdownify().convert_soup(body_elm)\n+            webpage_text = _CustomMarkdownify(**kwargs).convert_soup(body_elm)\n          else:\n-            webpage_text = _CustomMarkdownify().convert_soup(soup)\n+            webpage_text = _CustomMarkdownify(**kwargs).convert_soup(soup)\n          assert isinstance(webpage_text, str)\n\nFile: packages/markitdown/src/markitdown/converters/_markdownify.py (modified)\n      def __init__(self, **options: Any):\n          options[\"heading_style\"] = options.get(\"heading_style\", markdownify.ATX)\n+        options[\"keep_data_uris\"] = options.get(\"keep_data_uris\", False)\n          # Explicitly cast options to the expected type if necessary\n          super().__init__(**options)\n              return alt\n          # Remove dataURIs\n-        if src.startswith(\"data:\"):\n+        if src.startswith(\"data:\") and not self.options[\"keep_data_uris\"]:\n              src = src.split(\",\")[0] + \"...\"\n          return \"![%s](%s%s)\" % (alt, src, title_part)",
    "comment": "Add support for preserving base64 encoded images (#1140)",
    "language": "diff",
    "repo": "microsoft/markitdown",
    "sha": "52432bd228cb16c8ce5c11923c3ad90ca46ac307",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  },
  {
    "code": "File: packages/markitdown/src/markitdown/_markitdown.py (modified)\n          elif base_guess.extension is not None:\n              placeholder_filename = \"placeholder\" + base_guess.extension\n+        # Check if we have a seekable stream. If not, load the entire stream into memory.\n+        if not stream.seekable():\n+            buffer = io.BytesIO()\n+            while True:\n+                chunk = stream.read(4096)\n+                if not chunk:\n+                    break\n+                buffer.write(chunk)\n+            buffer.seek(0)\n+            stream = buffer\n+\n          # Add guesses based on stream content\n          for guess in _guess_stream_info_from_stream(\n              file_stream=stream, filename_hint=placeholder_filename\n\nFile: packages/markitdown/src/markitdown/converters/_outlook_msg_converter.py (modified)\n  # Try loading optional (but in this case, required) dependencies\n  # Save reporting of any exceptions for later\n  _dependency_exc_info = None\n+olefile = None\n  try:\n      import olefile\n  except ImportError:\n          # Brute force, check if we have an OLE file\n          cur_pos = file_stream.tell()\n          try:\n-            if not olefile.isOleFile(file_stream):\n+            if olefile and not olefile.isOleFile(file_stream):\n                  return False\n          finally:\n              file_stream.seek(cur_pos)",
    "comment": "feat(docker): improve dockerfile build (#220)",
    "language": "diff",
    "repo": "microsoft/markitdown",
    "sha": "515fa854bfcbd1b94d999690ef8945c8b96c75cb",
    "quality_score": 0.4,
    "comment_type": "commit_message"
  }
]